{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "import cv2\n",
    "import torch\n",
    "import csv\n",
    "from ultralytics import SAM\n",
    "from pathlib import Path\n",
    "import time as t\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T02:29:09.524343Z",
     "start_time": "2025-04-01T02:29:05.566727Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgroundingdino\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minference\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_model, load_image, predict\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\groundingdino\\util\\inference.py:12\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbisect\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgroundingdino\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mT\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgroundingdino\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m build_model\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgroundingdino\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmisc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m clean_state_dict\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgroundingdino\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mslconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SLConfig\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\groundingdino\\models\\__init__.py:8\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Grounding DINO\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# url: https://github.com/IDEA-Research/GroundingDINO\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mGroundingDINO\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m build_groundingdino\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbuild_model\u001B[39m(args):\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# we use register to maintain models from catdet6 on.\u001B[39;00m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregistry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MODULE_BUILD_FUNCS\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\groundingdino\\models\\GroundingDINO\\__init__.py:15\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Grounding DINO\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# url: https://github.com/IDEA-Research/GroundingDINO\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgroundingdino\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m build_groundingdino\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\groundingdino\\models\\GroundingDINO\\groundingdino.py:24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mboxes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nms\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, BertModel, BertTokenizer, RobertaModel, RobertaTokenizerFast\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgroundingdino\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m box_ops, get_tokenlizer\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgroundingdino\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmisc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     28\u001B[0m     NestedTensor,\n\u001B[0;32m     29\u001B[0m     accuracy,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     34\u001B[0m     nested_tensor_from_tensor_list,\n\u001B[0;32m     35\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\__init__.py:26\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dependency_versions_check\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     28\u001B[0m     OptionalDependencyNotAvailable,\n\u001B[0;32m     29\u001B[0m     _LazyModule,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m     logging,\n\u001B[0;32m     49\u001B[0m )\n\u001B[0;32m     52\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mget_logger(\u001B[38;5;18m__name__\u001B[39m)  \u001B[38;5;66;03m# pylint: disable=invalid-name\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdependency_versions_table\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deps\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m require_version, require_version_core\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# define which module versions we always want to check at run time\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# order specific notes:\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# - tqdm must be checked before tokenizers\u001B[39;00m\n\u001B[0;32m     25\u001B[0m pkgs_to_check_at_runtime \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtqdm\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyyaml\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     38\u001B[0m ]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\utils\\__init__.py:21\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lru_cache\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FrozenSet\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_full_repo_name  \u001B[38;5;66;03m# for backward compatibility\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HF_HUB_DISABLE_TELEMETRY \u001B[38;5;28;01mas\u001B[39;00m DISABLE_TELEMETRY  \u001B[38;5;66;03m# for backward compatibility\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpackaging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m version\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1412\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[1;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\huggingface_hub\\__init__.py:962\u001B[0m, in \u001B[0;36m_attach.<locals>.__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m    960\u001B[0m submod_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpackage_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_to_modules[name]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    961\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 962\u001B[0m     submod \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubmod_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    963\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    964\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError importing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubmod_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\importlib\\__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m     88\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\huggingface_hub\\hf_api.py:48\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPError\n\u001B[1;32m---> 48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm \u001B[38;5;28;01mas\u001B[39;00m base_tqdm\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontrib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconcurrent\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m thread_map\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m constants\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\tqdm\\auto.py:21\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m warnings\u001B[38;5;241m.\u001B[39mcatch_warnings():\n\u001B[0;32m     20\u001B[0m     warnings\u001B[38;5;241m.\u001B[39msimplefilter(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m, category\u001B[38;5;241m=\u001B[39mTqdmExperimentalWarning)\n\u001B[1;32m---> 21\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautonotebook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm \u001B[38;5;28;01mas\u001B[39;00m notebook_tqdm\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01masyncio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm \u001B[38;5;28;01mas\u001B[39;00m asyncio_tqdm\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstd\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm \u001B[38;5;28;01mas\u001B[39;00m std_tqdm\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\tqdm\\autonotebook.py:16\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIPKernelApp\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m get_ipython()\u001B[38;5;241m.\u001B[39mconfig:  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnotebook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WARN_NOIPYW, IProgress\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m IProgress \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstd\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TqdmWarning\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\tqdm\\notebook.py:23\u001B[0m\n\u001B[0;32m     21\u001B[0m IPY \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:  \u001B[38;5;66;03m# IPython 4.x\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mipywidgets\u001B[39;00m\n\u001B[0;32m     24\u001B[0m     IPY \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:  \u001B[38;5;66;03m# IPython 3.x / 2.x\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\ipywidgets\\__init__.py:29\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtraitlets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m link, dlink\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_ipython\n\u001B[1;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwidgets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_ipython_extension\u001B[39m(ip):\n\u001B[0;32m     33\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Set up Jupyter to work with widgets\"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\ipywidgets\\widgets\\__init__.py:22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwidget_output\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Output\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwidget_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RadioButtons, ToggleButtons, ToggleButtonsStyle, Dropdown, Select, SelectionSlider, SelectMultiple, SelectionRangeSlider\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwidget_selectioncontainer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tab, Accordion, Stack\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwidget_string\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTML, HTMLMath, Label, Text, Textarea, Password, Combobox\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwidget_controller\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Controller\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1322\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1262\u001B[0m, in \u001B[0;36m_find_spec\u001B[1;34m(name, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1532\u001B[0m, in \u001B[0;36mfind_spec\u001B[1;34m(cls, fullname, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1506\u001B[0m, in \u001B[0;36m_get_spec\u001B[1;34m(cls, fullname, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1605\u001B[0m, in \u001B[0;36mfind_spec\u001B[1;34m(self, fullname, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:147\u001B[0m, in \u001B[0;36m_path_stat\u001B[1;34m(path)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_labels(boxes, max_area):\n",
    "    clean_boxes = []\n",
    "    box_list = boxes.tolist()\n",
    "    for box in box_list:\n",
    "        # if width * height < 0.9, add box to list.\n",
    "        if (box[2] * box[3]) < max_area:\n",
    "            clean_boxes.append(box)\n",
    "    if len(clean_boxes) < 2:\n",
    "        return boxes\n",
    "    return torch.FloatTensor(clean_boxes)\n",
    "\n",
    "def load_dino_model(model_size='swint'):\n",
    "    #choose swinb or swint\n",
    "    if model_size == 'swint':\n",
    "        config_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinT_OGC.py\"\n",
    "        checkpoint_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GroundingDINO\\weights\\groundingdino_swint_ogc.pth\"\n",
    "    elif model_size == 'swinb':\n",
    "        checkpoint_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GroundingDINO\\weights\\groundingdino_swinb_cogcoor.pth\"\n",
    "        config_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinB_cfg.py\"\n",
    "\n",
    "    model = load_model(config_path, checkpoint_path)\n",
    "    return model\n",
    "\n",
    "def run_dino_from_model(model, img_path, prompt, box_threshold, text_threshold, maxarea=0.7, save_dir=\"DINO-labels\"):\n",
    "    image_source, image = load_image(img_path)\n",
    "    boxes, accuracy, obj_name = predict(model=model, image=image, caption=prompt, box_threshold=box_threshold,\n",
    "                                        text_threshold=text_threshold)\n",
    "\n",
    "    #Convert boxes from YOLOv8 format to xyxy\n",
    "    img_height, img_width = cv2.imread(img_path).shape[:2]\n",
    "    clean_boxes = clean_labels(boxes, maxarea)\n",
    "    absolute_boxes = [[(box[0] - (box[2] / 2)) * img_width,\n",
    "                       (box[1] - (box[3] / 2)) * img_height,\n",
    "                       (box[0] + (box[2] / 2)) * img_width,\n",
    "                       (box[1] + (box[3] / 2)) * img_height] for box in clean_boxes.tolist()]\n",
    "    save_labels = True\n",
    "    if save_labels:\n",
    "        clean_boxes = clean_boxes.tolist()\n",
    "        for x in clean_boxes:\n",
    "            x.insert(0, 0)\n",
    "        with open(f'{save_dir}/{os.path.splitext(os.path.basename(img_path))[0]}.txt', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            writer.writerows(clean_boxes)\n",
    "    return absolute_boxes\n",
    "\n",
    "def save_masks(sam_results, output_dir):\n",
    "    segments = sam_results[0].masks.xyn\n",
    "    with open(f\"{Path(output_dir) / Path(sam_results[0].path).stem}.txt\", \"w\") as f:\n",
    "        for i in range(len(segments)):\n",
    "            s = segments[i]\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            segment = map(str, segments[i].reshape(-1).tolist())\n",
    "            f.write(f\"0 \" + \" \".join(segment) + \"\\n\")\n",
    "\n",
    "def run_image(DINO, img_dir, output_dir, prompt, conf, box_threshold, save_dir):\n",
    "    sam_model = \"sam2_t.pt\"\n",
    "    dino_model = \"swint\"\n",
    "    start = t.time()\n",
    "    fname = os.path.basename(img_dir)\n",
    "    path = img_dir\n",
    "    boxes = run_dino_from_model(DINO, img_dir, prompt, conf, 0.1, box_threshold, save_dir=save_dir)\n",
    "    model = SAM(sam_model)\n",
    "    sam_results = model(img_dir, model=sam_model, bboxes=boxes, verbose=False)\n",
    "    save_masks(sam_results, output_dir)\n",
    "\n",
    "    print(f\"Completed in: {t.time() - start} seconds, masks saved in {output_dir}\")\n",
    "    return sam_results\n",
    "\n",
    "def adjust_masks(sam_results):\n",
    "    result = sam_results[0]\n",
    "\n",
    "    masks = result.masks.data.cpu().numpy()  # masks, (N, H, W)\n",
    "    masks = np.moveaxis(masks, 0, -1)  # masks, (H, W, N)\n",
    "    masks = np.moveaxis(masks, -1, 0)  # masks, (N, H, W)\n",
    "\n",
    "    return masks\n",
    "\n",
    "def overlay_with_borders(image, mask, color, thickness=2):\n",
    "    # Convert mask to uint8 type\n",
    "    mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Draw contours on the image\n",
    "    cv2.drawContours(image, contours, -1, color, thickness)\n",
    "    return image\n",
    "\n",
    "def draw_boxes_on_image(image, boxes):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on the image using absolute coordinates.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The original image.\n",
    "        boxes (list): List of bounding boxes in the format [x1, y1, x2, y2].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image with bounding boxes drawn on it.\n",
    "    \"\"\"\n",
    "    # Convert the OpenCV image (BGR) to PIL for drawing\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    # Iterate over the list of boxes and draw them\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=(255, 0, 255), width=2)  # Drawing a rectangle with purple border\n",
    "\n",
    "    # Convert back to OpenCV format for display\n",
    "    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def optimize_prompts(prompts_file, gt_path, img_dir, save_file, threshold, DINO):\n",
    "    inf_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\DINO-labels\"\n",
    "    if not os.path.exists(inf_path):\n",
    "        try:\n",
    "            os.makedirs(inf_path)\n",
    "            print(f\"Directory '{inf_path}' created as it was missing.\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    with open(prompts_file, 'r') as file:\n",
    "        result_dict = {}\n",
    "        for x in file:\n",
    "            result_dict[x.strip()] = {}\n",
    "\n",
    "    # result_dict = dict.fromkeys(prompts,{})\n",
    "    for prompt in result_dict.keys():\n",
    "        print(f'Trying prompt: \"{prompt}\"')\n",
    "\n",
    "        box_threshold = 0.3\n",
    "        text_threshold = 0.1\n",
    "        model_size = 'swint'\n",
    "        run_dino_from_model(DINO, img_dir, prompt, box_threshold, text_threshold, maxarea=threshold)\n",
    "\n",
    "        metrics = process_file(inf_path, gt_path, threshold=threshold)\n",
    "\n",
    "        result_dict[prompt]['iou_scores'] = np.mean(metrics['iou_scores'])\n",
    "\n",
    "    results = sorted(list(result_dict.items()), key=lambda a: a[1]['iou_scores'], reverse=True)\n",
    "    print(results)\n",
    "\n",
    "    with open(save_file, 'w') as output:\n",
    "        for prompt_stats in results:\n",
    "            output.write(str(prompt_stats) + '\\n')\n",
    "\n",
    "    return results\n",
    "\n",
    "def calculate_metrics(tp, fp, fn, tn):\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    mcc = ((tp * tn) - (fp * fn)) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) \\\n",
    "        if np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n",
    "    return precision, recall, f1, mcc, specificity\n",
    "\n",
    "def read_and_draw_boxes(file_path, image_dim=(1280, 720)):\n",
    "    boxes = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            class_id, x, y, width, height = map(float, line.strip().split())\n",
    "            x1 = (x-(width/2))*image_dim[0]\n",
    "            x2 = (x+(width/2))*image_dim[0]\n",
    "            y1 = (y-(height/2))*image_dim[1]\n",
    "            y2 = (y+(height/2))*image_dim[1]\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "    image = Image.new('L', image_dim, 0)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box, fill=255)\n",
    "        #draw.rectangle([1,1,20,20], fill=255)\n",
    "    #image.save(\"test.jpg\")\n",
    "    return np.array(image, dtype=np.uint8)\n",
    "\n",
    "def clean_labels_from_file(file_path, cleaning_threshold=0.6):\n",
    "    # Read the file and check if it has more than one line\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    if len(lines) > 1:\n",
    "        accepted_lines = []\n",
    "\n",
    "        # Process each line\n",
    "        for line in lines:\n",
    "            class_id, x, y, width, height = map(float, line.strip().split())\n",
    "            # if width * height < 0.9:\n",
    "            if (width * height) < cleaning_threshold:\n",
    "                accepted_lines.append(line)\n",
    "\n",
    "        # Overwrite the file with accepted lines\n",
    "        with open(file_path, 'w') as f:\n",
    "            if len(accepted_lines) > 0:\n",
    "                for line in accepted_lines:\n",
    "                    f.write(line)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def draw_boxes(boxes, image_dim=(1280, 720)):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes directly from a list of absolute boxes.\n",
    "\n",
    "    Parameters:\n",
    "    boxes (list): List of absolute box coordinates in xyxy format.\n",
    "    image_dim (tuple): Dimensions of the output image (width, height).\n",
    "\n",
    "    Returns:\n",
    "    np.array: Binary image with boxes drawn.\n",
    "    \"\"\"\n",
    "    # Create a blank image to draw the boxes\n",
    "    image = Image.new('L', image_dim, 0)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Draw each box on the image\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box, fill=255)\n",
    "\n",
    "    return np.array(image, dtype=np.uint8)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def prompt_optimizer(prompts_file, gt_path, img_path, save_file, threshold, DINO):\n",
    "    # Ensure inference path exists\n",
    "    inf_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\DINO-labels\"\n",
    "    os.makedirs(inf_path, exist_ok=True)\n",
    "\n",
    "    # Initialize result dictionary from prompt file\n",
    "    with open(prompts_file, 'r') as file:\n",
    "        result_dict = {x.strip(): {} for x in file}\n",
    "\n",
    "    # Process each prompt\n",
    "    for prompt in result_dict.keys():\n",
    "        print(f'Trying prompt: \"{prompt}\"')\n",
    "\n",
    "        # Run prediction and save labels\n",
    "        run_dino_from_model(DINO, img_path, prompt, box_threshold=0.3, text_threshold=0.1, maxarea=threshold)\n",
    "\n",
    "        # Process single predicted and ground truth file\n",
    "        predicted_mask_file = os.path.join(inf_path, f\"{os.path.splitext(os.path.basename(img_path))[0]}.txt\")\n",
    "        metrics = process_file(predicted_mask_file, gt_path, threshold)\n",
    "\n",
    "        # Save the IoU score for the prompt\n",
    "        result_dict[prompt]['iou_scores'] = np.mean(metrics['iou_scores'])\n",
    "\n",
    "    # Sort and save results\n",
    "    results = sorted(result_dict.items(), key=lambda a: a[1]['iou_scores'], reverse=True)\n",
    "    print(\"Results:\", results)\n",
    "\n",
    "    with open(save_file, 'w') as output:\n",
    "        for prompt_stats in results:\n",
    "            output.write(str(prompt_stats) + '\\n')\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_file(predicted_mask_file, ground_truth_mask_file, threshold):\n",
    "    # Initialize metrics dictionary\n",
    "    metrics = {\n",
    "        'iou_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': [],\n",
    "        'f1_scores': [],\n",
    "        'mcc_scores': [],\n",
    "        'specificity_scores': []\n",
    "    }\n",
    "\n",
    "    # Preprocess predicted mask\n",
    "    clean_labels_from_file(predicted_mask_file, threshold)\n",
    "    predicted_mask = read_and_draw_boxes(predicted_mask_file)\n",
    "    ground_truth_mask = read_and_draw_boxes(ground_truth_mask_file)\n",
    "\n",
    "    # Convert masks to binary\n",
    "    _, predicted_mask_bin = cv2.threshold(predicted_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "    _, ground_truth_mask_bin = cv2.threshold(ground_truth_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    predicted_mask_bin = predicted_mask_bin / 255\n",
    "    ground_truth_mask_bin = ground_truth_mask_bin / 255\n",
    "\n",
    "    # Calculate true positives, true negatives, false positives, and false negatives\n",
    "    tp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 1)))\n",
    "    tn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 0)))\n",
    "    fp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 0)))\n",
    "    fn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 1)))\n",
    "\n",
    "    # Calculate metrics\n",
    "    intersection = np.logical_and(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    union = np.logical_or(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    metrics['iou_scores'].append(np.sum(intersection) / np.sum(union))\n",
    "    # Calculate precision, recall, f1-score, MCC, and specificity\n",
    "    precision, recall, f1, mcc, specificity = calculate_metrics(tp, fp, fn, tn)\n",
    "    metrics['precision_scores'].append(precision)\n",
    "    metrics['recall_scores'].append(recall)\n",
    "    metrics['f1_scores'].append(f1)\n",
    "    metrics['mcc_scores'].append(mcc)\n",
    "    metrics['specificity_scores'].append(specificity)\n",
    "    #print(metrics['iou_scores'])\n",
    "    return metrics\n",
    "\n",
    "def process_mask_arrays(predicted_mask_array, ground_truth_mask_array):\n",
    "    # Resize predicted mask to match the ground truth mask's dimensions\n",
    "    if predicted_mask_array.shape != ground_truth_mask_array.shape:\n",
    "        predicted_mask_array = cv2.resize(predicted_mask_array, (ground_truth_mask_array.shape[1], ground_truth_mask_array.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Initialize metrics dictionary\n",
    "    metrics = {\n",
    "        'iou_scores': [],\n",
    "        #'pixel_accuracies': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': [],\n",
    "        'f1_scores': [],\n",
    "        'mcc_scores': [],\n",
    "        'specificity_scores': []\n",
    "    }\n",
    "\n",
    "    # Convert masks to binary based on threshold\n",
    "    _, predicted_mask_bin = cv2.threshold(predicted_mask_array, 127, 255, cv2.THRESH_BINARY)\n",
    "    _, ground_truth_mask_bin = cv2.threshold(ground_truth_mask_array, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Normalize binary masks for calculation\n",
    "    predicted_mask_bin = predicted_mask_bin / 255\n",
    "    ground_truth_mask_bin = ground_truth_mask_bin / 255\n",
    "\n",
    "    # Calculate true positives, true negatives, false positives, and false negatives\n",
    "    tp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 1)))\n",
    "    tn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 0)))\n",
    "    fp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 0)))\n",
    "    fn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 1)))\n",
    "\n",
    "    # Calculate IoU and pixel accuracy\n",
    "    intersection = np.logical_and(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    union = np.logical_or(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    metrics['iou_scores'].append(np.sum(intersection) / np.sum(union))\n",
    "    #metrics['pixel_accuracies'].append(pixel_accuracy(predicted_mask_bin, ground_truth_mask_bin))\n",
    "\n",
    "    # Calculate precision, recall, f1-score, MCC, and specificity\n",
    "    precision, recall, f1, mcc, specificity = calculate_metrics(tp, fp, fn, tn)\n",
    "    metrics['precision_scores'].append(precision)\n",
    "    metrics['recall_scores'].append(recall)\n",
    "    metrics['f1_scores'].append(f1)\n",
    "    metrics['mcc_scores'].append(mcc)\n",
    "    metrics['specificity_scores'].append(specificity)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def confidence_optimizer(prompt, DINO, gt_path, img_path, threshold):\n",
    "    inf_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\DINO-labels\"\n",
    "    os.makedirs(inf_path, exist_ok=True)\n",
    "\n",
    "    best_iou = 0\n",
    "    best_conf = 0\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    shape = image.shape\n",
    "\n",
    "    # Step 1: Precision 1 sweep (coarse) from 0.0 to 0.9 in steps of 0.1\n",
    "    for conf in np.arange(0.0, 0.91, 0.1):\n",
    "        box_threshold = conf\n",
    "        text_threshold = 0.1\n",
    "        boxes = run_dino_from_model(DINO, img_path, prompt, box_threshold, text_threshold)\n",
    "        pred_masks = draw_boxes(boxes, (shape[1], shape[0]))\n",
    "        gt_masks = read_and_draw_boxes(gt_path)\n",
    "\n",
    "        metrics = process_mask_arrays(pred_masks, gt_masks)\n",
    "        iou = np.mean(metrics['iou_scores'])\n",
    "        print('P1 rep')\n",
    "        print(f\"[Precision 1] Confidence: {conf:.1f}, IoU: {iou:.4f}\")\n",
    "\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_conf = conf\n",
    "\n",
    "    print(f\"Best from Precision 1: Confidence = {best_conf:.1f}, IoU = {best_iou:.4f}\")\n",
    "\n",
    "    # Step 2: Precision 2 sweep from (best_conf - 0.1) to (best_conf + 0.1) in steps of 0.01\n",
    "    lower = best_conf - 0.1\n",
    "    upper = best_conf + 0.1\n",
    "    step = 0.01\n",
    "\n",
    "    for conf in np.arange(lower, upper + step, step):\n",
    "        box_threshold = conf\n",
    "        text_threshold = 0.01\n",
    "        boxes = run_dino_from_model(DINO, img_path, prompt, box_threshold, text_threshold)\n",
    "        pred_masks = draw_boxes(boxes, (shape[1], shape[0]))\n",
    "        gt_masks = read_and_draw_boxes(gt_path)\n",
    "\n",
    "        metrics = process_mask_arrays(pred_masks, gt_masks)\n",
    "        iou = np.mean(metrics['iou_scores'])\n",
    "        print('P2 rep')\n",
    "        print(f\"[Precision 2] Confidence: {conf:.2f}, IoU: {iou:.4f}\")\n",
    "\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_conf = conf\n",
    "\n",
    "    print(f\"Final Best: Confidence = {best_conf:.2f}, IoU = {best_iou:.4f}\")\n",
    "    return best_iou, best_conf\n",
    "\n",
    "\n",
    "def multi_optimizer(img_dir, gt_label_dir, DINO, prompts, threshold=0.9):\n",
    "    start = t.time()\n",
    "    best_iou = 0\n",
    "    best_prompt = \"\"\n",
    "    best_conf = 0\n",
    "    for prompt in prompts:\n",
    "        print(f\"Trying prompt: '{prompt}'\")\n",
    "        iou, conf = confidence_optimizer(prompt, DINO, gt_label_dir, img_dir, threshold)\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_conf = conf\n",
    "            best_prompt = prompt\n",
    "        print(f\"So far: best prompt is '{best_prompt}', conf is {best_conf}, resulting in {best_iou} IOU)\")\n",
    "    print(f\"\\n\\n\\n\\n\\nFinal Result: best prompt is '{best_prompt}', conf is {best_conf}, resulting in {best_iou} IOU)\")\n",
    "    print(f\"final time: {t.time() - start}\")\n",
    "    return {\"prompt\": best_prompt, \"conf\": best_conf, \"iou\": best_iou}"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def sort_largest_file(folder_path):\n",
    "    # Dictionary to store file names and their line counts\n",
    "    file_line_counts = {}\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file is a .txt file\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Open the file and count lines\n",
    "            with open(file_path, 'r') as file:\n",
    "                line_count = sum(1 for line in file)\n",
    "            # Add the file and line count to the dictionary\n",
    "            file_line_counts[file_name] = line_count\n",
    "        else:\n",
    "            print(\"File encountered not in .txt format.\")\n",
    "    # Sort files by line count in descending order and return as list of file names\n",
    "    sorted_files = sorted(file_line_counts, key=file_line_counts.get, reverse=True)\n",
    "    return sorted_files\n",
    "\n",
    "# Usage\n",
    "folder_path = r'C:/Users/cmull/DataspellProjects/AutoAnnotate/autoannotate study/berries-bounding-box-1/train/labels'\n",
    "image_folder_path = r'C:/Users/cmull/DataspellProjects/AutoAnnotate/autoannotate study/berries-bounding-box-1/train/images'\n",
    "sorted_txt_files = sort_largest_file(folder_path)\n",
    "print(\"Files sorted by line count:\", sorted_txt_files)\n",
    "reference_txt = folder_path + '\\\\' + sorted_txt_files[0]\n",
    "reference_image = image_folder_path + '\\\\' + sorted_txt_files[0].split(\".txt\")[0] + \".jpg\""
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "box_threshold = 0.5\n",
    "DINO = load_dino_model()\n",
    "prompts_file = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\prompts\\blueberry-prompts.txt\""
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "prompt_result = prompt_optimizer(prompts_file, reference_txt, reference_image, \"best.txt\", box_threshold, DINO)\n",
    "\n",
    "top_2 = prompt_result[:2]\n",
    "top2 = [result[0] for result in prompt_result][0:2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(top2)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "multi_optimizer(reference_image, reference_txt, DINO, top2, box_threshold)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "import re\n",
    "\n",
    "def extract_descriptions(response):\n",
    "    \"\"\"\n",
    "    Extracts only the clean descriptions from the model response, removing metadata, numbering, and unnecessary text.\n",
    "\n",
    "    :param response: The raw response from the model.\n",
    "    :return: A list of cleaned descriptions without numbering or unwanted text.\n",
    "    \"\"\"\n",
    "    lines = response.split(\"\\n\")  # Split response into lines\n",
    "    unwanted_keywords = [\"user\", \"assistant\", \"describe\", \"text & image output\"]  # Keywords to ignore\n",
    "\n",
    "    descriptions = []\n",
    "    for line in lines:\n",
    "        clean_line = line.strip()\n",
    "        if not clean_line:\n",
    "            continue  # Skip empty lines\n",
    "        if any(keyword in clean_line.lower() for keyword in unwanted_keywords):\n",
    "            continue  # Skip lines with unwanted keywords\n",
    "\n",
    "        # Remove leading numbering like \"1. \", \"2)\", \"3 - \", etc.\n",
    "        clean_line = re.sub(r\"^\\s*\\d+[\\.\\)\\-]\\s*\", \"\", clean_line)\n",
    "\n",
    "        if clean_line:  # Only add if the line still has content\n",
    "            descriptions.append(clean_line)\n",
    "\n",
    "    return descriptions\n",
    "# Load Model and Processor\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model.tie_weights()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Image\n",
    "image_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\bounding-buds-1\\train\\images\\IMG_1067_JPG_jpg.rf.a07350fa4e778c7ae999106ecc5dde24.jpg\"\n",
    "raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "manual_entry = input(\"object in image\")\n",
    "# Define Conversation Prompt (Corrected)\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},  # Note: No \"image\": raw_image here!\n",
    "            {\"type\": \"text\", \"text\": f\"Describe the target of {manual_entry} in the image in 3 words maximum for prompt use in a zero-shot detection model, and give 5 separate entries, each separated by a new line, and its own separate descriptor of the target. number each prompt. then simply new line. strictly the prompts, no other response is required. use visual description of the target in the image only. no prompts should be the same.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Convert to Text Prompt\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Process Inputs Correctly\n",
    "inputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\").to(model.device)  # Ensure correct parameter order"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate Output\n",
    "output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "# Decode and Print Output\n",
    "response = processor.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Text & Image Output:\", response)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extracted descriptions\n",
    "cleaned_descriptions = extract_descriptions(response)\n",
    "\n",
    "# Print results\n",
    "for prompt in cleaned_descriptions:\n",
    "    print(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(cleaned_descriptions)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T02:38:29.646587Z",
     "start_time": "2025-04-01T02:37:50.355938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor, LlavaForConditionalGeneration, AutoModelForCausalLM, AutoModelForVision2Seq\n",
    "import re\n",
    "torch.cuda.empty_cache()\n",
    "def extract_descriptions(response):\n",
    "    \"\"\"\n",
    "    Extracts only the clean descriptions from the model response, removing metadata, numbering, and unnecessary text.\n",
    "\n",
    "    :param response: The raw response from the model.\n",
    "    :return: A list of cleaned descriptions without numbering or unwanted text.\n",
    "    \"\"\"\n",
    "    lines = response.split(\"\\n\")\n",
    "    unwanted_keywords = [\"user\", \"assistant\", \"describe\", \"text & image output\"]\n",
    "\n",
    "    descriptions = []\n",
    "    for line in lines:\n",
    "        clean_line = line.strip()\n",
    "        if not clean_line:\n",
    "            continue\n",
    "        if any(keyword in clean_line.lower() for keyword in unwanted_keywords):\n",
    "            continue\n",
    "\n",
    "        clean_line = re.sub(r\"^\\s*\\d+[\\.\\)\\-]\\s*\", \"\", clean_line)\n",
    "\n",
    "        if clean_line:\n",
    "            descriptions.append(clean_line)\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "# Model selection function\n",
    "def load_model_and_processor(model_name):\n",
    "    if model_name == \"llama\":\n",
    "        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "        model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "        model.tie_weights()\n",
    "    elif model_name == \"llava\":\n",
    "        model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "        processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "    elif model_name == \"deepseek\":\n",
    "        model_id = \"deepseek-ai/deepseek-vl2-tiny\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "    elif model_name == \"gemma\":\n",
    "        model_id = \"google/gemma-3-4b-it\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "        processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name\")\n",
    "    return model, processor\n",
    "\n",
    "# Select model here (change this to switch models)\n",
    "model_name = \"gemma\"  # Options: \"llama\", \"llava\", \"deepseek\", \"gemma\"\n",
    "model, processor = load_model_and_processor(model_name)\n",
    "\n",
    "# Load Image\n",
    "image_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\bounding-buds-1\\train\\images\\IMG_1067_JPG_jpg.rf.a07350fa4e778c7ae999106ecc5dde24.jpg\"\n",
    "raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "manual_entry = input(\"object in image: \")\n",
    "\n",
    "# Define Conversation Prompt\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": f\"Describe the target '{manual_entry}' in the image in 3 words maximum for prompt use in a zero-shot detection model. Provide exactly 5 distinct entries, each numbered and separated by a new line. No additional explanations or repeating descriptions.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Process inputs\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(**inputs, do_sample=True, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "# Decode and print output\n",
    "response = processor.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Text & Image Output ({model_name}):\", response)\n",
    "\n",
    "# Extract and print descriptions\n",
    "cleaned_descriptions = extract_descriptions(response)\n",
    "print(\"Extracted Descriptions:\", cleaned_descriptions)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0da9084107b840839310bf59ca4b0b1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 15.38 GiB. GPU 0 has a total capacity of 15.99 GiB of which 9.60 GiB is free. Of the allocated memory 4.92 GiB is allocated by PyTorch, and 121.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 83\u001B[0m\n\u001B[0;32m     80\u001B[0m inputs \u001B[38;5;241m=\u001B[39m processor(text\u001B[38;5;241m=\u001B[39mprompt, images\u001B[38;5;241m=\u001B[39mraw_image, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(model\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     82\u001B[0m \u001B[38;5;66;03m# Generate output\u001B[39;00m\n\u001B[1;32m---> 83\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.9\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;66;03m# Decode and print output\u001B[39;00m\n\u001B[0;32m     86\u001B[0m response \u001B[38;5;241m=\u001B[39m processor\u001B[38;5;241m.\u001B[39mdecode(output[\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\generation\\utils.py:2326\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001B[0m\n\u001B[0;32m   2318\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2319\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2320\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[0;32m   2321\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2322\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2323\u001B[0m     )\n\u001B[0;32m   2325\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[1;32m-> 2326\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2327\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2328\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2329\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2330\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2331\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2332\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2333\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2334\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2336\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[0;32m   2337\u001B[0m     \u001B[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001B[39;00m\n\u001B[0;32m   2338\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2339\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2340\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   2341\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2342\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2343\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\generation\\utils.py:3286\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[0;32m   3283\u001B[0m model_inputs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_hidden_states\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[0;32m   3285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_prefill:\n\u001B[1;32m-> 3286\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   3287\u001B[0m     is_prefill \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   3288\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\accelerate\\hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[1;34m(module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\models\\qwen2_5_vl\\modeling_qwen2_5_vl.py:1793\u001B[0m, in \u001B[0;36mQwen2_5_VLForConditionalGeneration.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts)\u001B[0m\n\u001B[0;32m   1791\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pixel_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1792\u001B[0m     pixel_values \u001B[38;5;241m=\u001B[39m pixel_values\u001B[38;5;241m.\u001B[39mtype(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvisual\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m-> 1793\u001B[0m     image_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvisual\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid_thw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_grid_thw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1794\u001B[0m     n_image_tokens \u001B[38;5;241m=\u001B[39m (input_ids \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mimage_token_id)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m   1795\u001B[0m     n_image_features \u001B[38;5;241m=\u001B[39m image_embeds\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\accelerate\\hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[1;34m(module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\models\\qwen2_5_vl\\modeling_qwen2_5_vl.py:557\u001B[0m, in \u001B[0;36mQwen2_5_VisionTransformerPretrainedModel.forward\u001B[1;34m(self, hidden_states, grid_thw)\u001B[0m\n\u001B[0;32m    553\u001B[0m         hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    554\u001B[0m             blk\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m, hidden_states, cu_seqlens_now, \u001B[38;5;28;01mNone\u001B[39;00m, position_embeddings\n\u001B[0;32m    555\u001B[0m         )\n\u001B[0;32m    556\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 557\u001B[0m         hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcu_seqlens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcu_seqlens_now\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    559\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerger(hidden_states)\n\u001B[0;32m    560\u001B[0m reverse_indices \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margsort(window_index)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\accelerate\\hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[1;34m(module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\models\\qwen2_5_vl\\modeling_qwen2_5_vl.py:350\u001B[0m, in \u001B[0;36mQwen2_5_VLVisionBlock.forward\u001B[1;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001B[0m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    344\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    345\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    348\u001B[0m     position_embeddings: Optional[Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    349\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 350\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcu_seqlens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcu_seqlens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrotary_pos_emb\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrotary_pos_emb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    356\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(hidden_states))\n\u001B[0;32m    357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\accelerate\\hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[1;34m(module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\models\\qwen2_5_vl\\modeling_qwen2_5_vl.py:319\u001B[0m, in \u001B[0;36mQwen2_5_VLVisionSdpaAttention.forward\u001B[1;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001B[0m\n\u001B[0;32m    317\u001B[0m k \u001B[38;5;241m=\u001B[39m k\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    318\u001B[0m v \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 319\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    320\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    321\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mreshape(seq_length, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 15.38 GiB. GPU 0 has a total capacity of 15.99 GiB of which 9.60 GiB is free. Of the allocated memory 4.92 GiB is allocated by PyTorch, and 121.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T03:42:20.602611Z",
     "start_time": "2025-04-01T03:40:43.737796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "# Function to extract clean descriptions from the model response\n",
    "def extract_descriptions(response):\n",
    "    lines = response.split(\"\\n\")\n",
    "    unwanted_keywords = [\"user\", \"assistant\", \"describe\", \"text & image output\"]\n",
    "\n",
    "    descriptions = []\n",
    "    for line in lines:\n",
    "        clean_line = line.strip()\n",
    "        if not clean_line:\n",
    "            continue\n",
    "        if any(keyword in clean_line.lower() for keyword in unwanted_keywords):\n",
    "            continue\n",
    "\n",
    "        clean_line = re.sub(r\"^\\s*\\d+[\\.\\)\\-]\\s*\", \"\", clean_line)\n",
    "\n",
    "        if clean_line:\n",
    "            descriptions.append(clean_line)\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "# Load Qwen2.5 VL model efficiently using Unsloth\n",
    "model_id = \"unsloth/Qwen2-VL-2B-Instruct-unsloth-bnb-4bit\"\n",
    "model, processor = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_id,\n",
    "    max_seq_length=1024,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Load Image\n",
    "image_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\bounding-buds-1\\train\\images\\IMG_1067_JPG_jpg.rf.a07350fa4e778c7ae999106ecc5dde24.jpg\"\n",
    "raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "manual_entry = input(\"Object in image: \")\n",
    "\n",
    "# Define Conversation Prompt\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": raw_image},\n",
    "            {\"type\": \"text\", \"text\": f\"Provide exactly 5 unique visual descriptions of '{manual_entry}' in the image. Maximum 3 words per description. Number them from 1 to 5. No additional text.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(text=[prompt], images=[raw_image], return_tensors=\"pt\")\n",
    "inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, do_sample=True, temperature=0.7, top_p=0.9, max_new_tokens=50)\n",
    "\n",
    "# Decode output\n",
    "response = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(\"Model Response:\", response)\n",
    "\n",
    "# Extract and print descriptions\n",
    "cleaned_descriptions = extract_descriptions(response)\n",
    "print(\"Extracted Descriptions:\", cleaned_descriptions)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2_Vl patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4080. Num GPUs = 2. Max memory: 15.992 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b770e44b09844d978c22af32eab4430c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f559272e93974d47939f855cffa8a5d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/572 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "818584891712477b969e71dfdee37703"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.33k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "668a88dfe36a4125956f93c1b03263ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b14022d48f194217838cbe53e65c38fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28fde1dcddaf48908c3c190240ccc91f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ae43c6f1c994ebbaeedee52582e7f8c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/392 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78980a128706416493c18583249624bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cd582acb1314e0b8bae28cc204452ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29d4aff6dc5442e280741c627ceec841"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 15.50 GiB. GPU 0 has a total capacity of 15.99 GiB of which 2.27 GiB is free. Of the allocated memory 11.98 GiB is allocated by PyTorch, and 266.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\unsloth\\models\\vision.py:205\u001B[0m, in \u001B[0;36munsloth_base_fast_generate\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode(), autocaster:\n\u001B[1;32m--> 205\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_generate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\generation\\utils.py:2326\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001B[0m\n\u001B[0;32m   2325\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[1;32m-> 2326\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2327\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2328\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2329\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2330\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2331\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2332\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2333\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2334\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2336\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[0;32m   2337\u001B[0m     \u001B[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\generation\\utils.py:3286\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[0;32m   3285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_prefill:\n\u001B[1;32m-> 3286\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   3287\u001B[0m     is_prefill \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\unsloth_compiled_cache\\unsloth_compiled_module_qwen2_vl.py:1331\u001B[0m, in \u001B[0;36mQwen2VLForConditionalGeneration.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **loss_kwargs)\u001B[0m\n\u001B[0;32m   1312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m   1313\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1314\u001B[0m     input_ids: torch\u001B[38;5;241m.\u001B[39mLongTensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1329\u001B[0m     cache_position: Optional[torch\u001B[38;5;241m.\u001B[39mLongTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloss_kwargs,\n\u001B[0;32m   1330\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n\u001B[1;32m-> 1331\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mQwen2VLForConditionalGeneration_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpixel_values_videos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_grid_thw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvideo_grid_thw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrope_deltas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mloss_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\unsloth_compiled_cache\\unsloth_compiled_module_qwen2_vl.py:926\u001B[0m, in \u001B[0;36mQwen2VLForConditionalGeneration_forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **loss_kwargs)\u001B[0m\n\u001B[0;32m    925\u001B[0m pixel_values \u001B[38;5;241m=\u001B[39m pixel_values\u001B[38;5;241m.\u001B[39mtype(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvisual\u001B[38;5;241m.\u001B[39mget_dtype())\n\u001B[1;32m--> 926\u001B[0m image_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvisual\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid_thw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_grid_thw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    927\u001B[0m n_image_tokens \u001B[38;5;241m=\u001B[39m (input_ids \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mimage_token_id)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:1038\u001B[0m, in \u001B[0;36mQwen2VisionTransformerPretrainedModel.forward\u001B[1;34m(self, hidden_states, grid_thw)\u001B[0m\n\u001B[0;32m   1037\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1038\u001B[0m         hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcu_seqlens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcu_seqlens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerger(hidden_states)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:452\u001B[0m, in \u001B[0;36mQwen2VLVisionBlock.forward\u001B[1;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001B[0m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    447\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    450\u001B[0m     position_embeddings: Optional[Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    451\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 452\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    453\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcu_seqlens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcu_seqlens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrotary_pos_emb\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrotary_pos_emb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    458\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(hidden_states))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\unsloth_compiled_cache\\unsloth_compiled_module_qwen2_vl.py:465\u001B[0m, in \u001B[0;36mVisionSdpaAttention.forward\u001B[1;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001B[0m\n\u001B[0;32m    458\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    459\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    460\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    463\u001B[0m     position_embeddings: Optional[Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    464\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 465\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVisionSdpaAttention_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcu_seqlens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrotary_pos_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\unsloth_compiled_cache\\unsloth_compiled_module_qwen2_vl.py:445\u001B[0m, in \u001B[0;36mVisionSdpaAttention_forward\u001B[1;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001B[0m\n\u001B[0;32m    444\u001B[0m v \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 445\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    446\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 15.50 GiB. GPU 0 has a total capacity of 15.99 GiB of which 2.93 GiB is free. Of the allocated memory 11.45 GiB is allocated by PyTorch, and 161.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 59\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# Generate output\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 59\u001B[0m     generated_ids \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.9\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;66;03m# Decode output\u001B[39;00m\n\u001B[0;32m     62\u001B[0m response \u001B[38;5;241m=\u001B[39m processor\u001B[38;5;241m.\u001B[39mbatch_decode(generated_ids, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\unsloth\\models\\vision.py:210\u001B[0m, in \u001B[0;36munsloth_base_fast_generate\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    208\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt_lookup_num_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode(), autocaster:\n\u001B[1;32m--> 210\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_generate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    212\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\generation\\utils.py:2326\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001B[0m\n\u001B[0;32m   2318\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2319\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2320\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[0;32m   2321\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2322\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2323\u001B[0m     )\n\u001B[0;32m   2325\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[1;32m-> 2326\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2327\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2328\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2329\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2330\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2331\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2332\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2333\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2334\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2336\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[0;32m   2337\u001B[0m     \u001B[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001B[39;00m\n\u001B[0;32m   2338\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2339\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2340\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   2341\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2342\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2343\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\generation\\utils.py:3286\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[0;32m   3283\u001B[0m model_inputs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_hidden_states\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[0;32m   3285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_prefill:\n\u001B[1;32m-> 3286\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   3287\u001B[0m     is_prefill \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   3288\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\unsloth_compiled_cache\\unsloth_compiled_module_qwen2_vl.py:1331\u001B[0m, in \u001B[0;36mQwen2VLForConditionalGeneration.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **loss_kwargs)\u001B[0m\n\u001B[0;32m   1312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m   1313\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1314\u001B[0m     input_ids: torch\u001B[38;5;241m.\u001B[39mLongTensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1329\u001B[0m     cache_position: Optional[torch\u001B[38;5;241m.\u001B[39mLongTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloss_kwargs,\n\u001B[0;32m   1330\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n\u001B[1;32m-> 1331\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mQwen2VLForConditionalGeneration_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpixel_values_videos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_grid_thw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvideo_grid_thw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrope_deltas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mloss_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\unsloth_compiled_cache\\unsloth_compiled_module_qwen2_vl.py:926\u001B[0m, in \u001B[0;36mQwen2VLForConditionalGeneration_forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **loss_kwargs)\u001B[0m\n\u001B[0;32m    924\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pixel_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    925\u001B[0m     pixel_values \u001B[38;5;241m=\u001B[39m pixel_values\u001B[38;5;241m.\u001B[39mtype(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvisual\u001B[38;5;241m.\u001B[39mget_dtype())\n\u001B[1;32m--> 926\u001B[0m     image_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvisual\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid_thw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_grid_thw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    927\u001B[0m     n_image_tokens \u001B[38;5;241m=\u001B[39m (input_ids \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mimage_token_id)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m    928\u001B[0m     n_image_features \u001B[38;5;241m=\u001B[39m image_embeds\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:1038\u001B[0m, in \u001B[0;36mQwen2VisionTransformerPretrainedModel.forward\u001B[1;34m(self, hidden_states, grid_thw)\u001B[0m\n\u001B[0;32m   1034\u001B[0m         hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m   1035\u001B[0m             blk\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m, hidden_states, cu_seqlens, \u001B[38;5;28;01mNone\u001B[39;00m, position_embeddings\n\u001B[0;32m   1036\u001B[0m         )\n\u001B[0;32m   1037\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1038\u001B[0m         hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcu_seqlens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcu_seqlens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerger(hidden_states)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:452\u001B[0m, in \u001B[0;36mQwen2VLVisionBlock.forward\u001B[1;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001B[0m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    447\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    450\u001B[0m     position_embeddings: Optional[Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    451\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 452\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    453\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcu_seqlens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcu_seqlens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrotary_pos_emb\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrotary_pos_emb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    458\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(hidden_states))\n\u001B[0;32m    459\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LLM_ENV\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\unsloth_compiled_cache\\unsloth_compiled_module_qwen2_vl.py:465\u001B[0m, in \u001B[0;36mVisionSdpaAttention.forward\u001B[1;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001B[0m\n\u001B[0;32m    458\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    459\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    460\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    463\u001B[0m     position_embeddings: Optional[Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    464\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 465\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVisionSdpaAttention_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcu_seqlens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrotary_pos_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\unsloth_compiled_cache\\unsloth_compiled_module_qwen2_vl.py:445\u001B[0m, in \u001B[0;36mVisionSdpaAttention_forward\u001B[1;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001B[0m\n\u001B[0;32m    443\u001B[0m k \u001B[38;5;241m=\u001B[39m k\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    444\u001B[0m v \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 445\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    446\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    447\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mreshape(seq_length, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 15.50 GiB. GPU 0 has a total capacity of 15.99 GiB of which 2.27 GiB is free. Of the allocated memory 11.98 GiB is allocated by PyTorch, and 266.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
