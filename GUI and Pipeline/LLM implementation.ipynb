{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "import cv2\n",
    "import torch\n",
    "import csv\n",
    "from ultralytics import SAM\n",
    "from pathlib import Path\n",
    "import time as t\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T23:17:17.134543Z",
     "start_time": "2025-03-25T23:17:17.132064Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_labels(boxes, max_area):\n",
    "    clean_boxes = []\n",
    "    box_list = boxes.tolist()\n",
    "    for box in box_list:\n",
    "        # if width * height < 0.9, add box to list.\n",
    "        if (box[2] * box[3]) < max_area:\n",
    "            clean_boxes.append(box)\n",
    "    if len(clean_boxes) < 2:\n",
    "        return boxes\n",
    "    return torch.FloatTensor(clean_boxes)\n",
    "\n",
    "def load_dino_model(model_size='swint'):\n",
    "    #choose swinb or swint\n",
    "    if model_size == 'swint':\n",
    "        config_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinT_OGC.py\"\n",
    "        checkpoint_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GroundingDINO\\weights\\groundingdino_swint_ogc.pth\"\n",
    "    elif model_size == 'swinb':\n",
    "        checkpoint_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GroundingDINO\\weights\\groundingdino_swinb_cogcoor.pth\"\n",
    "        config_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinB_cfg.py\"\n",
    "\n",
    "    model = load_model(config_path, checkpoint_path)\n",
    "    return model\n",
    "\n",
    "def run_dino_from_model(model, img_path, prompt, box_threshold, text_threshold, maxarea=0.7, save_dir=\"DINO-labels\"):\n",
    "    image_source, image = load_image(img_path)\n",
    "    boxes, accuracy, obj_name = predict(model=model, image=image, caption=prompt, box_threshold=box_threshold,\n",
    "                                        text_threshold=text_threshold)\n",
    "\n",
    "    #Convert boxes from YOLOv8 format to xyxy\n",
    "    img_height, img_width = cv2.imread(img_path).shape[:2]\n",
    "    clean_boxes = clean_labels(boxes, maxarea)\n",
    "    absolute_boxes = [[(box[0] - (box[2] / 2)) * img_width,\n",
    "                       (box[1] - (box[3] / 2)) * img_height,\n",
    "                       (box[0] + (box[2] / 2)) * img_width,\n",
    "                       (box[1] + (box[3] / 2)) * img_height] for box in clean_boxes.tolist()]\n",
    "    save_labels = True\n",
    "    if save_labels:\n",
    "        clean_boxes = clean_boxes.tolist()\n",
    "        for x in clean_boxes:\n",
    "            x.insert(0, 0)\n",
    "        with open(f'{save_dir}/{os.path.splitext(os.path.basename(img_path))[0]}.txt', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            writer.writerows(clean_boxes)\n",
    "    return absolute_boxes\n",
    "\n",
    "def save_masks(sam_results, output_dir):\n",
    "    segments = sam_results[0].masks.xyn\n",
    "    with open(f\"{Path(output_dir) / Path(sam_results[0].path).stem}.txt\", \"w\") as f:\n",
    "        for i in range(len(segments)):\n",
    "            s = segments[i]\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            segment = map(str, segments[i].reshape(-1).tolist())\n",
    "            f.write(f\"0 \" + \" \".join(segment) + \"\\n\")\n",
    "\n",
    "def run_image(DINO, img_dir, output_dir, prompt, conf, box_threshold, save_dir):\n",
    "    sam_model = \"sam2_t.pt\"\n",
    "    dino_model = \"swint\"\n",
    "    start = t.time()\n",
    "    fname = os.path.basename(img_dir)\n",
    "    path = img_dir\n",
    "    boxes = run_dino_from_model(DINO, img_dir, prompt, conf, 0.1, box_threshold, save_dir=save_dir)\n",
    "    model = SAM(sam_model)\n",
    "    sam_results = model(img_dir, model=sam_model, bboxes=boxes, verbose=False)\n",
    "    save_masks(sam_results, output_dir)\n",
    "\n",
    "    print(f\"Completed in: {t.time() - start} seconds, masks saved in {output_dir}\")\n",
    "    return sam_results\n",
    "\n",
    "def adjust_masks(sam_results):\n",
    "    result = sam_results[0]\n",
    "\n",
    "    masks = result.masks.data.cpu().numpy()  # masks, (N, H, W)\n",
    "    masks = np.moveaxis(masks, 0, -1)  # masks, (H, W, N)\n",
    "    masks = np.moveaxis(masks, -1, 0)  # masks, (N, H, W)\n",
    "\n",
    "    return masks\n",
    "\n",
    "def overlay_with_borders(image, mask, color, thickness=2):\n",
    "    # Convert mask to uint8 type\n",
    "    mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Draw contours on the image\n",
    "    cv2.drawContours(image, contours, -1, color, thickness)\n",
    "    return image\n",
    "\n",
    "def draw_boxes_on_image(image, boxes):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on the image using absolute coordinates.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The original image.\n",
    "        boxes (list): List of bounding boxes in the format [x1, y1, x2, y2].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image with bounding boxes drawn on it.\n",
    "    \"\"\"\n",
    "    # Convert the OpenCV image (BGR) to PIL for drawing\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    # Iterate over the list of boxes and draw them\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=(255, 0, 255), width=2)  # Drawing a rectangle with purple border\n",
    "\n",
    "    # Convert back to OpenCV format for display\n",
    "    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def optimize_prompts(prompts_file, gt_path, img_dir, save_file, threshold, DINO):\n",
    "    inf_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\DINO-labels\"\n",
    "    if not os.path.exists(inf_path):\n",
    "        try:\n",
    "            os.makedirs(inf_path)\n",
    "            print(f\"Directory '{inf_path}' created as it was missing.\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    with open(prompts_file, 'r') as file:\n",
    "        result_dict = {}\n",
    "        for x in file:\n",
    "            result_dict[x.strip()] = {}\n",
    "\n",
    "    # result_dict = dict.fromkeys(prompts,{})\n",
    "    for prompt in result_dict.keys():\n",
    "        print(f'Trying prompt: \"{prompt}\"')\n",
    "\n",
    "        box_threshold = 0.3\n",
    "        text_threshold = 0.1\n",
    "        model_size = 'swint'\n",
    "        run_dino_from_model(DINO, img_dir, prompt, box_threshold, text_threshold, maxarea=threshold)\n",
    "\n",
    "        metrics = process_file(inf_path, gt_path, threshold=threshold)\n",
    "\n",
    "        result_dict[prompt]['iou_scores'] = np.mean(metrics['iou_scores'])\n",
    "\n",
    "    results = sorted(list(result_dict.items()), key=lambda a: a[1]['iou_scores'], reverse=True)\n",
    "    print(results)\n",
    "\n",
    "    with open(save_file, 'w') as output:\n",
    "        for prompt_stats in results:\n",
    "            output.write(str(prompt_stats) + '\\n')\n",
    "\n",
    "    return results\n",
    "\n",
    "def calculate_metrics(tp, fp, fn, tn):\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    mcc = ((tp * tn) - (fp * fn)) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) \\\n",
    "        if np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n",
    "    return precision, recall, f1, mcc, specificity\n",
    "\n",
    "def read_and_draw_boxes(file_path, image_dim=(1280, 720)):\n",
    "    boxes = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            class_id, x, y, width, height = map(float, line.strip().split())\n",
    "            x1 = (x-(width/2))*image_dim[0]\n",
    "            x2 = (x+(width/2))*image_dim[0]\n",
    "            y1 = (y-(height/2))*image_dim[1]\n",
    "            y2 = (y+(height/2))*image_dim[1]\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "    image = Image.new('L', image_dim, 0)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box, fill=255)\n",
    "        #draw.rectangle([1,1,20,20], fill=255)\n",
    "    #image.save(\"test.jpg\")\n",
    "    return np.array(image, dtype=np.uint8)\n",
    "\n",
    "def clean_labels_from_file(file_path, cleaning_threshold=0.6):\n",
    "    # Read the file and check if it has more than one line\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    if len(lines) > 1:\n",
    "        accepted_lines = []\n",
    "\n",
    "        # Process each line\n",
    "        for line in lines:\n",
    "            class_id, x, y, width, height = map(float, line.strip().split())\n",
    "            # if width * height < 0.9:\n",
    "            if (width * height) < cleaning_threshold:\n",
    "                accepted_lines.append(line)\n",
    "\n",
    "        # Overwrite the file with accepted lines\n",
    "        with open(file_path, 'w') as f:\n",
    "            if len(accepted_lines) > 0:\n",
    "                for line in accepted_lines:\n",
    "                    f.write(line)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T23:17:17.405796Z",
     "start_time": "2025-03-25T23:17:17.393690Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "def draw_boxes(boxes, image_dim=(1280, 720)):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes directly from a list of absolute boxes.\n",
    "\n",
    "    Parameters:\n",
    "    boxes (list): List of absolute box coordinates in xyxy format.\n",
    "    image_dim (tuple): Dimensions of the output image (width, height).\n",
    "\n",
    "    Returns:\n",
    "    np.array: Binary image with boxes drawn.\n",
    "    \"\"\"\n",
    "    # Create a blank image to draw the boxes\n",
    "    image = Image.new('L', image_dim, 0)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Draw each box on the image\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box, fill=255)\n",
    "\n",
    "    return np.array(image, dtype=np.uint8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T23:17:17.936542Z",
     "start_time": "2025-03-25T23:17:17.933411Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "def prompt_optimizer(prompts_file, gt_path, img_path, save_file, threshold, DINO):\n",
    "    # Ensure inference path exists\n",
    "    inf_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\DINO-labels\"\n",
    "    os.makedirs(inf_path, exist_ok=True)\n",
    "\n",
    "    # Initialize result dictionary from prompt file\n",
    "    with open(prompts_file, 'r') as file:\n",
    "        result_dict = {x.strip(): {} for x in file}\n",
    "\n",
    "    # Process each prompt\n",
    "    for prompt in result_dict.keys():\n",
    "        print(f'Trying prompt: \"{prompt}\"')\n",
    "\n",
    "        # Run prediction and save labels\n",
    "        run_dino_from_model(DINO, img_path, prompt, box_threshold=0.3, text_threshold=0.1, maxarea=threshold)\n",
    "\n",
    "        # Process single predicted and ground truth file\n",
    "        predicted_mask_file = os.path.join(inf_path, f\"{os.path.splitext(os.path.basename(img_path))[0]}.txt\")\n",
    "        metrics = process_file(predicted_mask_file, gt_path, threshold)\n",
    "\n",
    "        # Save the IoU score for the prompt\n",
    "        result_dict[prompt]['iou_scores'] = np.mean(metrics['iou_scores'])\n",
    "\n",
    "    # Sort and save results\n",
    "    results = sorted(result_dict.items(), key=lambda a: a[1]['iou_scores'], reverse=True)\n",
    "    print(\"Results:\", results)\n",
    "\n",
    "    with open(save_file, 'w') as output:\n",
    "        for prompt_stats in results:\n",
    "            output.write(str(prompt_stats) + '\\n')\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_file(predicted_mask_file, ground_truth_mask_file, threshold):\n",
    "    # Initialize metrics dictionary\n",
    "    metrics = {\n",
    "        'iou_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': [],\n",
    "        'f1_scores': [],\n",
    "        'mcc_scores': [],\n",
    "        'specificity_scores': []\n",
    "    }\n",
    "\n",
    "    # Preprocess predicted mask\n",
    "    clean_labels_from_file(predicted_mask_file, threshold)\n",
    "    predicted_mask = read_and_draw_boxes(predicted_mask_file)\n",
    "    ground_truth_mask = read_and_draw_boxes(ground_truth_mask_file)\n",
    "\n",
    "    # Convert masks to binary\n",
    "    _, predicted_mask_bin = cv2.threshold(predicted_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "    _, ground_truth_mask_bin = cv2.threshold(ground_truth_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    predicted_mask_bin = predicted_mask_bin / 255\n",
    "    ground_truth_mask_bin = ground_truth_mask_bin / 255\n",
    "\n",
    "    # Calculate true positives, true negatives, false positives, and false negatives\n",
    "    tp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 1)))\n",
    "    tn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 0)))\n",
    "    fp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 0)))\n",
    "    fn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 1)))\n",
    "\n",
    "    # Calculate metrics\n",
    "    intersection = np.logical_and(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    union = np.logical_or(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    metrics['iou_scores'].append(np.sum(intersection) / np.sum(union))\n",
    "    # Calculate precision, recall, f1-score, MCC, and specificity\n",
    "    precision, recall, f1, mcc, specificity = calculate_metrics(tp, fp, fn, tn)\n",
    "    metrics['precision_scores'].append(precision)\n",
    "    metrics['recall_scores'].append(recall)\n",
    "    metrics['f1_scores'].append(f1)\n",
    "    metrics['mcc_scores'].append(mcc)\n",
    "    metrics['specificity_scores'].append(specificity)\n",
    "    #print(metrics['iou_scores'])\n",
    "    return metrics\n",
    "\n",
    "def process_mask_arrays(predicted_mask_array, ground_truth_mask_array):\n",
    "    # Resize predicted mask to match the ground truth mask's dimensions\n",
    "    if predicted_mask_array.shape != ground_truth_mask_array.shape:\n",
    "        predicted_mask_array = cv2.resize(predicted_mask_array, (ground_truth_mask_array.shape[1], ground_truth_mask_array.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Initialize metrics dictionary\n",
    "    metrics = {\n",
    "        'iou_scores': [],\n",
    "        #'pixel_accuracies': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': [],\n",
    "        'f1_scores': [],\n",
    "        'mcc_scores': [],\n",
    "        'specificity_scores': []\n",
    "    }\n",
    "\n",
    "    # Convert masks to binary based on threshold\n",
    "    _, predicted_mask_bin = cv2.threshold(predicted_mask_array, 127, 255, cv2.THRESH_BINARY)\n",
    "    _, ground_truth_mask_bin = cv2.threshold(ground_truth_mask_array, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Normalize binary masks for calculation\n",
    "    predicted_mask_bin = predicted_mask_bin / 255\n",
    "    ground_truth_mask_bin = ground_truth_mask_bin / 255\n",
    "\n",
    "    # Calculate true positives, true negatives, false positives, and false negatives\n",
    "    tp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 1)))\n",
    "    tn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 0)))\n",
    "    fp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 0)))\n",
    "    fn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 1)))\n",
    "\n",
    "    # Calculate IoU and pixel accuracy\n",
    "    intersection = np.logical_and(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    union = np.logical_or(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    metrics['iou_scores'].append(np.sum(intersection) / np.sum(union))\n",
    "    #metrics['pixel_accuracies'].append(pixel_accuracy(predicted_mask_bin, ground_truth_mask_bin))\n",
    "\n",
    "    # Calculate precision, recall, f1-score, MCC, and specificity\n",
    "    precision, recall, f1, mcc, specificity = calculate_metrics(tp, fp, fn, tn)\n",
    "    metrics['precision_scores'].append(precision)\n",
    "    metrics['recall_scores'].append(recall)\n",
    "    metrics['f1_scores'].append(f1)\n",
    "    metrics['mcc_scores'].append(mcc)\n",
    "    metrics['specificity_scores'].append(specificity)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def confidence_optimizer(prompt, DINO, gt_path, img_path, threshold):\n",
    "    inf_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\DINO-labels\"\n",
    "    os.makedirs(inf_path, exist_ok=True)\n",
    "\n",
    "    best_iou = 0\n",
    "    best_conf = 0\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    shape = image.shape\n",
    "\n",
    "    # Step 1: Precision 1 sweep (coarse) from 0.0 to 0.9 in steps of 0.1\n",
    "    for conf in np.arange(0.0, 0.91, 0.1):\n",
    "        box_threshold = conf\n",
    "        text_threshold = 0.1\n",
    "        boxes = run_dino_from_model(DINO, img_path, prompt, box_threshold, text_threshold)\n",
    "        pred_masks = draw_boxes(boxes, (shape[1], shape[0]))\n",
    "        gt_masks = read_and_draw_boxes(gt_path)\n",
    "\n",
    "        metrics = process_mask_arrays(pred_masks, gt_masks)\n",
    "        iou = np.mean(metrics['iou_scores'])\n",
    "        print('P1 rep')\n",
    "        print(f\"[Precision 1] Confidence: {conf:.1f}, IoU: {iou:.4f}\")\n",
    "\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_conf = conf\n",
    "\n",
    "    print(f\"Best from Precision 1: Confidence = {best_conf:.1f}, IoU = {best_iou:.4f}\")\n",
    "\n",
    "    # Step 2: Precision 2 sweep from (best_conf - 0.1) to (best_conf + 0.1) in steps of 0.01\n",
    "    lower = best_conf - 0.1\n",
    "    upper = best_conf + 0.1\n",
    "    step = 0.01\n",
    "\n",
    "    for conf in np.arange(lower, upper + step, step):\n",
    "        box_threshold = conf\n",
    "        text_threshold = 0.01\n",
    "        boxes = run_dino_from_model(DINO, img_path, prompt, box_threshold, text_threshold)\n",
    "        pred_masks = draw_boxes(boxes, (shape[1], shape[0]))\n",
    "        gt_masks = read_and_draw_boxes(gt_path)\n",
    "\n",
    "        metrics = process_mask_arrays(pred_masks, gt_masks)\n",
    "        iou = np.mean(metrics['iou_scores'])\n",
    "        print('P2 rep')\n",
    "        print(f\"[Precision 2] Confidence: {conf:.2f}, IoU: {iou:.4f}\")\n",
    "\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_conf = conf\n",
    "\n",
    "    print(f\"Final Best: Confidence = {best_conf:.2f}, IoU = {best_iou:.4f}\")\n",
    "    return best_iou, best_conf\n",
    "\n",
    "\n",
    "def multi_optimizer(img_dir, gt_label_dir, DINO, prompts, threshold=0.9):\n",
    "    start = t.time()\n",
    "    best_iou = 0\n",
    "    best_prompt = \"\"\n",
    "    best_conf = 0\n",
    "    for prompt in prompts:\n",
    "        print(f\"Trying prompt: '{prompt}'\")\n",
    "        iou, conf = confidence_optimizer(prompt, DINO, gt_label_dir, img_dir, threshold)\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_conf = conf\n",
    "            best_prompt = prompt\n",
    "        print(f\"So far: best prompt is '{best_prompt}', conf is {best_conf}, resulting in {best_iou} IOU)\")\n",
    "    print(f\"\\n\\n\\n\\n\\nFinal Result: best prompt is '{best_prompt}', conf is {best_conf}, resulting in {best_iou} IOU)\")\n",
    "    print(f\"final time: {t.time() - start}\")\n",
    "    return {\"prompt\": best_prompt, \"conf\": best_conf, \"iou\": best_iou}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T23:19:45.984762Z",
     "start_time": "2025-03-25T23:19:45.973714Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "def sort_largest_file(folder_path):\n",
    "    # Dictionary to store file names and their line counts\n",
    "    file_line_counts = {}\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file is a .txt file\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Open the file and count lines\n",
    "            with open(file_path, 'r') as file:\n",
    "                line_count = sum(1 for line in file)\n",
    "            # Add the file and line count to the dictionary\n",
    "            file_line_counts[file_name] = line_count\n",
    "        else:\n",
    "            print(\"File encountered not in .txt format.\")\n",
    "    # Sort files by line count in descending order and return as list of file names\n",
    "    sorted_files = sorted(file_line_counts, key=file_line_counts.get, reverse=True)\n",
    "    return sorted_files\n",
    "\n",
    "# Usage\n",
    "folder_path = r'C:/Users/cmull/DataspellProjects/AutoAnnotate/autoannotate study/berries-bounding-box-1/train/labels'\n",
    "image_folder_path = r'C:/Users/cmull/DataspellProjects/AutoAnnotate/autoannotate study/berries-bounding-box-1/train/images'\n",
    "sorted_txt_files = sort_largest_file(folder_path)\n",
    "print(\"Files sorted by line count:\", sorted_txt_files)\n",
    "reference_txt = folder_path + '\\\\' + sorted_txt_files[0]\n",
    "reference_image = image_folder_path + '\\\\' + sorted_txt_files[0].split(\".txt\")[0] + \".jpg\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T23:19:47.426757Z",
     "start_time": "2025-03-25T23:19:47.422330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files sorted by line count: ['IMG_9331_jpg.rf.20009327b80c55eec840b8b4f5cddf57.txt', 'IMG_9355_jpg.rf.40d4de298491188a33bcdfd995d9e855.txt', 'IMG_9379_jpg.rf.42c280b08420d4271486e3cdebe8a30e.txt', 'IMG_9394_jpg.rf.93cd662dac6324bfa4ef17b55494eaf7.txt', 'IMG_9383_jpg.rf.7af81e391f70df26bca8c741d75bcf24.txt', 'IMG_9387_jpg.rf.9ae726fc1ddc490013a19db8c1c2a1f1.txt']\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "box_threshold = 0.5\n",
    "DINO = load_dino_model()\n",
    "prompts_file = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\prompts\\blueberry-prompts.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T23:19:48.753838Z",
     "start_time": "2025-03-25T23:19:47.916397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "prompt_result = prompt_optimizer(prompts_file, reference_txt, reference_image, \"best.txt\", box_threshold, DINO)\n",
    "\n",
    "top_2 = prompt_result[:2]\n",
    "top2 = [result[0] for result in prompt_result][0:2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T23:19:57.119458Z",
     "start_time": "2025-03-25T23:19:48.907686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying prompt: \"blueberry\"\n",
      "Trying prompt: \"a blueberry\"\n",
      "Trying prompt: \"single blueberry\"\n",
      "Trying prompt: \"a single blueberry\"\n",
      "Trying prompt: \"a single, round blueberry\"\n",
      "Trying prompt: \"single, round blueberry\"\n",
      "Trying prompt: \"individual blueberry\"\n",
      "Trying prompt: \"an individual blueberry\"\n",
      "Trying prompt: \"one blueberries\"\n",
      "Trying prompt: \"small blue sphere\"\n",
      "Trying prompt: \"a small blue sphere\"\n",
      "Trying prompt: \"a small blue berry\"\n",
      "Trying prompt: \"a blueberry among a patch of green leaves\"\n",
      "Trying prompt: \"a wild blueberry\"\n",
      "Trying prompt: \"wild blueberry\"\n",
      "Trying prompt: \"single  wild blueberry\"\n",
      "Trying prompt: \"a single wild blueberry\"\n",
      "Trying prompt: \"a single, round  wild blueberry\"\n",
      "Trying prompt: \"blueberries\"\n",
      "Trying prompt: \"a blue berry\"\n",
      "Trying prompt: \"blue berry\"\n",
      "Trying prompt: \"a small blueberry\"\n",
      "Trying prompt: \"blueberry among a patch of green leaves\"\n",
      "Trying prompt: \"a round blueberry among a patch of green leaves\"\n",
      "Trying prompt: \"a blue sphere  among a patch of green leaves\"\n",
      "Trying prompt: \"blueberry in leaves\"\n",
      "Trying prompt: \"a blueberry in leaves\"\n",
      "Trying prompt: \"one individual blueberry\"\n",
      "Trying prompt: \"one blueberry\"\n",
      "Trying prompt: \"one round, blue berry\"\n",
      "Trying prompt: \"one round, blue sphere\"\n",
      "Trying prompt: \"one blueberry in a field of green leaves\"\n",
      "Trying prompt: \"blue sphere\"\n",
      "Trying prompt: \"a blue fruit\"\n",
      "Trying prompt: \"a blueberry in the foreground or background of a green field\"\n",
      "Trying prompt: \"blueberry in the background of a green field\"\n",
      "Trying prompt: \"a blueberry in the background of a green field\"\n",
      "Trying prompt: \"a blueberry in the background of a leafy field\"\n",
      "Trying prompt: \"blueberry in the background of a leafy field\"\n",
      "Trying prompt: \"Round Blueberry\"\n",
      "Trying prompt: \"Spherical Blueberry\"\n",
      "Trying prompt: \"Globular Blueberry\"\n",
      "Trying prompt: \"Plump Blueberry\"\n",
      "Trying prompt: \"Tiny Blueberry\"\n",
      "Trying prompt: \"Compact Blueberry\"\n",
      "Trying prompt: \"Smooth Blueberry\"\n",
      "Trying prompt: \"Egg-shaped Blueberry\"\n",
      "Trying prompt: \"Oval Blueberry\"\n",
      "Trying prompt: \"Perfectly-formed Blueberry\"\n",
      "Trying prompt: \"Circular Blueberry\"\n",
      "Trying prompt: \"Sphere-like Blueberry\"\n",
      "Trying prompt: \"Bulbous Blueberry\"\n",
      "Trying prompt: \"Curved Blueberry\"\n",
      "Trying prompt: \"Rounded Blueberry\"\n",
      "Trying prompt: \"Uniform Blueberry\"\n",
      "Trying prompt: \"Well-rounded Blueberry\"\n",
      "Trying prompt: \"Balanced Blueberry\"\n",
      "Trying prompt: \"Symmetrical Blueberry\"\n",
      "Trying prompt: \"Neat Blueberry\"\n",
      "Trying prompt: \"all blueberries\"\n",
      "Results: [('a single blueberry', {'iou_scores': 0.5484957341715312}), ('a small blueberry', {'iou_scores': 0.5469134618458489}), ('Globular Blueberry', {'iou_scores': 0.5459632294164668}), ('a small blue berry', {'iou_scores': 0.5436086170246518}), ('Spherical Blueberry', {'iou_scores': 0.5103877918440831}), ('individual blueberry', {'iou_scores': 0.5098564266180492}), ('a blueberry', {'iou_scores': 0.509624145785877}), ('Uniform Blueberry', {'iou_scores': 0.5091737891737892}), ('Curved Blueberry', {'iou_scores': 0.5081627438176088}), ('single blueberry', {'iou_scores': 0.5081314682133515}), ('a wild blueberry', {'iou_scores': 0.5078339708173962}), ('Bulbous Blueberry', {'iou_scores': 0.5073575365036077}), ('single  wild blueberry', {'iou_scores': 0.5070111328290984}), ('Balanced Blueberry', {'iou_scores': 0.5061651583710407}), ('small blue sphere', {'iou_scores': 0.5047110111095486}), ('a blue berry', {'iou_scores': 0.5041075849651137}), ('Egg-shaped Blueberry', {'iou_scores': 0.49831748354059985}), ('Rounded Blueberry', {'iou_scores': 0.49382250580046405}), ('Sphere-like Blueberry', {'iou_scores': 0.49310652964830537}), ('Smooth Blueberry', {'iou_scores': 0.4926214269114571}), ('Circular Blueberry', {'iou_scores': 0.47055951567229765}), ('Oval Blueberry', {'iou_scores': 0.4701368523949169}), ('Compact Blueberry', {'iou_scores': 0.4564541553551099}), ('wild blueberry', {'iou_scores': 0.4558460024062124}), ('Tiny Blueberry', {'iou_scores': 0.45562208040002633}), ('blueberry', {'iou_scores': 0.4490923744099474}), ('blue berry', {'iou_scores': 0.447848786250524}), ('blueberries', {'iou_scores': 0.43472537776335246}), ('Neat Blueberry', {'iou_scores': 0.42837909419969983}), ('Symmetrical Blueberry', {'iou_scores': 0.42685287002419176}), ('Plump Blueberry', {'iou_scores': 0.42642979791308944}), ('Round Blueberry', {'iou_scores': 0.42482303209184824}), ('blue sphere', {'iou_scores': 0.42447331077393297}), ('one blueberries', {'iou_scores': 0.4190859811882673}), ('a blue fruit', {'iou_scores': 0.41786984276594835}), ('all blueberries', {'iou_scores': 0.4178174822377569}), ('one blueberry', {'iou_scores': 0.4081824062095731}), ('an individual blueberry', {'iou_scores': 0.39342844790315606}), ('a single wild blueberry', {'iou_scores': 0.3922283743907695}), ('Well-rounded Blueberry', {'iou_scores': 0.39153998678122937}), ('a single, round blueberry', {'iou_scores': 0.3900641552886988}), ('single, round blueberry', {'iou_scores': 0.3900641552886988}), ('a single, round  wild blueberry', {'iou_scores': 0.3887129757899598}), ('Perfectly-formed Blueberry', {'iou_scores': 0.3367961357771455}), ('one individual blueberry', {'iou_scores': 0.3359947428946936}), ('a small blue sphere', {'iou_scores': 0.33490937746256894}), ('one round, blue berry', {'iou_scores': 0.3317058975685463}), ('a blueberry in the background of a leafy field', {'iou_scores': 0.32459335552345664}), ('a blueberry in the foreground or background of a green field', {'iou_scores': 0.3238002756546799}), ('blueberry among a patch of green leaves', {'iou_scores': 0.32355994588982917}), ('a blueberry in the background of a green field', {'iou_scores': 0.32279923804765326}), ('blueberry in the background of a green field', {'iou_scores': 0.3217261068988112}), ('blueberry in the background of a leafy field', {'iou_scores': 0.3217261068988112}), ('a round blueberry among a patch of green leaves', {'iou_scores': 0.12989801395598496}), ('one round, blue sphere', {'iou_scores': 0.1291952877531409}), ('a blueberry among a patch of green leaves', {'iou_scores': 0.12918149466192172}), ('one blueberry in a field of green leaves', {'iou_scores': 0.12846374349718653}), ('a blue sphere  among a patch of green leaves', {'iou_scores': 0.1273907703105808}), ('a blueberry in leaves', {'iou_scores': 0.055499333150145096}), ('blueberry in leaves', {'iou_scores': 0.05530917300062783})]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T23:19:57.128930Z",
     "start_time": "2025-03-25T23:19:57.125962Z"
    }
   },
   "cell_type": "code",
   "source": "print(top2)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a single blueberry', 'a small blueberry']\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "multi_optimizer(reference_image, reference_txt, DINO, top2, box_threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T23:20:05.512330Z",
     "start_time": "2025-03-25T23:19:57.209566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying prompt: 'a single blueberry'\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.0, IoU: 0.0297\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.1, IoU: 0.4341\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.2, IoU: 0.5795\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.3, IoU: 0.5478\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.4, IoU: 0.3398\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.5, IoU: 0.1316\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.6, IoU: 0.0000\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.7, IoU: 0.0000\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.8, IoU: 0.0000\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.9, IoU: 0.0000\n",
      "Best from Precision 1: Confidence = 0.2, IoU = 0.5795\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.10, IoU: 0.4341\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.11, IoU: 0.4341\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.12, IoU: 0.4345\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.13, IoU: 0.4476\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.14, IoU: 0.4476\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.15, IoU: 0.4830\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.16, IoU: 0.5470\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.17, IoU: 0.5553\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.18, IoU: 0.5256\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.19, IoU: 0.5256\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.20, IoU: 0.5795\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.21, IoU: 0.5098\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.22, IoU: 0.5098\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.23, IoU: 0.5098\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.24, IoU: 0.5617\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.25, IoU: 0.5617\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.26, IoU: 0.5617\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.27, IoU: 0.5617\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.28, IoU: 0.5478\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.29, IoU: 0.5478\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.30, IoU: 0.5478\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.31, IoU: 0.3937\n",
      "Final Best: Confidence = 0.20, IoU = 0.5795\n",
      "So far: best prompt is 'a single blueberry', conf is 0.2, resulting in 0.5795104004528089 IOU)\n",
      "Trying prompt: 'a small blueberry'\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.0, IoU: 0.0287\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.1, IoU: 0.4355\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.2, IoU: 0.5567\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.3, IoU: 0.5490\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.4, IoU: 0.3406\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.5, IoU: 0.1319\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.6, IoU: 0.0000\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.7, IoU: 0.0000\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.8, IoU: 0.0000\n",
      "P1 rep\n",
      "[Precision 1] Confidence: 0.9, IoU: 0.0000\n",
      "Best from Precision 1: Confidence = 0.2, IoU = 0.5567\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.10, IoU: 0.4355\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.11, IoU: 0.4355\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.12, IoU: 0.4355\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.13, IoU: 0.4357\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.14, IoU: 0.4493\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.15, IoU: 0.4493\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.16, IoU: 0.4851\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.17, IoU: 0.4851\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.18, IoU: 0.5490\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.19, IoU: 0.5490\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.20, IoU: 0.5567\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.21, IoU: 0.5807\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.22, IoU: 0.5553\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.23, IoU: 0.5108\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.24, IoU: 0.5108\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.25, IoU: 0.5629\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.26, IoU: 0.5629\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.27, IoU: 0.5490\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.28, IoU: 0.5490\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.29, IoU: 0.5490\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.30, IoU: 0.5490\n",
      "P2 rep\n",
      "[Precision 2] Confidence: 0.31, IoU: 0.5490\n",
      "Final Best: Confidence = 0.21, IoU = 0.5807\n",
      "So far: best prompt is 'a small blueberry', conf is 0.20999999999999996, resulting in 0.5806771778584392 IOU)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Final Result: best prompt is 'a small blueberry', conf is 0.20999999999999996, resulting in 0.5806771778584392 IOU)\n",
      "final time: 8.2997567653656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': 'a small blueberry',\n",
       " 'conf': 0.20999999999999996,\n",
       " 'iou': 0.5806771778584392}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T01:29:36.425508Z",
     "start_time": "2025-03-24T01:29:16.961589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "import re\n",
    "\n",
    "def extract_descriptions(response):\n",
    "    \"\"\"\n",
    "    Extracts only the clean descriptions from the model response, removing metadata, numbering, and unnecessary text.\n",
    "\n",
    "    :param response: The raw response from the model.\n",
    "    :return: A list of cleaned descriptions without numbering or unwanted text.\n",
    "    \"\"\"\n",
    "    lines = response.split(\"\\n\")  # Split response into lines\n",
    "    unwanted_keywords = [\"user\", \"assistant\", \"describe\", \"text & image output\"]  # Keywords to ignore\n",
    "\n",
    "    descriptions = []\n",
    "    for line in lines:\n",
    "        clean_line = line.strip()\n",
    "        if not clean_line:\n",
    "            continue  # Skip empty lines\n",
    "        if any(keyword in clean_line.lower() for keyword in unwanted_keywords):\n",
    "            continue  # Skip lines with unwanted keywords\n",
    "\n",
    "        # Remove leading numbering like \"1. \", \"2)\", \"3 - \", etc.\n",
    "        clean_line = re.sub(r\"^\\s*\\d+[\\.\\)\\-]\\s*\", \"\", clean_line)\n",
    "\n",
    "        if clean_line:  # Only add if the line still has content\n",
    "            descriptions.append(clean_line)\n",
    "\n",
    "    return descriptions\n",
    "# Load Model and Processor\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model.tie_weights()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad68ca2503054a6ba5449fa6ad7ec45e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T01:29:44.014909Z",
     "start_time": "2025-03-24T01:29:36.428511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Image\n",
    "image_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\bounding-buds-1\\train\\images\\IMG_1067_JPG_jpg.rf.a07350fa4e778c7ae999106ecc5dde24.jpg\"\n",
    "raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "manual_entry = input(\"object in image\")\n",
    "# Define Conversation Prompt (Corrected)\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},  # Note: No \"image\": raw_image here!\n",
    "            {\"type\": \"text\", \"text\": f\"Describe the {manual_entry} of the image in 3 words maximum for prompt use in a zero-shot detection model, and give 5 separate entries, each separated by a new line, and its own separate descriptor of the target. number each prompt. then simply new line. strictly the prompts, no other response is required. use visual description of the target in the image only. no prompts should be the same.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Convert to Text Prompt\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Process Inputs Correctly\n",
    "inputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\").to(model.device)  # Ensure correct parameter order"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T01:29:50.502823Z",
     "start_time": "2025-03-24T01:29:44.747418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate Output\n",
    "output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "# Decode and Print Output\n",
    "response = processor.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Text & Image Output:\", response)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text & Image Output: user\n",
      "\n",
      "Describe the blueberry buds of the image in 3 words maximum for prompt use in a zero-shot detection model, and give 5 separate entries, each separated by a new line, and its own separate descriptor of the target. number each prompt. then simply new line. strictly the prompts, no other response is required. use visual description of the target in the image only. no prompts should be the same.assistant\n",
      "\n",
      "1. Small greenish-red, closed flower-like structures at end of twigs\n",
      "2. Buds of small, closed flower-like structures\n",
      "3. Small greenish-red, flower-like structures on branches\n",
      "4. Small, greenish-red, closed flower-like structures on end of branches\n",
      "5. Closed flower-like structures on end of small branches\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T01:30:50.060305Z",
     "start_time": "2025-03-24T01:30:50.057506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extracted descriptions\n",
    "cleaned_descriptions = extract_descriptions(response)\n",
    "\n",
    "# Print results\n",
    "for prompt in cleaned_descriptions:\n",
    "    print(prompt)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small greenish-red, closed flower-like structures at end of twigs\n",
      "Buds of small, closed flower-like structures\n",
      "Small greenish-red, flower-like structures on branches\n",
      "Small, greenish-red, closed flower-like structures on end of branches\n",
      "Closed flower-like structures on end of small branches\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T01:30:53.489928Z",
     "start_time": "2025-03-24T01:30:53.487487Z"
    }
   },
   "cell_type": "code",
   "source": "print(cleaned_descriptions)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Small greenish-red, closed flower-like structures at end of twigs', 'Buds of small, closed flower-like structures', 'Small greenish-red, flower-like structures on branches', 'Small, greenish-red, closed flower-like structures on end of branches', 'Closed flower-like structures on end of small branches']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
