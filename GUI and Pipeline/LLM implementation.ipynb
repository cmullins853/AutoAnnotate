{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict\n",
    "import cv2\n",
    "import torch\n",
    "import csv\n",
    "from ultralytics import SAM\n",
    "from pathlib import Path\n",
    "import time as t\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def clean_labels(boxes, max_area):\n",
    "    clean_boxes = []\n",
    "    box_list = boxes.tolist()\n",
    "    for box in box_list:\n",
    "        # if width * height < 0.9, add box to list.\n",
    "        if (box[2] * box[3]) < max_area:\n",
    "            clean_boxes.append(box)\n",
    "    if len(clean_boxes) < 2:\n",
    "        return boxes\n",
    "    return torch.FloatTensor(clean_boxes)\n",
    "\n",
    "def load_dino_model(model_size='swint'):\n",
    "    #choose swinb or swint\n",
    "    if model_size == 'swint':\n",
    "        config_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinT_OGC.py\"\n",
    "        checkpoint_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\weights\\groundingdino_swint_ogc.pth\"\n",
    "    elif model_size == 'swinb':\n",
    "        checkpoint_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\weights\\groundingdino_swinb_cogcoor.pth\"\n",
    "        config_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinB_cfg.py\"\n",
    "\n",
    "    model = load_model(config_path, checkpoint_path)\n",
    "    return model\n",
    "\n",
    "def run_dino_from_model(model, img_path, prompt, box_threshold, text_threshold, maxarea=0.7, save_dir=\"DINO-labels\"):\n",
    "    image_source, image = load_image(img_path)\n",
    "    boxes, accuracy, obj_name = predict(model=model, image=image, caption=prompt, box_threshold=box_threshold,\n",
    "                                        text_threshold=text_threshold)\n",
    "\n",
    "    #Convert boxes from YOLOv8 format to xyxy\n",
    "    img_height, img_width = cv2.imread(img_path).shape[:2]\n",
    "    clean_boxes = clean_labels(boxes, maxarea)\n",
    "    absolute_boxes = [[(box[0] - (box[2] / 2)) * img_width,\n",
    "                       (box[1] - (box[3] / 2)) * img_height,\n",
    "                       (box[0] + (box[2] / 2)) * img_width,\n",
    "                       (box[1] + (box[3] / 2)) * img_height] for box in clean_boxes.tolist()]\n",
    "    save_labels = True\n",
    "    if save_labels:\n",
    "        clean_boxes = clean_boxes.tolist()\n",
    "        for x in clean_boxes:\n",
    "            x.insert(0, 0)\n",
    "        with open(f'{save_dir}/{os.path.splitext(os.path.basename(img_path))[0]}.txt', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            writer.writerows(clean_boxes)\n",
    "    return absolute_boxes\n",
    "\n",
    "def save_masks(sam_results, output_dir):\n",
    "    segments = sam_results[0].masks.xyn\n",
    "    with open(f\"{Path(output_dir) / Path(sam_results[0].path).stem}.txt\", \"w\") as f:\n",
    "        for i in range(len(segments)):\n",
    "            s = segments[i]\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            segment = map(str, segments[i].reshape(-1).tolist())\n",
    "            f.write(f\"0 \" + \" \".join(segment) + \"\\n\")\n",
    "\n",
    "def run_image(DINO, img_dir, output_dir, prompt, conf, box_threshold, save_dir):\n",
    "    sam_model = \"sam2_t.pt\"\n",
    "    dino_model = \"swint\"\n",
    "    start = t.time()\n",
    "    fname = os.path.basename(img_dir)\n",
    "    path = img_dir\n",
    "    boxes = run_dino_from_model(DINO, img_dir, prompt, conf, 0.1, box_threshold, save_dir=save_dir)\n",
    "    model = SAM(sam_model)\n",
    "    sam_results = model(img_dir, model=sam_model, bboxes=boxes, verbose=False)\n",
    "    save_masks(sam_results, output_dir)\n",
    "\n",
    "    print(f\"Completed in: {t.time() - start} seconds, masks saved in {output_dir}\")\n",
    "    return sam_results\n",
    "\n",
    "def adjust_masks(sam_results):\n",
    "    result = sam_results[0]\n",
    "\n",
    "    masks = result.masks.data.cpu().numpy()  # masks, (N, H, W)\n",
    "    masks = np.moveaxis(masks, 0, -1)  # masks, (H, W, N)\n",
    "    masks = np.moveaxis(masks, -1, 0)  # masks, (N, H, W)\n",
    "\n",
    "    return masks\n",
    "\n",
    "def overlay_with_borders(image, mask, color, thickness=2):\n",
    "    # Convert mask to uint8 type\n",
    "    mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Draw contours on the image\n",
    "    cv2.drawContours(image, contours, -1, color, thickness)\n",
    "    return image\n",
    "\n",
    "def draw_boxes_on_image(image, boxes):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on the image using absolute coordinates.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The original image.\n",
    "        boxes (list): List of bounding boxes in the format [x1, y1, x2, y2].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image with bounding boxes drawn on it.\n",
    "    \"\"\"\n",
    "    # Convert the OpenCV image (BGR) to PIL for drawing\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    # Iterate over the list of boxes and draw them\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=(255, 0, 255), width=2)  # Drawing a rectangle with purple border\n",
    "\n",
    "    # Convert back to OpenCV format for display\n",
    "    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def optimize_prompts(prompts_file, gt_path, img_dir, save_file, threshold, DINO):\n",
    "    inf_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\DINO-labels\"\n",
    "    if not os.path.exists(inf_path):\n",
    "        try:\n",
    "            os.makedirs(inf_path)\n",
    "            print(f\"Directory '{inf_path}' created as it was missing.\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    with open(prompts_file, 'r') as file:\n",
    "        result_dict = {}\n",
    "        for x in file:\n",
    "            result_dict[x.strip()] = {}\n",
    "\n",
    "    # result_dict = dict.fromkeys(prompts,{})\n",
    "    for prompt in result_dict.keys():\n",
    "        print(f'Trying prompt: \"{prompt}\"')\n",
    "\n",
    "        box_threshold = 0.3\n",
    "        text_threshold = 0.1\n",
    "        model_size = 'swint'\n",
    "        run_dino_from_model(DINO, img_dir, prompt, box_threshold, text_threshold, maxarea=threshold)\n",
    "\n",
    "        metrics = process_file(inf_path, gt_path, threshold=threshold)\n",
    "\n",
    "        result_dict[prompt]['iou_scores'] = np.mean(metrics['iou_scores'])\n",
    "\n",
    "    results = sorted(list(result_dict.items()), key=lambda a: a[1]['iou_scores'], reverse=True)\n",
    "    print(results)\n",
    "\n",
    "    with open(save_file, 'w') as output:\n",
    "        for prompt_stats in results:\n",
    "            output.write(str(prompt_stats) + '\\n')\n",
    "\n",
    "    return results\n",
    "\n",
    "def calculate_metrics(tp, fp, fn, tn):\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    mcc = ((tp * tn) - (fp * fn)) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) \\\n",
    "        if np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n",
    "    return precision, recall, f1, mcc, specificity\n",
    "\n",
    "def read_and_draw_boxes(file_path, image_dim=(1280, 720)):\n",
    "    boxes = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            class_id, x, y, width, height = map(float, line.strip().split())\n",
    "            x1 = (x-(width/2))*image_dim[0]\n",
    "            x2 = (x+(width/2))*image_dim[0]\n",
    "            y1 = (y-(height/2))*image_dim[1]\n",
    "            y2 = (y+(height/2))*image_dim[1]\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "    image = Image.new('L', image_dim, 0)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box, fill=255)\n",
    "        #draw.rectangle([1,1,20,20], fill=255)\n",
    "    #image.save(\"test.jpg\")\n",
    "    return np.array(image, dtype=np.uint8)\n",
    "\n",
    "def clean_labels_from_file(file_path, cleaning_threshold=0.6):\n",
    "    # Read the file and check if it has more than one line\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    if len(lines) > 1:\n",
    "        accepted_lines = []\n",
    "\n",
    "        # Process each line\n",
    "        for line in lines:\n",
    "            class_id, x, y, width, height = map(float, line.strip().split())\n",
    "            # if width * height < 0.9:\n",
    "            if (width * height) < cleaning_threshold:\n",
    "                accepted_lines.append(line)\n",
    "\n",
    "        # Overwrite the file with accepted lines\n",
    "        with open(file_path, 'w') as f:\n",
    "            if len(accepted_lines) > 0:\n",
    "                for line in accepted_lines:\n",
    "                    f.write(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def draw_boxes(boxes, image_dim=(1280, 720)):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes directly from a list of absolute boxes.\n",
    "\n",
    "    Parameters:\n",
    "    boxes (list): List of absolute box coordinates in xyxy format.\n",
    "    image_dim (tuple): Dimensions of the output image (width, height).\n",
    "\n",
    "    Returns:\n",
    "    np.array: Binary image with boxes drawn.\n",
    "    \"\"\"\n",
    "    # Create a blank image to draw the boxes\n",
    "    image = Image.new('L', image_dim, 0)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Draw each box on the image\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box, fill=255)\n",
    "\n",
    "    return np.array(image, dtype=np.uint8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def prompt_optimizer(prompts_file, gt_path, img_path, save_file, threshold, DINO):\n",
    "    # Ensure inference path exists\n",
    "    inf_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\DINO-labels\"\n",
    "    os.makedirs(inf_path, exist_ok=True)\n",
    "\n",
    "    # Initialize result dictionary from prompt file\n",
    "    with open(prompts_file, 'r') as file:\n",
    "        result_dict = {x.strip(): {} for x in file}\n",
    "\n",
    "    # Process each prompt\n",
    "    for prompt in result_dict.keys():\n",
    "        print(f'Trying prompt: \"{prompt}\"')\n",
    "\n",
    "        # Run prediction and save labels\n",
    "        run_dino_from_model(DINO, img_path, prompt, box_threshold=0.3, text_threshold=0.1, maxarea=threshold)\n",
    "\n",
    "        # Process single predicted and ground truth file\n",
    "        predicted_mask_file = os.path.join(inf_path, f\"{os.path.splitext(os.path.basename(img_path))[0]}.txt\")\n",
    "        metrics = process_file(predicted_mask_file, gt_path, threshold)\n",
    "\n",
    "        # Save the IoU score for the prompt\n",
    "        result_dict[prompt]['iou_scores'] = np.mean(metrics['iou_scores'])\n",
    "\n",
    "    # Sort and save results\n",
    "    results = sorted(result_dict.items(), key=lambda a: a[1]['iou_scores'], reverse=True)\n",
    "    print(\"Results:\", results)\n",
    "\n",
    "    with open(save_file, 'w') as output:\n",
    "        for prompt_stats in results:\n",
    "            output.write(str(prompt_stats) + '\\n')\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_file(predicted_mask_file, ground_truth_mask_file, threshold):\n",
    "    # Initialize metrics dictionary\n",
    "    metrics = {\n",
    "        'iou_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': [],\n",
    "        'f1_scores': [],\n",
    "        'mcc_scores': [],\n",
    "        'specificity_scores': []\n",
    "    }\n",
    "\n",
    "    # Preprocess predicted mask\n",
    "    clean_labels_from_file(predicted_mask_file, threshold)\n",
    "    predicted_mask = read_and_draw_boxes(predicted_mask_file)\n",
    "    ground_truth_mask = read_and_draw_boxes(ground_truth_mask_file)\n",
    "\n",
    "    # Convert masks to binary\n",
    "    _, predicted_mask_bin = cv2.threshold(predicted_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "    _, ground_truth_mask_bin = cv2.threshold(ground_truth_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    predicted_mask_bin = predicted_mask_bin / 255\n",
    "    ground_truth_mask_bin = ground_truth_mask_bin / 255\n",
    "\n",
    "    # Calculate true positives, true negatives, false positives, and false negatives\n",
    "    tp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 1)))\n",
    "    tn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 0)))\n",
    "    fp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 0)))\n",
    "    fn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 1)))\n",
    "\n",
    "    # Calculate metrics\n",
    "    intersection = np.logical_and(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    union = np.logical_or(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    metrics['iou_scores'].append(np.sum(intersection) / np.sum(union))\n",
    "    # Calculate precision, recall, f1-score, MCC, and specificity\n",
    "    precision, recall, f1, mcc, specificity = calculate_metrics(tp, fp, fn, tn)\n",
    "    metrics['precision_scores'].append(precision)\n",
    "    metrics['recall_scores'].append(recall)\n",
    "    metrics['f1_scores'].append(f1)\n",
    "    metrics['mcc_scores'].append(mcc)\n",
    "    metrics['specificity_scores'].append(specificity)\n",
    "    #print(metrics['iou_scores'])\n",
    "    return metrics\n",
    "\n",
    "def process_mask_arrays(predicted_mask_array, ground_truth_mask_array):\n",
    "    # Resize predicted mask to match the ground truth mask's dimensions\n",
    "    if predicted_mask_array.shape != ground_truth_mask_array.shape:\n",
    "        predicted_mask_array = cv2.resize(predicted_mask_array, (ground_truth_mask_array.shape[1], ground_truth_mask_array.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Initialize metrics dictionary\n",
    "    metrics = {\n",
    "        'iou_scores': [],\n",
    "        'pixel_accuracies': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': [],\n",
    "        'f1_scores': [],\n",
    "        'mcc_scores': [],\n",
    "        'specificity_scores': []\n",
    "    }\n",
    "\n",
    "    # Convert masks to binary based on threshold\n",
    "    _, predicted_mask_bin = cv2.threshold(predicted_mask_array, 127, 255, cv2.THRESH_BINARY)\n",
    "    _, ground_truth_mask_bin = cv2.threshold(ground_truth_mask_array, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Normalize binary masks for calculation\n",
    "    predicted_mask_bin = predicted_mask_bin / 255\n",
    "    ground_truth_mask_bin = ground_truth_mask_bin / 255\n",
    "\n",
    "    # Calculate true positives, true negatives, false positives, and false negatives\n",
    "    tp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 1)))\n",
    "    tn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 0)))\n",
    "    fp = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 0)))\n",
    "    fn = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 1)))\n",
    "\n",
    "    # Calculate IoU and pixel accuracy\n",
    "    intersection = np.logical_and(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    union = np.logical_or(predicted_mask_bin, ground_truth_mask_bin)\n",
    "    metrics['iou_scores'].append(np.sum(intersection) / np.sum(union))\n",
    "    metrics['pixel_accuracies'].append(pixel_accuracy(predicted_mask_bin, ground_truth_mask_bin))\n",
    "\n",
    "    # Calculate precision, recall, f1-score, MCC, and specificity\n",
    "    precision, recall, f1, mcc, specificity = calculate_metrics(tp, fp, fn, tn)\n",
    "    metrics['precision_scores'].append(precision)\n",
    "    metrics['recall_scores'].append(recall)\n",
    "    metrics['f1_scores'].append(f1)\n",
    "    metrics['mcc_scores'].append(mcc)\n",
    "    metrics['specificity_scores'].append(specificity)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def confidence_optimizer(prompt, DINO, gt_path, img_path, threshold):\n",
    "    inf_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\DINO-labels\"\n",
    "    os.makedirs(inf_path, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    best_iou = 0\n",
    "    best_conf = 0\n",
    "    final_precision = 5  # Number of decimal points in confidence\n",
    "    ubound = 0.9\n",
    "    lbound = 0.0\n",
    "    image = cv2.imread(img_path)\n",
    "    shape = image.shape\n",
    "    # Loop over precision levels\n",
    "    for precision in range(1, final_precision + 1):\n",
    "        esc = 0  # Escape counter to break if no improvement\n",
    "\n",
    "        # Range of confidence thresholds for the current precision level\n",
    "        for conf in [x / (10 ** precision) for x in range(int(lbound * (10 ** precision)), int(ubound * (10 ** precision)))]:\n",
    "            # Run the model with the current confidence threshold\n",
    "            box_threshold = conf\n",
    "            text_threshold = 0.01\n",
    "            boxes = run_dino_from_model(DINO, img_path, prompt, box_threshold, text_threshold)\n",
    "            pred_masks = draw_boxes(boxes, (shape[1], shape[0]))\n",
    "            gt_masks = read_and_draw_boxes(gt_path)\n",
    "            # Process the predicted and ground truth files for IoU calculation\n",
    "            #predicted_mask_file = os.path.join(inf_path, f\"{os.path.splitext(os.path.basename(img_path))[0]}.txt\")\n",
    "            #metrics = process_file(predicted_mask_file, gt_path, threshold)\n",
    "\n",
    "            metrics = process_mask_arrays(pred_masks, gt_masks)\n",
    "            iou = np.mean(metrics['iou_scores'])\n",
    "            # Update the best IoU and confidence threshold if the current IoU is higher\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_conf = conf\n",
    "                esc = 0  # Reset escape counter if improvement found\n",
    "            else:\n",
    "                esc += 1\n",
    "                # Exit loop early if no improvement found for multiple tries\n",
    "                if esc > 2 * precision:\n",
    "                    break\n",
    "\n",
    "            print(f\"Confidence: {conf}, IoU: {iou} (Best IoU: {best_iou})\")\n",
    "\n",
    "        print(f\"Best IoU at precision {precision} is {best_iou} with confidence = {best_conf}\")\n",
    "\n",
    "        # Adjust bounds based on best confidence found at the current precision level\n",
    "        lbound = max(0, best_conf - (1 / (10 ** precision)))\n",
    "        ubound = min(0.9, best_conf + (1 / (10 ** precision)))\n",
    "\n",
    "        # Early exit condition based on best confidence stability\n",
    "        if (best_conf > (0.2 * (10 ** precision))) and precision >= 2:\n",
    "            print(f\"Final Result: Best IoU is {best_iou} with confidence = {best_conf}\")\n",
    "            return best_iou, best_conf\n",
    "\n",
    "    return best_iou, best_conf\n",
    "\n",
    "def multi_optimizer(img_dir, gt_label_dir, DINO, prompts, threshold=0.9):\n",
    "    start = t.time()\n",
    "    best_iou = 0\n",
    "    best_prompt = \"\"\n",
    "    best_conf = 0\n",
    "    for prompt in prompts:\n",
    "        print(f\"Trying prompt: '{prompt}'\")\n",
    "        iou, conf = confidence_optimizer(prompt, DINO, gt_label_dir, img_dir, threshold)\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_conf = conf\n",
    "            best_prompt = prompt\n",
    "        print(f\"So far: best prompt is '{best_prompt}', conf is {best_conf}, resulting in {best_iou} IOU)\")\n",
    "    print(f\"\\n\\n\\n\\n\\nFinal Result: best prompt is '{best_prompt}', conf is {best_conf}, resulting in {best_iou} IOU)\")\n",
    "    print(f\"final time: {t.time() - start}\")\n",
    "    return {\"prompt\": best_prompt, \"conf\": best_conf, \"iou\": best_iou}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files sorted by line count: ['IMG_9355_jpg.rf.40d4de298491188a33bcdfd995d9e855.txt', 'IMG_9379_jpg.rf.42c280b08420d4271486e3cdebe8a30e.txt', 'IMG_9331_jpg.rf.20009327b80c55eec840b8b4f5cddf57.txt', 'IMG_9383_jpg.rf.7af81e391f70df26bca8c741d75bcf24.txt', 'IMG_9387_jpg.rf.9ae726fc1ddc490013a19db8c1c2a1f1.txt', 'IMG_9394_jpg.rf.93cd662dac6324bfa4ef17b55494eaf7.txt']\n"
     ]
    }
   ],
   "source": [
    "def sort_largest_file(folder):\n",
    "    # Dictionary to store file names and their line counts\n",
    "    file_line_counts = {}\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file is a .txt file\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Open the file and count lines\n",
    "            with open(file_path, 'r') as file:\n",
    "                line_count = sum(1 for line in file)\n",
    "            # Add the file and line count to the dictionary\n",
    "            file_line_counts[file_name] = line_count\n",
    "        else:\n",
    "            print(\"File encountered not in .txt format.\")\n",
    "    # Sort files by line count in descending order and return as list of file names\n",
    "    sorted_files = sorted(file_line_counts, key=file_line_counts.get, reverse=True)\n",
    "    return sorted_files\n",
    "\n",
    "# Usage\n",
    "folder_path = r'C:\\Users\\Mechanized Systems\\DataspellProjects\\AutoAnnotate\\autoannotate study\\berries-bounding-box-1\\train\\gen labels'\n",
    "image_folder_path = r'C:\\Users\\Mechanized Systems\\DataspellProjects\\AutoAnnotate\\autoannotate study\\berries-bounding-box-1\\train\\images'\n",
    "sorted_txt_files = sort_largest_file(folder_path)\n",
    "print(\"Files sorted by line count:\", sorted_txt_files)\n",
    "reference_txt = folder_path + '\\\\' + sorted_txt_files[0]\n",
    "reference_image = image_folder_path + '\\\\' + sorted_txt_files[0].split(\".txt\")[0] + \".jpg\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "box_threshold = 0.9\n",
    "DINO = load_dino_model()\n",
    "prompts_file = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\blueberry-prompts.txt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying prompt: \"blueberry\"\n",
      "Trying prompt: \"a blueberry\"\n",
      "Trying prompt: \"single blueberry\"\n",
      "Trying prompt: \"a single blueberry\"\n",
      "Trying prompt: \"a single, round blueberry\"\n",
      "Trying prompt: \"single, round blueberry\"\n",
      "Trying prompt: \"individual blueberry\"\n",
      "Trying prompt: \"an individual blueberry\"\n",
      "Trying prompt: \"one blueberries\"\n",
      "Trying prompt: \"small blue sphere\"\n",
      "Trying prompt: \"a small blue sphere\"\n",
      "Trying prompt: \"a small blue berry\"\n",
      "Trying prompt: \"a blueberry among a patch of green leaves\"\n",
      "Trying prompt: \"a wild blueberry\"\n",
      "Trying prompt: \"wild blueberry\"\n",
      "Trying prompt: \"single  wild blueberry\"\n",
      "Trying prompt: \"a single wild blueberry\"\n",
      "Trying prompt: \"a single, round  wild blueberry\"\n",
      "Trying prompt: \"blueberries\"\n",
      "Trying prompt: \"a blue berry\"\n",
      "Trying prompt: \"blue berry\"\n",
      "Trying prompt: \"a small blueberry\"\n",
      "Trying prompt: \"blueberry among a patch of green leaves\"\n",
      "Trying prompt: \"a round blueberry among a patch of green leaves\"\n",
      "Trying prompt: \"a blue sphere  among a patch of green leaves\"\n",
      "Trying prompt: \"blueberry in leaves\"\n",
      "Trying prompt: \"a blueberry in leaves\"\n",
      "Trying prompt: \"one individual blueberry\"\n",
      "Trying prompt: \"one blueberry\"\n",
      "Trying prompt: \"one round, blue berry\"\n",
      "Trying prompt: \"one round, blue sphere\"\n",
      "Trying prompt: \"one blueberry in a field of green leaves\"\n",
      "Trying prompt: \"blue sphere\"\n",
      "Trying prompt: \"a blue fruit\"\n",
      "Trying prompt: \"a blueberry in the foreground or background of a green field\"\n",
      "Trying prompt: \"blueberry in the background of a green field\"\n",
      "Trying prompt: \"a blueberry in the background of a green field\"\n",
      "Trying prompt: \"a blueberry in the background of a leafy field\"\n",
      "Trying prompt: \"blueberry in the background of a leafy field\"\n",
      "Trying prompt: \"Round Blueberry\"\n",
      "Trying prompt: \"Spherical Blueberry\"\n",
      "Trying prompt: \"Globular Blueberry\"\n",
      "Trying prompt: \"Plump Blueberry\"\n",
      "Trying prompt: \"Tiny Blueberry\"\n",
      "Trying prompt: \"Compact Blueberry\"\n",
      "Trying prompt: \"Smooth Blueberry\"\n",
      "Trying prompt: \"Egg-shaped Blueberry\"\n",
      "Trying prompt: \"Oval Blueberry\"\n",
      "Trying prompt: \"Perfectly-formed Blueberry\"\n",
      "Trying prompt: \"Circular Blueberry\"\n",
      "Trying prompt: \"Sphere-like Blueberry\"\n",
      "Trying prompt: \"Bulbous Blueberry\"\n",
      "Trying prompt: \"Curved Blueberry\"\n",
      "Trying prompt: \"Rounded Blueberry\"\n",
      "Trying prompt: \"Uniform Blueberry\"\n",
      "Trying prompt: \"Well-rounded Blueberry\"\n",
      "Trying prompt: \"Balanced Blueberry\"\n",
      "Trying prompt: \"Symmetrical Blueberry\"\n",
      "Trying prompt: \"Neat Blueberry\"\n",
      "Trying prompt: \"all blueberries\"\n",
      "Results: [('blue sphere', {'iou_scores': 0.9802219210570896}), ('blueberries', {'iou_scores': 0.9769146678670182}), ('Neat Blueberry', {'iou_scores': 0.9758431305243285}), ('blueberry', {'iou_scores': 0.9727292248228473}), ('wild blueberry', {'iou_scores': 0.9680983437741354}), ('blue berry', {'iou_scores': 0.9625805089219723}), ('Spherical Blueberry', {'iou_scores': 0.9560951522177852}), ('Symmetrical Blueberry', {'iou_scores': 0.9558148513577281}), ('Globular Blueberry', {'iou_scores': 0.9550202698354818}), ('Uniform Blueberry', {'iou_scores': 0.954891572467343}), ('Compact Blueberry', {'iou_scores': 0.9538190943995195}), ('Plump Blueberry', {'iou_scores': 0.9525741766499206}), ('Circular Blueberry', {'iou_scores': 0.9498475675211473}), ('single blueberry', {'iou_scores': 0.9496758126153978}), ('Balanced Blueberry', {'iou_scores': 0.9475166410650282}), ('Rounded Blueberry', {'iou_scores': 0.9469921422130619}), ('Oval Blueberry', {'iou_scores': 0.9466271630383443}), ('Curved Blueberry', {'iou_scores': 0.9461929367553487}), ('a wild blueberry', {'iou_scores': 0.9460413946619294}), ('single  wild blueberry', {'iou_scores': 0.9459401434153463}), ('individual blueberry', {'iou_scores': 0.9456610416935034}), ('Smooth Blueberry', {'iou_scores': 0.9448666752544119}), ('Round Blueberry', {'iou_scores': 0.9427325081657212}), ('a blue fruit', {'iou_scores': 0.9420188391038696}), ('Tiny Blueberry', {'iou_scores': 0.9412812915968912}), ('all blueberries', {'iou_scores': 0.9403313514314128}), ('a blue berry', {'iou_scores': 0.9397300716997048}), ('Bulbous Blueberry', {'iou_scores': 0.939649619992271}), ('a blueberry', {'iou_scores': 0.9389515172063062}), ('Egg-shaped Blueberry', {'iou_scores': 0.9347816744026131}), ('a small blue berry', {'iou_scores': 0.9303079416531604}), ('a small blueberry', {'iou_scores': 0.9162432739511651}), ('small blue sphere', {'iou_scores': 0.8914026189010802}), ('an individual blueberry', {'iou_scores': 0.877675710318308}), ('one blueberry', {'iou_scores': 0.8669324747873179}), ('a single blueberry', {'iou_scores': 0.8184789386543961}), ('blueberry in leaves', {'iou_scores': 0.7884591260715033}), ('Sphere-like Blueberry', {'iou_scores': 0.7813633145235694}), ('Well-rounded Blueberry', {'iou_scores': 0.7628978439425051}), ('a single wild blueberry', {'iou_scores': 0.714744069645569}), ('Perfectly-formed Blueberry', {'iou_scores': 0.6888045540796964}), ('one round, blue berry', {'iou_scores': 0.6686425482544035}), ('single, round blueberry', {'iou_scores': 0.6380623280220132}), ('one blueberries', {'iou_scores': 0.5761079249217935}), ('a small blue sphere', {'iou_scores': 0.4079842539899876}), ('a blueberry in leaves', {'iou_scores': 0.400414007981391}), ('one individual blueberry', {'iou_scores': 0.3750885098165433}), ('a single, round blueberry', {'iou_scores': 0.3632141398069108}), ('a single, round  wild blueberry', {'iou_scores': 0.32246423508184047}), ('a blueberry in the background of a green field', {'iou_scores': 0.31759002476871945}), ('a blueberry in the background of a leafy field', {'iou_scores': 0.31598111649766075}), ('a blueberry in the foreground or background of a green field', {'iou_scores': 0.315473548496185}), ('one round, blue sphere', {'iou_scores': 0.31041608965208684}), ('one blueberry in a field of green leaves', {'iou_scores': 0.2223174678010415}), ('blueberry in the background of a green field', {'iou_scores': 0.220645907892763}), ('blueberry among a patch of green leaves', {'iou_scores': 0.21827705386085053}), ('blueberry in the background of a leafy field', {'iou_scores': 0.2179215156775756}), ('a blueberry among a patch of green leaves', {'iou_scores': 0.21372603739561644}), ('a blue sphere  among a patch of green leaves', {'iou_scores': 0.21287128712871287}), ('a round blueberry among a patch of green leaves', {'iou_scores': 0.2120945409269476})]\n"
     ]
    }
   ],
   "source": [
    "prompt_result = prompt_optimizer(prompts_file, reference_txt, reference_image, \"best.txt\", box_threshold, DINO)\n",
    "\n",
    "top_2 = prompt_result[:2]\n",
    "top2 = [result[0] for result in prompt_result][0:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying prompt: 'blue sphere'\n",
      "Confidence: 0.0, IoU: 0.05072426621759726 (Best IoU: 0.05072426621759726)\n",
      "Confidence: 0.1, IoU: 0.4112817961316002 (Best IoU: 0.4112817961316002)\n",
      "Confidence: 0.2, IoU: 0.927526649034223 (Best IoU: 0.927526649034223)\n",
      "Confidence: 0.3, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.4, IoU: 0.2884772074274197 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.5, IoU: 0.10518292682926829 (Best IoU: 0.9714591509097396)\n",
      "Best IoU at precision 1 is 0.9714591509097396 with confidence = 0.3\n",
      "Confidence: 0.2, IoU: 0.927526649034223 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.21, IoU: 0.927526649034223 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.22, IoU: 0.940243332723986 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.23, IoU: 0.940243332723986 (Best IoU: 0.9714591509097396)\n",
      "Best IoU at precision 2 is 0.9714591509097396 with confidence = 0.3\n",
      "Confidence: 0.29, IoU: 0.959717211925198 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.291, IoU: 0.959717211925198 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.292, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.293, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.294, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.295, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Best IoU at precision 3 is 0.9714591509097396 with confidence = 0.3\n",
      "Confidence: 0.299, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.2991, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.2992, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.2993, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.2994, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.2995, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.2996, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.2997, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Best IoU at precision 4 is 0.9714591509097396 with confidence = 0.3\n",
      "Confidence: 0.2999, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.29991, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.29992, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.29993, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.29994, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.29995, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.29996, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.29997, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.29998, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Confidence: 0.29999, IoU: 0.9714591509097396 (Best IoU: 0.9714591509097396)\n",
      "Best IoU at precision 5 is 0.9714591509097396 with confidence = 0.3\n",
      "So far: best prompt is 'blue sphere', conf is 0.3, resulting in 0.9714591509097396 IOU)\n",
      "Trying prompt: 'blueberries'\n",
      "Confidence: 0.0, IoU: 0.050738820592714784 (Best IoU: 0.050738820592714784)\n",
      "Confidence: 0.1, IoU: 0.13405422872079537 (Best IoU: 0.13405422872079537)\n",
      "Confidence: 0.2, IoU: 0.5050942963537287 (Best IoU: 0.5050942963537287)\n",
      "Confidence: 0.3, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.4, IoU: 0.7746642063253658 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.5, IoU: 0.04438041585137721 (Best IoU: 0.9682576954275712)\n",
      "Best IoU at precision 1 is 0.9682576954275712 with confidence = 0.3\n",
      "Confidence: 0.2, IoU: 0.5050942963537287 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.21, IoU: 0.5050942963537287 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.22, IoU: 0.5050942963537287 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.23, IoU: 0.5050942963537287 (Best IoU: 0.9682576954275712)\n",
      "Best IoU at precision 2 is 0.9682576954275712 with confidence = 0.3\n",
      "Confidence: 0.29, IoU: 0.9545043244039477 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.291, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.292, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.293, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.294, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.295, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Best IoU at precision 3 is 0.9682576954275712 with confidence = 0.3\n",
      "Confidence: 0.299, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.2991, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.2992, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.2993, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.2994, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.2995, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.2996, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.2997, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Best IoU at precision 4 is 0.9682576954275712 with confidence = 0.3\n",
      "Confidence: 0.2999, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.29991, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.29992, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.29993, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.29994, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.29995, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.29996, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.29997, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.29998, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Confidence: 0.29999, IoU: 0.9682576954275712 (Best IoU: 0.9682576954275712)\n",
      "Best IoU at precision 5 is 0.9682576954275712 with confidence = 0.3\n",
      "So far: best prompt is 'blue sphere', conf is 0.3, resulting in 0.9714591509097396 IOU)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Final Result: best prompt is 'blue sphere', conf is 0.3, resulting in 0.9714591509097396 IOU)\n",
      "final time: 30.39535427093506\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'prompt': 'blue sphere', 'conf': 0.3, 'iou': 0.9714591509097396}"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_optimizer(reference_image, reference_txt, DINO, top2, box_threshold)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_automatic_annotation():\n",
    "    message_box = QtWidgets.QMessageBox()\n",
    "\n",
    "    prompt_result = optimize_prompts(self.prompts_list_label.text(), self.labelled_folder_label.text()+\"\\\\labels\", self.labelled_folder_label.text()+\"\\\\images\", \"best.txt\", 0.8 )\n",
    "    top_10 = prompt_result[:5]\n",
    "    multi_optimize(self.labelled_folder_label.text()+\"\\\\labels\", self.labelled_folder_label.text()+\"\\\\images\", \"swint\", top10, )\n",
    "    # Then run using the optimized parameters\n",
    "\n",
    "\n",
    "    message_box.setStyleSheet(\"QLabel { color: black; font-size: 24px; } QMessageBox { background-color: white; }\")\n",
    "    message_box.setText(\"Annotations saved to the output folder.\")\n",
    "    message_box.exec_()\n",
    "\n",
    "    def prompt_selection(self):\n",
    "        if not hasattr(self, \"prompt_buttons_added\"):\n",
    "            list_prompts_btn = QtWidgets.QPushButton(\"List of Prompts\")\n",
    "            list_prompts_btn.setStyleSheet(\"background-color: #4f82ff; color: white; font-size: 24px;\")\n",
    "            list_prompts_btn.setFixedSize(400, 100)\n",
    "            list_prompts_btn.clicked.connect(self.handle_list_of_prompts)\n",
    "            self.right_layout.addWidget(list_prompts_btn, alignment=QtCore.Qt.AlignTop)\n",
    "\n",
    "            generate_prompts_btn = QtWidgets.QPushButton(\"Generate Prompts\")\n",
    "            generate_prompts_btn.setStyleSheet(\"background-color: #4f82ff; color: white; font-size: 24px;\")\n",
    "            generate_prompts_btn.setFixedSize(400, 100)\n",
    "            generate_prompts_btn.clicked.connect(self.handle_generate_prompts)\n",
    "            self.right_layout.addWidget(generate_prompts_btn, alignment=QtCore.Qt.AlignTop)\n",
    "\n",
    "            self.prompt_buttons_added = True\n",
    "\n",
    "    def handle_list_of_prompts(self):\n",
    "        message_box = QtWidgets.QMessageBox()\n",
    "        message_box.setStyleSheet(\"QLabel { color: black; font-size: 24px; } QMessageBox { background-color: white; }\")\n",
    "        options = QtWidgets.QFileDialog.Options()\n",
    "        options |= QtWidgets.QFileDialog.DontUseNativeDialog\n",
    "        dialog = QtWidgets.QFileDialog(self, \"Select Prompts List\", \"\", \"Image Files (*.txt *.csv)\", options=options)\n",
    "        dialog.setStyleSheet(\"QWidget { background-color: white; color: black; }\")\n",
    "        if dialog.exec_() == QtWidgets.QDialog.Accepted:\n",
    "            sample_image_path = dialog.selectedFiles()[0]\n",
    "            if sample_image_path:\n",
    "                prompt = \"a sample prompt\"  # Update this as needed\n",
    "                prompts = generate_prompts(sample_image_path, prompt)\n",
    "                if prompts:\n",
    "                    message_box = QtWidgets.QMessageBox()\n",
    "                    message_box.setStyleSheet(\"QLabel { color: black; font-size: 24px; } QMessageBox { background-color: white; }\")\n",
    "                    message_box.setText(\"Selected list of prompts.\")\n",
    "                    message_box.exec_()\n",
    "\n",
    "    def handle_generate_prompts(self):\n",
    "        options = QtWidgets.QFileDialog.Options()\n",
    "        options |= QtWidgets.QFileDialog.DontUseNativeDialog\n",
    "        dialog = QtWidgets.QFileDialog(self, \"Select Sample Image\", \"\", \"Image Files (*.png *.jpg *.jpeg)\", options=options)\n",
    "        dialog.setStyleSheet(\"QWidget { background-color: white; color: black; }\")\n",
    "        if dialog.exec_() == QtWidgets.QDialog.Accepted:\n",
    "            sample_image_path = dialog.selectedFiles()[0]\n",
    "            if sample_image_path:\n",
    "                prompt = \"a sample prompt\"  # Update this as needed\n",
    "                prompts = generate_prompts(sample_image_path, prompt)\n",
    "                if prompts:\n",
    "                    message_box = QtWidgets.QMessageBox()\n",
    "                    message_box.setStyleSheet(\"QLabel { color: black; font-size: 24px; } QMessageBox { background-color: white; }\")\n",
    "                    message_box.setText(\"\\n\".join(prompts))\n",
    "                    message_box.exec_()\n",
    "\n",
    "    def generate_prompts(self, image_path, prompt):\n",
    "        # This is a placeholder for the actual logic to generate prompts\n",
    "        return [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T14:21:53.824218Z",
     "start_time": "2025-03-20T14:21:50.178937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model and processor\n",
    "model_name = \"llava-hf/llava-1.5-7b\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "def generate_image_descriptions(image_path, output_format=\"newline\"):\n",
    "    \"\"\"\n",
    "    Generates ten short descriptions of an image (max 5 words each).\n",
    "\n",
    "    :param image_path: Path to the image file.\n",
    "    :param output_format: \"newline\" or \"comma\".\n",
    "    :return: String with short descriptions.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    prompt = \"Describe this image in ten short phrases, five words maximum each.\"\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=100)\n",
    "\n",
    "    text_descriptions = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Extract and limit to 10 concise descriptions\n",
    "    descriptions = [desc.strip() for desc in text_descriptions.split(\". \") if desc]\n",
    "    descriptions = [desc.split(\" \")[:5] for desc in descriptions]  # Limit words per entry\n",
    "    descriptions = [\" \".join(desc) for desc in descriptions[:10]]\n",
    "\n",
    "    return \"\\n\".join(descriptions) if output_format == \"newline\" else \", \".join(descriptions)\n",
    "\n",
    "# Example usage\n",
    "image_path = \"example.jpg\"\n",
    "print(generate_image_descriptions(image_path, output_format=\"newline\"))\n"
   ],
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "llava-hf/llava-1.5-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 304\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\requests\\models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[1;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/llava-hf/llava-1.5-7b/resolve/main/processor_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRepositoryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\transformers\\utils\\hub.py:402\u001B[0m, in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    401\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[1;32m--> 402\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    403\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    405\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    406\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    407\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    408\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    409\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    410\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    415\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    416\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\huggingface_hub\\file_download.py:1221\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[0;32m   1220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1222\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[0;32m   1223\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1224\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[0;32m   1225\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1226\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1227\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1228\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1229\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[0;32m   1230\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1231\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1232\u001B[0m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1233\u001B[0m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1234\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[0;32m   1235\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1236\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1237\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\huggingface_hub\\file_download.py:1325\u001B[0m, in \u001B[0;36m_hf_hub_download_to_cache_dir\u001B[1;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001B[0m\n\u001B[0;32m   1324\u001B[0m     \u001B[38;5;66;03m# Otherwise, raise appropriate error\u001B[39;00m\n\u001B[1;32m-> 1325\u001B[0m     \u001B[43m_raise_on_head_call_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead_call_error\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1327\u001B[0m \u001B[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\huggingface_hub\\file_download.py:1823\u001B[0m, in \u001B[0;36m_raise_on_head_call_error\u001B[1;34m(head_call_error, force_download, local_files_only)\u001B[0m\n\u001B[0;32m   1821\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, RepositoryNotFoundError) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, GatedRepoError):\n\u001B[0;32m   1822\u001B[0m     \u001B[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001B[39;00m\n\u001B[1;32m-> 1823\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m head_call_error\n\u001B[0;32m   1824\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1825\u001B[0m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\huggingface_hub\\file_download.py:1722\u001B[0m, in \u001B[0;36m_get_metadata_or_catch_error\u001B[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001B[0m\n\u001B[0;32m   1721\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1722\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m \u001B[43mget_hf_file_metadata\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1723\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\huggingface_hub\\file_download.py:1645\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001B[0m\n\u001B[0;32m   1644\u001B[0m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[1;32m-> 1645\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1646\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHEAD\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1647\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1648\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1649\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1650\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1651\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1652\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1653\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1654\u001B[0m hf_raise_for_status(r)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\huggingface_hub\\file_download.py:372\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[1;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[0;32m    371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[1;32m--> 372\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    373\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    374\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    375\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    376\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    377\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    379\u001B[0m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[0;32m    380\u001B[0m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\huggingface_hub\\file_download.py:396\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[1;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[0;32m    395\u001B[0m response \u001B[38;5;241m=\u001B[39m get_session()\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:352\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    344\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    345\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Client Error.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    346\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    350\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make sure you are authenticated.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    351\u001B[0m     )\n\u001B[1;32m--> 352\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RepositoryNotFoundError(message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m400\u001B[39m:\n",
      "\u001B[1;31mRepositoryNotFoundError\u001B[0m: 401 Client Error. (Request ID: Root=1-67dc2481-65a8b1a0720d754f4acae7f6;bb1ef6ce-141f-42ba-ae14-3e14fc6a5d87)\n\nRepository Not Found for url: https://huggingface.co/llava-hf/llava-1.5-7b/resolve/main/processor_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Load the model and processor\u001B[39;00m\n\u001B[0;32m      6\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllava-hf/llava-1.5-7b\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 7\u001B[0m processor \u001B[38;5;241m=\u001B[39m \u001B[43mAutoProcessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m model \u001B[38;5;241m=\u001B[39m LlavaForConditionalGeneration\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name, torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16, device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_image_descriptions\u001B[39m(image_path, output_format\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnewline\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py:245\u001B[0m, in \u001B[0;36mAutoProcessor.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    240\u001B[0m get_file_from_repo_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    241\u001B[0m     key: kwargs[key] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(get_file_from_repo)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m kwargs\n\u001B[0;32m    242\u001B[0m }\n\u001B[0;32m    244\u001B[0m \u001B[38;5;66;03m# Let's start by checking whether the processor class is saved in a processor config\u001B[39;00m\n\u001B[1;32m--> 245\u001B[0m processor_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mget_file_from_repo\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mPROCESSOR_NAME\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mget_file_from_repo_kwargs\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m processor_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    249\u001B[0m     config_dict, _ \u001B[38;5;241m=\u001B[39m ProcessorMixin\u001B[38;5;241m.\u001B[39mget_processor_dict(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\transformers\\utils\\hub.py:554\u001B[0m, in \u001B[0;36mget_file_from_repo\u001B[1;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    551\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`token` and `use_auth_token` are both specified. Please set only the argument `token`.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    552\u001B[0m     token \u001B[38;5;241m=\u001B[39m use_auth_token\n\u001B[1;32m--> 554\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    555\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath_or_repo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    556\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    557\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    558\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    559\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    560\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    561\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    562\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    563\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    564\u001B[0m \u001B[43m    \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_gated_repo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_missing_entries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    567\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_connection_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    568\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\transformers\\utils\\hub.py:425\u001B[0m, in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    420\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[0;32m    421\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to access a gated repo.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMake sure to have access to it at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    422\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    423\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    424\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 425\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[0;32m    426\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a local folder and is not a valid model identifier \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    427\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlisted on \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf this is a private repository, make sure to pass a token \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    428\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    429\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`token=<your_token>`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    430\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    431\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RevisionNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    432\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[0;32m    433\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    434\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor this model name. Check the model page at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    435\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for available revisions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    436\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mOSError\u001B[0m: llava-hf/llava-1.5-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T18:26:56.516297Z",
     "start_time": "2025-03-20T18:26:51.261301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor, MllamaProcessor\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "processor = MllamaProcessor.from_pretrained(model_id)\n",
    "\n",
    "with open(r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\berries-bounding-box-1\\train\\images\\IMG_9331_jpg.rf.20009327b80c55eec840b8b4f5cddf57.jpg\", \"rb\") as f:\n",
    "    raw_image = PIL_Image.open(f).convert(\"RGB\")\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True,tokenize=False)\n",
    "\n",
    "inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "print(\"text&image_output: \",processor.decode(output[0])[len(prompt):])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8541b9cae98e48799f22a9f55f9151a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid input text. Please provide a string, or a list of strings",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 24\u001B[0m\n\u001B[0;32m     12\u001B[0m conversation \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     13\u001B[0m     {\n\u001B[0;32m     14\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     19\u001B[0m     },\n\u001B[0;32m     20\u001B[0m ]\n\u001B[0;32m     22\u001B[0m prompt \u001B[38;5;241m=\u001B[39m processor\u001B[38;5;241m.\u001B[39mapply_chat_template(conversation, add_generation_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,tokenize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 24\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[43mprocessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraw_image\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(model\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     25\u001B[0m output \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.7\u001B[39m, top_p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m, max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext&image_output: \u001B[39m\u001B[38;5;124m\"\u001B[39m,processor\u001B[38;5;241m.\u001B[39mdecode(output[\u001B[38;5;241m0\u001B[39m])[\u001B[38;5;28mlen\u001B[39m(prompt):])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\transformers\\models\\mllama\\processing_mllama.py:285\u001B[0m, in \u001B[0;36mMllamaProcessor.__call__\u001B[1;34m(self, images, text, audio, videos, **kwargs)\u001B[0m\n\u001B[0;32m    283\u001B[0m     text \u001B[38;5;241m=\u001B[39m [text]\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(text, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(t, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m text)):\n\u001B[1;32m--> 285\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid input text. Please provide a string, or a list of strings\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    286\u001B[0m n_images_in_text \u001B[38;5;241m=\u001B[39m [t\u001B[38;5;241m.\u001B[39mcount(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_token) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m text]\n\u001B[0;32m    287\u001B[0m text \u001B[38;5;241m=\u001B[39m [build_string_from_input(text_item, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbos_token, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_token) \u001B[38;5;28;01mfor\u001B[39;00m text_item \u001B[38;5;129;01min\u001B[39;00m text]\n",
      "\u001B[1;31mValueError\u001B[0m: Invalid input text. Please provide a string, or a list of strings"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:07:44.507012Z",
     "start_time": "2025-03-20T20:07:40.438097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7bbc9e04188435fb5c2d21665dec9c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:07:44.514134Z",
     "start_time": "2025-03-20T20:07:44.510016Z"
    }
   },
   "cell_type": "code",
   "source": "model.tie_weights()\n",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:07:45.050908Z",
     "start_time": "2025-03-20T20:07:45.028107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#with open(r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\berries-bounding-box-1\\train\\images\\IMG_9331_jpg.rf.20009327b80c55eec840b8b4f5cddf57.jpg\", \"rb\") as f:\n",
    "    #raw_image = PIL_Image.open(f).convert(\"RGB\")\n",
    "image_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\berries-bounding-box-1\\train\\images\\IMG_9331_jpg.rf.20009327b80c55eec840b8b4f5cddf57.jpg\"\n",
    "raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True,tokenize=False)\n",
    "\n",
    "inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "print(\"text&image_output: \",processor.decode(output[0])[len(prompt):])"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input text. Please provide a string, or a list of strings",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 17\u001B[0m\n\u001B[0;32m      5\u001B[0m conversation \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      6\u001B[0m     {\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m     },\n\u001B[0;32m     13\u001B[0m ]\n\u001B[0;32m     15\u001B[0m prompt \u001B[38;5;241m=\u001B[39m processor\u001B[38;5;241m.\u001B[39mapply_chat_template(conversation, add_generation_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,tokenize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 17\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[43mprocessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraw_image\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(model\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     18\u001B[0m output \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.7\u001B[39m, top_p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m, max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext&image_output: \u001B[39m\u001B[38;5;124m\"\u001B[39m,processor\u001B[38;5;241m.\u001B[39mdecode(output[\u001B[38;5;241m0\u001B[39m])[\u001B[38;5;28mlen\u001B[39m(prompt):])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AutoAnnotate\\Lib\\site-packages\\transformers\\models\\mllama\\processing_mllama.py:285\u001B[0m, in \u001B[0;36mMllamaProcessor.__call__\u001B[1;34m(self, images, text, audio, videos, **kwargs)\u001B[0m\n\u001B[0;32m    283\u001B[0m     text \u001B[38;5;241m=\u001B[39m [text]\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(text, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(t, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m text)):\n\u001B[1;32m--> 285\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid input text. Please provide a string, or a list of strings\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    286\u001B[0m n_images_in_text \u001B[38;5;241m=\u001B[39m [t\u001B[38;5;241m.\u001B[39mcount(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_token) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m text]\n\u001B[0;32m    287\u001B[0m text \u001B[38;5;241m=\u001B[39m [build_string_from_input(text_item, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbos_token, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_token) \u001B[38;5;28;01mfor\u001B[39;00m text_item \u001B[38;5;129;01min\u001B[39;00m text]\n",
      "\u001B[1;31mValueError\u001B[0m: Invalid input text. Please provide a string, or a list of strings"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:09:25.964506Z",
     "start_time": "2025-03-20T20:09:10.297268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# Load Model and Processor\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model.tie_weights()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66faaf4a3cd54eb9bbe787988da86d1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:18:24.788597Z",
     "start_time": "2025-03-20T20:18:18.518823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Image\n",
    "image_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\berries-bounding-box-1\\train\\images\\IMG_9331_jpg.rf.20009327b80c55eec840b8b4f5cddf57.jpg\"\n",
    "raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "manual_entry = input(\"object in image\")\n",
    "# Define Conversation Prompt (Corrected)\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},  # Note: No \"image\": raw_image here!\n",
    "            {\"type\": \"text\", \"text\": f\"Describe the {manual_entry} of the image in 3 words maximum for prompt use in a zero-shot detection model, and give 5 separate entries, each separated by a new line, and its own separate descriptor of the target. do not numerate.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Convert to Text Prompt\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# Process Inputs Correctly\n",
    "inputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\").to(model.device)  # Ensure correct parameter order"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:18:29.623300Z",
     "start_time": "2025-03-20T20:18:27.461521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate Output\n",
    "output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "# Decode and Print Output\n",
    "response = processor.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Text & Image Output:\", response)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text & Image Output: user\n",
      "\n",
      "Describe the blueberry of the image in 3 words maximum for prompt use in a zero-shot detection model, and give 5 separate entries, each separated by a new line, and its own separate descriptor of the target. do not numerate.assistant\n",
      "\n",
      "blue berries \n",
      "blue fruit\n",
      "round berries\n",
      "small blue\n",
      "blue fruit clusters\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:24:18.678065Z",
     "start_time": "2025-03-20T20:24:18.674287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_descriptions(response):\n",
    "    \"\"\"\n",
    "    Extracts only the descriptions from the model response, removing metadata and unnecessary text.\n",
    "\n",
    "    :param response: The raw response from the model.\n",
    "    :return: A list of clean descriptions.\n",
    "    \"\"\"\n",
    "    lines = response.split(\"\\n\")  # Split response into lines\n",
    "    unwanted_keywords = [\"user\", \"assistant\", \"describe\", \"text & image output\"]  # Keywords to ignore\n",
    "\n",
    "    descriptions = []\n",
    "    for line in lines:\n",
    "        clean_line = line.strip()\n",
    "        if clean_line and not any(keyword in clean_line.lower() for keyword in unwanted_keywords):\n",
    "            descriptions.append(clean_line)\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "# Extracted descriptions\n",
    "cleaned_descriptions = extract_descriptions(response)\n",
    "\n",
    "# Print results\n",
    "for desc in cleaned_descriptions:\n",
    "    print(desc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue berries\n",
      "blue fruit\n",
      "round berries\n",
      "small blue\n",
      "blue fruit clusters\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
