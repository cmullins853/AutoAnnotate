{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "import pynvml\n",
    "import torch\n",
    "import time\n",
    "from ultralytics import SAM\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon\n",
    "import time as t\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "bab3a1dff14e2c25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_labels(boxes, max_area):\n",
    "    clean_boxes = []\n",
    "    box_list = boxes.tolist()\n",
    "    for box in box_list:\n",
    "        #if width * height < 0.9, add box to list.\n",
    "        if (box[2] * box[3]) < max_area:\n",
    "            clean_boxes.append(box)\n",
    "    if len(clean_boxes) < 1:\n",
    "        return boxes\n",
    "    return torch.FloatTensor(clean_boxes)\n",
    "\n",
    "\n",
    "def run_dino(img_path, prompt, box_threshold, text_threshold, model_size, maxarea=0.7, save_dir=\"DINO-labels\"):\n",
    "    #choose swinb or swint\n",
    "    if model_size == 'swint':\n",
    "        config_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinT_OGC.py\"\n",
    "        checkpoint_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\GroundingDINO\\weights\\groundingdino_swint_ogc.pth\"\n",
    "    elif model_size == 'swinb':\n",
    "        checkpoint_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\GroundingDINO\\weights\\groundingdino_swinb_cogcoor.pth\"\n",
    "        config_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinB_cfg.py\"\n",
    "\n",
    "    model = load_model(config_path, checkpoint_path)\n",
    "\n",
    "    image_source, image = load_image(img_path)\n",
    "\n",
    "    boxes, accuracy, obj_name = predict(model=model, image=image, caption=prompt, box_threshold=box_threshold,\n",
    "                                        text_threshold=text_threshold)\n",
    "\n",
    "    #print(boxes, accuracy, obj_name)\n",
    "    #Convert boxes from YOLOv8 format to xyxy\n",
    "    img_height, img_width = cv2.imread(img_path).shape[:2]\n",
    "    clean_boxes = clean_labels(boxes, maxarea)\n",
    "    absolute_boxes = [[(box[0] - (box[2] / 2)) * img_width,\n",
    "                       (box[1] - (box[3] / 2)) * img_height,\n",
    "                       (box[0] + (box[2] / 2)) * img_width,\n",
    "                       (box[1] + (box[3] / 2)) * img_height] for box in clean_boxes.tolist()]\n",
    "    #annotated_frame = annotate(image_source=image_source, boxes=clean_boxes, logits=accuracy, phrases=obj_name)\n",
    "    #sv.plot_image(annotated_frame, (16,16))\n",
    "    save_labels = True\n",
    "    #print(clean_boxes)\n",
    "    if save_labels:\n",
    "        clean_boxes = clean_boxes.tolist()\n",
    "\n",
    "        for x in clean_boxes:\n",
    "            x.insert(0, 0)\n",
    "\n",
    "        with open(f'{save_dir}/{os.path.splitext(os.path.basename(img_path))[0]}.txt', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            writer.writerows(clean_boxes)\n",
    "            #print(\"Labels saved in /DINO-labels\")\n",
    "\n",
    "    return absolute_boxes\n",
    "\n",
    "\n",
    "def load_dino_model(model_size):\n",
    "    #choose swinb or swint\n",
    "    if model_size == 'swint':\n",
    "        config_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinT_OGC.py\"\n",
    "        checkpoint_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\GroundingDINO\\weights\\groundingdino_swint_ogc.pth\"\n",
    "    elif model_size == 'swinb':\n",
    "        checkpoint_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\GroundingDINO\\weights\\groundingdino_swinb_cogcoor.pth\"\n",
    "        config_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinB_cfg.py\"\n",
    "\n",
    "    model = load_model(config_path, checkpoint_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_dino_from_model(model, img_path, prompt, box_threshold, text_threshold, maxarea=0.7, save_dir=\"DINO-labels\"):\n",
    "    image_source, image = load_image(img_path)\n",
    "    boxes, accuracy, obj_name = predict(model=model, image=image, caption=prompt, box_threshold=box_threshold,\n",
    "                                        text_threshold=text_threshold)\n",
    "\n",
    "    #print(boxes, accuracy, obj_name)\n",
    "    #Convert boxes from YOLOv8 format to xyxy\n",
    "    img_height, img_width = cv2.imread(img_path).shape[:2]\n",
    "    clean_boxes = clean_labels(boxes, maxarea)\n",
    "    absolute_boxes = [[(box[0] - (box[2] / 2)) * img_width,\n",
    "                       (box[1] - (box[3] / 2)) * img_height,\n",
    "                       (box[0] + (box[2] / 2)) * img_width,\n",
    "                       (box[1] + (box[3] / 2)) * img_height] for box in clean_boxes.tolist()]\n",
    "    #annotated_frame = annotate(image_source=image_source, boxes=clean_boxes, logits=accuracy, phrases=obj_name)\n",
    "    #sv.plot_image(annotated_frame, (16,16))\n",
    "    save_labels = True\n",
    "    if save_labels:\n",
    "        clean_boxes = clean_boxes.tolist()\n",
    "\n",
    "        for x in clean_boxes:\n",
    "            x.insert(0, 0)\n",
    "\n",
    "        with open(f'{save_dir}/{os.path.splitext(os.path.basename(img_path))[0]}.txt', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            writer.writerows(clean_boxes)\n",
    "            #print(\"Labels saved in /DINO-labels\")\n",
    "    return absolute_boxes\n",
    "\n",
    "def calculate_metrics(TP, FP, FN, TN):\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    mcc = ((TP * TN) - (FP * FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) if np.sqrt(\n",
    "        (TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if TN + FP > 0 else 0\n",
    "    return precision, recall, f1, mcc, specificity\n",
    "\n",
    "\n",
    "def pixel_accuracy(predicted, ground_truth):\n",
    "    correct = np.sum(predicted == ground_truth)\n",
    "    total = predicted.shape[0] * predicted.shape[1]\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def read_and_draw_boxes(file_path, image_dim=(1280, 720)):\n",
    "    boxes = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            class_id, x, y, width, height = map(float, line.strip().split())\n",
    "            x1 = (x - (width / 2)) * image_dim[0]\n",
    "            x2 = (x + (width / 2)) * image_dim[0]\n",
    "            y1 = (y - (height / 2)) * image_dim[1]\n",
    "            y2 = (y + (height / 2)) * image_dim[1]\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "    image = Image.new('L', image_dim, 0)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box, fill=255)\n",
    "        #draw.rectangle([1,1,20,20], fill=255)\n",
    "    image.save(\"test.jpg\")\n",
    "    return np.array(image, dtype=np.uint8)\n",
    "\n",
    "\n",
    "def calculate_pixel_metrics(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Calculate IoU based on pixel values from two masks.\n",
    "    \"\"\"\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def process_files(predicted_mask_dir, ground_truth_mask_dir):\n",
    "    predicted_files = os.listdir(ground_truth_mask_dir)\n",
    "    metrics = {\n",
    "        'iou_scores': [],\n",
    "        'pixel_accuracies': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': [],\n",
    "        'f1_scores': [],\n",
    "        'mcc_scores': [],\n",
    "        'specificity_scores': []\n",
    "    }\n",
    "\n",
    "    for fname in predicted_files:\n",
    "        predicted_mask_path = os.path.join(predicted_mask_dir, fname)\n",
    "        ground_truth_mask_path = os.path.join(ground_truth_mask_dir, os.path.splitext(fname)[0] + '.txt')\n",
    "\n",
    "        if not os.path.exists(ground_truth_mask_path):\n",
    "            metrics['iou_scores'].append(0)\n",
    "            metrics['pixel_accuracies'].append(0)\n",
    "            metrics['precision_scores'].append(0)\n",
    "            metrics['recall_scores'].append(0)\n",
    "            metrics['f1_scores'].append(0)\n",
    "            metrics['mcc_scores'].append(0)\n",
    "            metrics['specificity_scores'].append(0)\n",
    "            continue\n",
    "        #print(ground_truth_mask_path)\n",
    "        predicted_mask = read_and_draw_boxes(predicted_mask_path)\n",
    "        ground_truth_mask = read_and_draw_boxes(ground_truth_mask_path)\n",
    "\n",
    "        COMMON_HEIGHT, COMMON_WIDTH = 1280, 720  # or any other desired size\n",
    "\n",
    "        predicted_mask = cv2.resize(predicted_mask, (COMMON_WIDTH, COMMON_HEIGHT))\n",
    "\n",
    "        ground_truth_mask = cv2.resize(ground_truth_mask, (COMMON_WIDTH, COMMON_HEIGHT))\n",
    "\n",
    "        _, predicted_mask_bin = cv2.threshold(predicted_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        _, ground_truth_mask_bin = cv2.threshold(ground_truth_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        predicted_mask_bin = predicted_mask_bin / 255\n",
    "        ground_truth_mask_bin = ground_truth_mask_bin / 255\n",
    "        TP = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 1)))\n",
    "        TN = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 0)))\n",
    "        FP = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 0)))\n",
    "        FN = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 1)))\n",
    "\n",
    "        intersection = np.logical_and(predicted_mask_bin, ground_truth_mask_bin)\n",
    "        union = np.logical_or(predicted_mask_bin, ground_truth_mask_bin)\n",
    "        metrics['iou_scores'].append(np.sum(intersection) / np.sum(union))\n",
    "        metrics['pixel_accuracies'].append(pixel_accuracy(predicted_mask_bin, ground_truth_mask_bin))\n",
    "        precision, recall, f1, mcc, specificity = calculate_metrics(TP, FP, FN, TN)\n",
    "        metrics['precision_scores'].append(precision)\n",
    "        metrics['recall_scores'].append(recall)\n",
    "        metrics['f1_scores'].append(f1)\n",
    "        metrics['mcc_scores'].append(mcc)\n",
    "        metrics['specificity_scores'].append(specificity)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def optimize_prompts(prompts_file, gt_path, img_dir, save_file):\n",
    "    inf_path = fr\"C:\\Users\\Mechanized Systems\\DataspellProjects\\AutoAnnotate\\autoannotate study\\DINO-labels\"\n",
    "\n",
    "    with  open(prompts_file, 'r') as file:\n",
    "        result_dict = {}\n",
    "        for x in file:\n",
    "            result_dict[x.strip()] = {}\n",
    "\n",
    "    #result_dict = dict.fromkeys(prompts,{})\n",
    "    for prompt in result_dict.keys():\n",
    "        print(f'Trying prompt: \"{prompt}\"')\n",
    "        for fname in os.listdir(img_dir):\n",
    "            box_threshold = 0.3\n",
    "            text_threshold = 0.1\n",
    "            model_size = 'swint'\n",
    "            run_dino(os.path.join(img_dir, fname), prompt, box_threshold, text_threshold, model_size)\n",
    "\n",
    "        metrics = process_files(inf_path, gt_path)\n",
    "\n",
    "        result_dict[prompt]['iou_scores'] = np.mean(metrics['iou_scores'])\n",
    "\n",
    "    results = sorted(list(result_dict.items()), key=lambda a: a[1]['iou_scores'], reverse=True)\n",
    "    print(results)\n",
    "\n",
    "    with open(save_file, 'w') as output:\n",
    "        for prompt_stats in results:\n",
    "            output.write(str(prompt_stats) + '\\n')\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def optimize_confidence(prompt, model_size, gt_path, img_dir):\n",
    "    inf_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\DINO-labels\"\n",
    "    best_iou = 0\n",
    "    #number of decimal points in confidence\n",
    "    final_precision = 2\n",
    "    ubound = 0.9\n",
    "    lbound = 0.0\n",
    "    for precision in [x + 1 for x in range(final_precision)]:\n",
    "        for conf in [x / (10 ** precision) for x in\n",
    "                     range(int(lbound * (10 ** precision)), int(ubound * (10 ** precision)))]:\n",
    "            for fname in os.listdir(img_dir):\n",
    "                prompt = prompt\n",
    "                box_threshold = conf\n",
    "                text_threshold = 0.01\n",
    "                model_size = model_size\n",
    "                boxes = run_dino(os.path.join(img_dir, fname), prompt, box_threshold, text_threshold, model_size)\n",
    "            metrics = process_files(inf_path, gt_path)\n",
    "            iou = np.mean(metrics['iou_scores'])\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_conf = conf\n",
    "            print(f\"confidence: {conf}, IOU: {iou} (best: {best_iou})\")\n",
    "        print(f\"Best IOU at p{precision} is {best_iou} with confidence = {best_conf}\")\n",
    "        lbound = max(0, best_conf - (1 / (10 ** precision)))\n",
    "        ubound = min(0.9, best_conf + (1 / (10 ** precision)))\n",
    "    return best_iou, best_conf\n",
    "\n",
    "def multi_optmize(img_dir, gt_label_dir, model_size, prompts):\n",
    "    print(\"Be sure to change the category folders and model size in each function!\")\n",
    "    t.sleep(2)\n",
    "    start = t.time()\n",
    "    best_iou = 0\n",
    "    for prompt in prompts:\n",
    "        print(f\"Trying prompt: '{prompt}'\")\n",
    "        iou, conf = optimize_confidence(prompt, model_size, gt_label_dir, img_dir)\n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_conf = conf\n",
    "            best_prompt = prompt\n",
    "        print(f\"So far: best prompt is '{best_prompt}', conf is {best_conf}, resulting in {best_iou} IOU)\")\n",
    "    print(f\"\\n\\n\\n\\n\\nFinal Result: best prompt is '{best_prompt}', conf is {best_conf}, resulting in {best_iou} IOU)\")\n",
    "    print(f\"final time: {t.time() - start}\")\n",
    "    return {\"prompt\": best_prompt, \"conf\": best_conf, \"iou\": best_iou}\n",
    "\n",
    "def save_masks(sam_results, output_dir):\n",
    "    segments = sam_results[0].masks.xyn\n",
    "    with open(f\"{Path(output_dir) / Path(sam_results[0].path).stem}.txt\", \"w\") as f:\n",
    "        for i in range(len(segments)):\n",
    "            s = segments[i]\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            coords = np.array(s).reshape(-1, 2)\n",
    "            polygon = Polygon(coords)\n",
    "            segment = map(str, segments[i].reshape(-1).tolist())\n",
    "            print(polygon.area)\n",
    "            f.write(f\"0 \" + \" \".join(segment) + \"\\n\")\n",
    "\n",
    "\n",
    "def append_mask(sam_results, output_dir):\n",
    "    segments = sam_results[0].masks.xyn\n",
    "    with open(f\"{Path(output_dir) / Path(sam_results[0].path).stem}.txt\", \"a\") as f:\n",
    "        for i in range(len(segments)):\n",
    "            s = segments[i]\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            segment = map(str, segments[i].reshape(-1).tolist())\n",
    "            f.write(f\"0 \" + \" \".join(segment) + \"\\n\")\n"
   ],
   "id": "6d7cf2b7bc9fff17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Grounding DINO",
   "id": "bdd2f6378063889e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth_paths = [\n",
    "    r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\Bounding_Berries_LLM\\train\\images\",\n",
    "    r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\Bounding_Buds_LLM\\train\\images\",\n",
    "    r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\Bounding_redleaf_LLM\\train\\images\",\n",
    "    r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\Bounding_Fescue_LLM\\train\\images\"]\n",
    "dino_model = 'swint'\n",
    "prompts = ['Dark blue globes', 'Young blueberry clusters', 'Red leaves', 'light-colored mold stain']\n",
    "confidences = [0.35, 0.30, 0.40, 0.30]\n",
    "SAM_model = 'sam2_t.pt'\n",
    "DINO_time = {\n",
    "    'berries': [],\n",
    "    'buds': [],\n",
    "    'red leaf': [],\n",
    "    'fescue': [],\n",
    "}\n",
    "\n",
    "sam_masks = []\n",
    "categories = ['berries', 'buds', 'red leaf', 'fescue']\n",
    "max_areas = [0.3, 0.3, 0.8, 0.5]\n",
    "save_dir = 'DINO-labels'\n",
    "\n",
    "for category in categories:\n",
    "    save_path = fr\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\study_2\\{category}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "# Initialize NVML\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming single GPU (index 0)\n",
    "power = []\n",
    "\n",
    "dino = load_dino_model(dino_model)\n",
    "initial_time = time.time()\n",
    "power_before = pynvml.nvmlDeviceGetPowerUsage(handle)  # in milliwatts\n",
    "for x in range(len(categories)):\n",
    "    img_path = ground_truth_paths[x]\n",
    "    prompt = prompts[x]\n",
    "    conf = confidences[x]\n",
    "    for fname in os.listdir(img_path):\n",
    "        save_path = fr\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\study_2\\{categories[x]}\"\n",
    "        path = img_path + \"\\\\\" + fname\n",
    "        time_start = time.time()\n",
    "        boxes = run_dino_from_model(dino, path, prompt, conf, 0.1, max_areas[x], save_path)\n",
    "        DINO_time[f'{categories[x]}'].append(time.time() - time_start)\n",
    "        power.append(pynvml.nvmlDeviceGetPowerUsage(handle))\n",
    "\n",
    "# Get final power reading\n",
    "duration = time.time() - initial_time  # Time in seconds\n",
    "power_after = np.mean(power)\n",
    "# Calculate energy usage\n",
    "avg_power = (power_after - power_before)  # Average power in milliwatts\n",
    "energy_consumed = (avg_power * duration) / 1000  # Convert to Joules\n",
    "\n",
    "# Shutdown NVML\n",
    "pynvml.nvmlShutdown()\n",
    "\n",
    "\n",
    "for j in power:\n",
    "    print(j)\n",
    "print(power_before)\n",
    "print(f\"{energy_consumed} J\")\n",
    "print(DINO_time)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics = process_files(r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\fescue\",\n",
    "                        r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\Bounding_Fescue_LLM\\train\\labels\")\n",
    "\n",
    "print(f\"Average IoU: {np.mean(metrics['iou_scores'])}\")\n",
    "print(f\"Average Precision: {np.mean(metrics['precision_scores'])}\")\n",
    "print(f\"Average Recall: {np.mean(metrics['recall_scores'])}\")\n",
    "print(f\"Average F1: {np.mean(metrics['f1_scores'])}\")"
   ],
   "id": "c0d43bf6b2301759"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Grounded SAM",
   "id": "799c669884b10a95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming single GPU (index 0)\n",
    "power = []\n",
    "\n",
    "SAM_time = {\n",
    "    'berries': [],\n",
    "    'buds': [],\n",
    "    'red leaf': [],\n",
    "    'fescue': [],\n",
    "}\n",
    "\n",
    "dino = load_dino_model(dino_model)\n",
    "model = SAM(SAM_model)\n",
    "initial_time = time.time()\n",
    "power_before = pynvml.nvmlDeviceGetPowerUsage(handle)  # in milliwatts\n",
    "for x in range(len(categories)):\n",
    "    img_path = ground_truth_paths[x]\n",
    "    prompt = prompts[x]\n",
    "    conf = confidences[x]\n",
    "    for fname in os.listdir(img_path):\n",
    "        sam_masks.clear()\n",
    "        save_path = fr\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\SAM_test\\{categories[x]}\"\n",
    "        path = img_path + \"\\\\\" + fname\n",
    "        time_start = time.time()\n",
    "        boxes = run_dino_from_model(dino, path, prompt, conf, 0.1, max_areas[x])\n",
    "        sam_results = model(os.path.join(img_path, fname), model=model, bboxes=boxes)\n",
    "        save_masks(sam_results, save_path)\n",
    "        SAM_time[f'{categories[x]}'].append(time.time() - time_start)\n",
    "        power.append(pynvml.nvmlDeviceGetPowerUsage(handle))\n",
    "\n",
    "# Get final power reading\n",
    "duration = time.time() - initial_time  # Time in seconds\n",
    "power_after = np.mean(power)\n",
    "# Calculate energy usage\n",
    "avg_power = (power_after - power_before)  # Average power in milliwatts\n",
    "energy_consumed = (avg_power * duration) / 1000  # Convert to Joules\n",
    "\n",
    "# Shutdown NVML\n",
    "pynvml.nvmlShutdown()\n",
    "\n",
    "\n",
    "for j in power:\n",
    "    print(j)\n",
    "print(power_before)\n",
    "print(f\"{energy_consumed} J\")\n",
    "print(SAM_time)"
   ],
   "id": "17e2810564d0d7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
