{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T00:25:05.137394Z",
     "start_time": "2025-05-22T00:25:00.979889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "import pynvml\n",
    "import torch\n",
    "import time\n",
    "from ultralytics import SAM\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from ultralytics import YOLOE\n",
    "from ultralytics.models.yolo.yoloe import YOLOEVPSegPredictor\n",
    "from tkinter import filedialog, Tk\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "bab3a1dff14e2c25",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T00:26:26.474209Z",
     "start_time": "2025-05-22T00:26:26.461089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_labels(boxes, max_area):\n",
    "    clean_boxes = []\n",
    "    box_list = boxes.tolist()\n",
    "    for box in box_list:\n",
    "        #if width * height < 0.9, add box to list.\n",
    "        if (box[2] * box[3]) < max_area:\n",
    "            clean_boxes.append(box)\n",
    "    if len(clean_boxes) < 1:\n",
    "        return boxes\n",
    "    return torch.FloatTensor(clean_boxes)\n",
    "\n",
    "def load_dino_model(model_size):\n",
    "    #choose swinb or swint\n",
    "    if model_size == 'swint':\n",
    "        config_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinT_OGC.py\"\n",
    "        checkpoint_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\GroundingDINO\\weights\\groundingdino_swint_ogc.pth\"\n",
    "    elif model_size == 'swinb':\n",
    "        checkpoint_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\GUI and Pipeline\\GroundingDINO\\weights\\groundingdino_swinb_cogcoor.pth\"\n",
    "        config_path = r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinB_cfg.py\"\n",
    "\n",
    "    model = load_model(config_path, checkpoint_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_dino_from_model(model, img_path, prompt, box_threshold, text_threshold, maxarea=0.7, save_dir=\"DINO-labels\"):\n",
    "    image_source, image = load_image(img_path)\n",
    "    boxes, accuracy, obj_name = predict(model=model, image=image, caption=prompt, box_threshold=box_threshold,\n",
    "                                        text_threshold=text_threshold)\n",
    "\n",
    "    #print(boxes, accuracy, obj_name)\n",
    "    #Convert boxes from YOLOv8 format to xyxy\n",
    "    img_height, img_width = cv2.imread(img_path).shape[:2]\n",
    "    clean_boxes = clean_labels(boxes, maxarea)\n",
    "    absolute_boxes = [[(box[0] - (box[2] / 2)) * img_width,\n",
    "                       (box[1] - (box[3] / 2)) * img_height,\n",
    "                       (box[0] + (box[2] / 2)) * img_width,\n",
    "                       (box[1] + (box[3] / 2)) * img_height] for box in clean_boxes.tolist()]\n",
    "    #annotated_frame = annotate(image_source=image_source, boxes=clean_boxes, logits=accuracy, phrases=obj_name)\n",
    "    #sv.plot_image(annotated_frame, (16,16))\n",
    "    save_labels = True\n",
    "    if save_labels:\n",
    "        clean_boxes = clean_boxes.tolist()\n",
    "\n",
    "        for x in clean_boxes:\n",
    "            x.insert(0, 0)\n",
    "\n",
    "        with open(f'{save_dir}/{os.path.splitext(os.path.basename(img_path))[0]}.txt', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            writer.writerows(clean_boxes)\n",
    "            #print(\"Labels saved in /DINO-labels\")\n",
    "    return absolute_boxes\n",
    "\n",
    "def calculate_metrics(TP, FP, FN, TN):\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    mcc = ((TP * TN) - (FP * FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) if np.sqrt(\n",
    "        (TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if TN + FP > 0 else 0\n",
    "    return precision, recall, f1, mcc, specificity\n",
    "\n",
    "\n",
    "def read_and_draw_boxes(file_path, image_dim=(1280, 720)):\n",
    "    boxes = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            class_id, x, y, width, height = map(float, line.strip().split())\n",
    "            x1 = (x - (width / 2)) * image_dim[0]\n",
    "            x2 = (x + (width / 2)) * image_dim[0]\n",
    "            y1 = (y - (height / 2)) * image_dim[1]\n",
    "            y2 = (y + (height / 2)) * image_dim[1]\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "    image = Image.new('L', image_dim, 0)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box, fill=255)\n",
    "        #draw.rectangle([1,1,20,20], fill=255)\n",
    "    image.save(\"test.jpg\")\n",
    "    return np.array(image, dtype=np.uint8)\n",
    "\n",
    "def process_files(predicted_mask_dir, ground_truth_mask_dir):\n",
    "    predicted_files = os.listdir(ground_truth_mask_dir)\n",
    "    metrics = {\n",
    "        'iou_scores': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': [],\n",
    "        'f1_scores': [],\n",
    "        'mcc_scores': [],\n",
    "        'specificity_scores': []\n",
    "    }\n",
    "\n",
    "    for fname in predicted_files:\n",
    "        predicted_mask_path = os.path.join(predicted_mask_dir, fname)\n",
    "        ground_truth_mask_path = os.path.join(ground_truth_mask_dir, os.path.splitext(fname)[0] + '.txt')\n",
    "\n",
    "        if not os.path.exists(ground_truth_mask_path):\n",
    "            metrics['iou_scores'].append(0)\n",
    "            metrics['precision_scores'].append(0)\n",
    "            metrics['recall_scores'].append(0)\n",
    "            metrics['f1_scores'].append(0)\n",
    "            metrics['mcc_scores'].append(0)\n",
    "            metrics['specificity_scores'].append(0)\n",
    "            continue\n",
    "        #print(ground_truth_mask_path)\n",
    "        predicted_mask = read_and_draw_boxes(predicted_mask_path)\n",
    "        ground_truth_mask = read_and_draw_boxes(ground_truth_mask_path)\n",
    "\n",
    "        COMMON_HEIGHT, COMMON_WIDTH = 1280, 720  # or any other desired size\n",
    "\n",
    "        predicted_mask = cv2.resize(predicted_mask, (COMMON_WIDTH, COMMON_HEIGHT))\n",
    "\n",
    "        ground_truth_mask = cv2.resize(ground_truth_mask, (COMMON_WIDTH, COMMON_HEIGHT))\n",
    "\n",
    "        _, predicted_mask_bin = cv2.threshold(predicted_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        _, ground_truth_mask_bin = cv2.threshold(ground_truth_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        predicted_mask_bin = predicted_mask_bin / 255\n",
    "        ground_truth_mask_bin = ground_truth_mask_bin / 255\n",
    "        TP = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 1)))\n",
    "        TN = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 0)))\n",
    "        FP = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 0)))\n",
    "        FN = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 1)))\n",
    "\n",
    "        intersection = np.logical_and(predicted_mask_bin, ground_truth_mask_bin)\n",
    "        union = np.logical_or(predicted_mask_bin, ground_truth_mask_bin)\n",
    "        metrics['iou_scores'].append(np.sum(intersection) / np.sum(union))\n",
    "        precision, recall, f1, mcc, specificity = calculate_metrics(TP, FP, FN, TN)\n",
    "        metrics['precision_scores'].append(precision)\n",
    "        metrics['recall_scores'].append(recall)\n",
    "        metrics['f1_scores'].append(f1)\n",
    "        metrics['mcc_scores'].append(mcc)\n",
    "        metrics['specificity_scores'].append(specificity)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def save_masks(sam_results, output_dir):\n",
    "    segments = sam_results[0].masks.xyn\n",
    "    with open(f\"{Path(output_dir) / Path(sam_results[0].path).stem}.txt\", \"w\") as f:\n",
    "        for i in range(len(segments)):\n",
    "            s = segments[i]\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            coords = np.array(s).reshape(-1, 2)\n",
    "            polygon = Polygon(coords)\n",
    "            segment = map(str, segments[i].reshape(-1).tolist())\n",
    "            print(polygon.area)\n",
    "            f.write(f\"0 \" + \" \".join(segment) + \"\\n\")\n",
    "\n",
    "\n",
    "def append_mask(sam_results, output_dir):\n",
    "    segments = sam_results[0].masks.xyn\n",
    "    with open(f\"{Path(output_dir) / Path(sam_results[0].path).stem}.txt\", \"a\") as f:\n",
    "        for i in range(len(segments)):\n",
    "            s = segments[i]\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            segment = map(str, segments[i].reshape(-1).tolist())\n",
    "            f.write(f\"0 \" + \" \".join(segment) + \"\\n\")\n",
    "\n",
    "def load_YOLOE(model_path=\"yoloe-11l-seg.pt\"):\n",
    "    model = YOLOE(model_path)\n",
    "    return model\n",
    "# Set text prompt to detect person and bus. You only need to do this once after you load the model.\n",
    "\n",
    "def run_YOLOE(model, folder_path, names=\"\", conf=0.05, box_threshold=0.9):\n",
    "    model.set_classes(names, model.get_text_pe(names))\n",
    "    results = model.predict(folder_path, conf=conf)\n",
    "    for result in results:\n",
    "        class_ids = result.boxes.cls.int().tolist()  # noqa\n",
    "        shape = result.orig_shape\n",
    "        boxes = result.boxes.xyxy\n",
    "        max_area = shape[0]*shape[1]*box_threshold\n",
    "        boxes = clean_labels(boxes, max_area)\n",
    "    if len(boxes):\n",
    "        return boxes\n",
    "\n",
    "def run_YOLOE_vis(model, image_path, visual_prompts, conf=0.05, box_threshold=0.9):\n",
    "    results = model.predict(source=image_path, visual_prompts=visual_prompts, predictor=YOLOEVPSegPredictor, conf=conf)\n",
    "    for result in results:\n",
    "        class_ids = result.boxes.cls.int().tolist()  # noqa\n",
    "        shape = result.orig_shape\n",
    "        boxes = result.boxes.xyxy\n",
    "        max_area = shape[0]*shape[1]*box_threshold\n",
    "        boxes = clean_labels(boxes, max_area)\n",
    "    if len(boxes):\n",
    "        return boxes, results"
   ],
   "id": "6d7cf2b7bc9fff17",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text-Prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a62623010175e85b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Grounding DINO"
   ],
   "id": "bdd2f6378063889e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ground_truth_paths = [\n",
    "    r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\Bounding_Berries_LLM\\train\\images\",\n",
    "    r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\Bounding_Buds_LLM\\train\\images\",\n",
    "    r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\Bounding_redleaf_LLM\\train\\images\",\n",
    "    r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\Bounding_Fescue_LLM\\train\\images\"]\n",
    "dino_model = 'swint'\n",
    "prompts = ['Dark blue globes', 'Young blueberry clusters', 'Red leaves', 'light-colored mold stain']\n",
    "confidences = [0.35, 0.30, 0.40, 0.30]\n",
    "SAM_model = 'sam2_t.pt'\n",
    "DINO_time = {\n",
    "    'berries': [],\n",
    "    'buds': [],\n",
    "    'red leaf': [],\n",
    "    'fescue': [],\n",
    "}\n",
    "\n",
    "sam_masks = []\n",
    "categories = ['berries', 'buds', 'red leaf', 'fescue']\n",
    "max_areas = [0.3, 0.3, 0.8, 0.5]\n",
    "save_dir = 'DINO-labels'\n",
    "\n",
    "for category in categories:\n",
    "    save_path = fr\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\study_2\\GroundingDINO\\{category}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "# Initialize NVML\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming single GPU (index 0)\n",
    "power = []\n",
    "\n",
    "dino = load_dino_model(dino_model)\n",
    "initial_time = time.time()\n",
    "power_before = pynvml.nvmlDeviceGetPowerUsage(handle)  # in milliwatts\n",
    "for x in range(len(categories)):\n",
    "    img_path = ground_truth_paths[x]\n",
    "    prompt = prompts[x]\n",
    "    conf = confidences[x]\n",
    "    for fname in os.listdir(img_path):\n",
    "        save_path = fr\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\study_2\\GroundingDINO\\{categories[x]}\"\n",
    "        path = img_path + \"\\\\\" + fname\n",
    "        time_start = time.time()\n",
    "        boxes = run_dino_from_model(dino, path, prompt, conf, 0.1, max_areas[x], save_path)\n",
    "        DINO_time[f'{categories[x]}'].append(time.time() - time_start)\n",
    "        power.append(pynvml.nvmlDeviceGetPowerUsage(handle))\n",
    "\n",
    "# Get final power reading\n",
    "duration = time.time() - initial_time  # Time in seconds\n",
    "power_after = np.mean(power)\n",
    "# Calculate energy usage\n",
    "avg_power = (power_after - power_before)  # Average power in milliwatts\n",
    "energy_consumed = (avg_power * duration) / 1000  # Convert to Joules\n",
    "\n",
    "# Shutdown NVML\n",
    "pynvml.nvmlShutdown()\n",
    "\n",
    "\n",
    "for j in power:\n",
    "    print(j)\n",
    "print(power_before)\n",
    "print(f\"{energy_consumed} J\")\n",
    "print(DINO_time)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = process_files(r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\fescue\",\n",
    "                        r\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\Bounding_Fescue_LLM\\train\\labels\")\n",
    "\n",
    "print(f\"Average IoU: {np.mean(metrics['iou_scores'])}\")\n",
    "print(f\"Average Precision: {np.mean(metrics['precision_scores'])}\")\n",
    "print(f\"Average Recall: {np.mean(metrics['recall_scores'])}\")\n",
    "print(f\"Average F1: {np.mean(metrics['f1_scores'])}\")"
   ],
   "id": "c0d43bf6b2301759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Grounded SAM"
   ],
   "id": "799c669884b10a95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for category in categories:\n",
    "    save_path = fr\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\study_2\\GroundedSAM\\{category}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming single GPU (index 0)\n",
    "power = []\n",
    "\n",
    "SAM_time = {\n",
    "    'berries': [],\n",
    "    'buds': [],\n",
    "    'red leaf': [],\n",
    "    'fescue': [],\n",
    "}\n",
    "\n",
    "dino = load_dino_model(dino_model)\n",
    "model = SAM(SAM_model)\n",
    "initial_time = time.time()\n",
    "power_before = pynvml.nvmlDeviceGetPowerUsage(handle)  # in milliwatts\n",
    "for x in range(len(categories)):\n",
    "    img_path = ground_truth_paths[x]\n",
    "    prompt = prompts[x]\n",
    "    conf = confidences[x]\n",
    "    for fname in os.listdir(img_path):\n",
    "        sam_masks.clear()\n",
    "        save_path = fr\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\SAM_test\\{categories[x]}\"\n",
    "        path = img_path + \"\\\\\" + fname\n",
    "        time_start = time.time()\n",
    "        boxes = run_dino_from_model(dino, path, prompt, conf, 0.1, max_areas[x])\n",
    "        sam_results = model(os.path.join(img_path, fname), model=model, bboxes=boxes)\n",
    "        save_masks(sam_results, save_path)\n",
    "        SAM_time[f'{categories[x]}'].append(time.time() - time_start)\n",
    "        power.append(pynvml.nvmlDeviceGetPowerUsage(handle))\n",
    "\n",
    "# Get final power reading\n",
    "duration = time.time() - initial_time  # Time in seconds\n",
    "power_after = np.mean(power)\n",
    "# Calculate energy usage\n",
    "avg_power = (power_after - power_before)  # Average power in milliwatts\n",
    "energy_consumed = (avg_power * duration) / 1000  # Convert to Joules\n",
    "\n",
    "# Shutdown NVML\n",
    "pynvml.nvmlShutdown()\n",
    "\n",
    "\n",
    "for j in power:\n",
    "    print(j)\n",
    "print(power_before)\n",
    "print(f\"{energy_consumed} J\")\n",
    "print(SAM_time)"
   ],
   "id": "17e2810564d0d7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    save_path = fr\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\study_2\\YOLOE\\{category}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "# Initialize NVML\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming single GPU (index 0)\n",
    "power = []\n",
    "\n",
    "YOLO_time = {\n",
    "    'berries': [],\n",
    "    'buds': [],\n",
    "    'red leaf': [],\n",
    "    'fescue': [],\n",
    "}\n",
    "\n",
    "yoloe = load_YOLOE()\n",
    "initial_time = time.time()\n",
    "power_before = pynvml.nvmlDeviceGetPowerUsage(handle)  # in milliwatts\n",
    "for x in range(len(categories)):\n",
    "    img_path = ground_truth_paths[x]\n",
    "    prompt = prompts[x]\n",
    "    conf = confidences[x]\n",
    "    for fname in os.listdir(img_path):\n",
    "        save_path = fr\"C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\study_2\\YOLOE\\{categories[x]}\"\n",
    "        path = img_path + \"\\\\\" + fname\n",
    "        time_start = time.time()\n",
    "        boxes = run_YOLOE(yoloe, path, prompt, conf)\n",
    "        YOLO_time[f'{categories[x]}'].append(time.time() - time_start)\n",
    "        power.append(pynvml.nvmlDeviceGetPowerUsage(handle))\n",
    "\n",
    "# Get final power reading\n",
    "duration = time.time() - initial_time  # Time in seconds\n",
    "power_after = np.mean(power)\n",
    "# Calculate energy usage\n",
    "avg_power = (power_after - power_before)  # Average power in milliwatts\n",
    "energy_consumed = (avg_power * duration) / 1000  # Convert to Joules\n",
    "\n",
    "# Shutdown NVML\n",
    "pynvml.nvmlShutdown()\n",
    "\n",
    "\n",
    "for j in power:\n",
    "    print(j)\n",
    "print(power_before)\n",
    "print(f\"{energy_consumed} J\")\n",
    "print(YOLO_time)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d310a81f0da8a723"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Box-Prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6ae5ad1676a7ec2"
  },
  {
   "cell_type": "code",
   "source": [
    "use_same_class = True\n",
    "default_class_id = 0\n",
    "max_display_width = 1200\n",
    "\n",
    "# === Globals ===\n",
    "boxes = []\n",
    "class_ids = []\n",
    "drawing = False\n",
    "current_box = []\n",
    "resize_ratio = 1.0\n",
    "corner_size = 8\n",
    "selected_idx = -1\n",
    "corner_drag = False\n",
    "corner_index = -1\n",
    "mouse_x, mouse_y = 0, 0\n",
    "\n",
    "\n",
    "def point_near(p1, p2, thresh=10):\n",
    "    return abs(p1[0] - p2[0]) < thresh and abs(p1[1] - p2[1]) < thresh\n",
    "\n",
    "\n",
    "def find_corner(point, box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    corners = [(x1, y1), (x2, y1), (x2, y2), (x1, y2)]  # All 4 corners\n",
    "    for i, (cx, cy) in enumerate(corners):\n",
    "        if point_near(point, (cx, cy), thresh=corner_size):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def click_event(event, x, y, flags, param):\n",
    "    global drawing, current_box, boxes, class_ids\n",
    "    global selected_idx, corner_drag, corner_index, mouse_x, mouse_y\n",
    "\n",
    "    mouse_x, mouse_y = x, y\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        for idx, box in enumerate(boxes):\n",
    "            ci = find_corner((x, y), box)\n",
    "            if ci != -1:\n",
    "                selected_idx = idx\n",
    "                corner_drag = True\n",
    "                corner_index = ci\n",
    "                return\n",
    "        drawing = True\n",
    "        current_box = [(x, y)]\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        mouse_x, mouse_y = x, y\n",
    "        if drawing and current_box:\n",
    "            current_box = [current_box[0], (x, y)]\n",
    "        elif corner_drag and selected_idx != -1:\n",
    "            box = boxes[selected_idx]\n",
    "            # Allow dragging corners inward and outward\n",
    "            if corner_index == 0:  # top-left\n",
    "                box[0], box[1] = x, y\n",
    "            elif corner_index == 1:  # top-right\n",
    "                box[2], box[1] = x, y\n",
    "            elif corner_index == 2:  # bottom-right\n",
    "                box[2], box[3] = x, y\n",
    "            elif corner_index == 3:  # bottom-left\n",
    "                box[0], box[3] = x, y\n",
    "            boxes[selected_idx] = box\n",
    "\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        if drawing and len(current_box) == 2:\n",
    "            x1, y1 = current_box[0]\n",
    "            x2, y2 = current_box[1]\n",
    "            box = [min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)]\n",
    "            boxes.append(box)\n",
    "            class_ids.append(\n",
    "                default_class_id if use_same_class else int(input(f\"Enter class ID for box {len(boxes)}: \")))\n",
    "            current_box.clear()\n",
    "        drawing = False\n",
    "        corner_drag = False\n",
    "        corner_index = -1\n",
    "\n",
    "\n",
    "def draw_all(img):\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        color = (0, 255, 0) if i != selected_idx else (0, 0, 255)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        # Draw corner handles\n",
    "        for (cx, cy) in [(x1, y1), (x2, y1), (x2, y2), (x1, y2)]:\n",
    "            cv2.rectangle(img, (cx - corner_size, cy - corner_size),\n",
    "                          (cx + corner_size, cy + corner_size), (255, 255, 0), -1)\n",
    "\n",
    "\n",
    "def draw_cursor_guides(img):\n",
    "    step = 10\n",
    "    for y in range(0, img.shape[0], step * 2):\n",
    "        cv2.line(img, (mouse_x, y), (mouse_x, y + step), (200, 200, 200), 1)\n",
    "    for x in range(0, img.shape[1], step * 2):\n",
    "        cv2.line(img, (x, mouse_y), (x + step, mouse_y), (200, 200, 200), 1)\n",
    "\n",
    "\n",
    "def main(image_path):\n",
    "    global resize_ratio, selected_idx\n",
    "\n",
    "    original = cv2.imread(image_path)\n",
    "    if original is None:\n",
    "        print(\"Image not found.\")\n",
    "        return\n",
    "\n",
    "    h, w = original.shape[:2]\n",
    "    if w > max_display_width:\n",
    "        resize_ratio = max_display_width / w\n",
    "        display = cv2.resize(original, (int(w * resize_ratio), int(h * resize_ratio)))\n",
    "    else:\n",
    "        display = original.copy()\n",
    "\n",
    "    clone = display.copy()\n",
    "    cv2.namedWindow(\"Image\")\n",
    "    cv2.setMouseCallback(\"Image\", click_event)\n",
    "\n",
    "    print(\n",
    "        \"Draw boxes (click-drag). Drag corners to edit. \\nPress 'Enter' to run inference.\\nPress 'Backspace' while box is selected to delete it.\")\n",
    "\n",
    "    while True:\n",
    "        img_show = clone.copy()\n",
    "        draw_all(img_show)\n",
    "        draw_cursor_guides(img_show)\n",
    "\n",
    "        if drawing and len(current_box) == 2:\n",
    "            cv2.rectangle(img_show, current_box[0], current_box[1], (255, 255, 255), 1)\n",
    "\n",
    "        cv2.imshow(\"Image\", img_show)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('\\r'):\n",
    "            break\n",
    "        elif key == ord('\\b'):  # Backspace key\n",
    "            if selected_idx != -1 and selected_idx < len(boxes):\n",
    "                print(f\"Deleted box {selected_idx + 1}\")\n",
    "                boxes.pop(selected_idx)\n",
    "                class_ids.pop(selected_idx)\n",
    "                selected_idx = -1\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if not boxes:\n",
    "        print(\"No boxes labeled.\")\n",
    "        return\n",
    "\n",
    "    # Normalize coordinates before inference\n",
    "    bboxes = []\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        x1, x2 = sorted([x1, x2])\n",
    "        y1, y2 = sorted([y1, y2])\n",
    "        scaled_box = [x1 / resize_ratio, y1 / resize_ratio, x2 / resize_ratio, y2 / resize_ratio]\n",
    "        bboxes.append(scaled_box)\n",
    "\n",
    "    visual_prompts = dict(\n",
    "        bboxes=np.array(bboxes, dtype=np.float32),\n",
    "        cls=np.array(class_ids, dtype=np.int32),\n",
    "    )\n",
    "    return visual_prompts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T00:25:11.074447Z",
     "start_time": "2025-05-22T00:25:11.065572Z"
    }
   },
   "id": "8eb6b6e28d077023",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T00:25:32.653192Z",
     "start_time": "2025-05-22T00:25:11.913167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "root = Tk()\n",
    "root.withdraw()  # Hide the main window\n",
    "\n",
    "file_path = filedialog.askopenfilename(\n",
    "    title=\"Select an Image\",\n",
    "    filetypes=[(\"Image Files\", \"*.png;*.jpg;*.jpeg;*.bmp;*.gif\")]\n",
    ")"
   ],
   "id": "676af00a1fe3746c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "prompt = main(file_path)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T00:25:51.738664Z",
     "start_time": "2025-05-22T00:25:42.393128Z"
    }
   },
   "id": "989c39f090c7746b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draw boxes (click-drag). Drag corners to edit. \n",
      "Press 'Enter' to run inference.\n",
      "Press 'Backspace' while box is selected to delete it.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### YOLOE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3ed8c8b47cdc0eb"
  },
  {
   "cell_type": "code",
   "source": [
    "yoloe = load_YOLOE()\n",
    "\n",
    "boxes, results = run_YOLOE_vis(yoloe, file_path, prompt, conf=0.05, box_threshold=0.9)\n",
    "\n",
    "results[0].show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T00:27:31.658965Z",
     "start_time": "2025-05-22T00:27:30.618815Z"
    }
   },
   "id": "526f7d229d9729af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.129  Python-3.12.8 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4080, 16376MiB)\n",
      "YOLOe-11l-seg summary (fused): 227 layers, 35,117,862 parameters, 2,254,374 gradients\n",
      "\n",
      "image 1/1 C:\\Users\\cmull\\DataspellProjects\\AutoAnnotate\\autoannotate study\\Bounding_Berries_LLM\\train\\images\\IMG_9394_jpg.rf.93cd662dac6324bfa4ef17b55494eaf7.jpg: 448x640 9 object0s, 83.6ms\n",
      "Speed: 1.6ms preprocess, 83.6ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "48bc5a88c88796e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
