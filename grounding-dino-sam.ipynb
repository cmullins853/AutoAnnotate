{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "\n",
    "#There are two versions of Grounding Dino, GroundingDINO-T (tiny), and GroundingDINO-B (base) (with backbones SwinT and SwinB, respectively). In their paper they show results for a SwinL backbone, but it is not available anywhere that I can find.\n",
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import torch\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def clean_labels(boxes, max_area):\n",
    "    clean_boxes = []\n",
    "    box_list = boxes.tolist()\n",
    "    for box in box_list:\n",
    "        #if width * height < 0.9, add box to list.\n",
    "        if (box[2]*box[3])<max_area:\n",
    "            clean_boxes.append(box)\n",
    "    if len(clean_boxes)<2:\n",
    "        return boxes\n",
    "    return torch.FloatTensor(clean_boxes)\n",
    "\n",
    "\n",
    "def run_dino(img_path, prompt, box_threshold, text_threshold, model_size, maxarea=0.7, save_dir=\"DINO-labels\"):\n",
    "    #choose swinb or swint\n",
    "    if model_size == 'swint':\n",
    "        config_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinT_OGC.py\"\n",
    "        checkpoint_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\weights\\groundingdino_swint_ogc.pth\"\n",
    "    elif model_size == 'swinb':\n",
    "        checkpoint_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\weights\\groundingdino_swinb_cogcoor.pth\"\n",
    "        config_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinB_cfg.py\"\n",
    "\n",
    "    model = load_model(config_path, checkpoint_path)\n",
    "\n",
    "    image_source, image = load_image(img_path)\n",
    "\n",
    "    boxes, accuracy, obj_name = predict(model = model, image = image, caption = prompt, box_threshold = box_threshold, text_threshold = text_threshold)\n",
    "\n",
    "    #print(boxes, accuracy, obj_name)\n",
    "    #Convert boxes from YOLOv8 format to xyxy\n",
    "    img_height, img_width = cv2.imread(img_path).shape[:2]\n",
    "    clean_boxes = clean_labels(boxes, maxarea)\n",
    "    absolute_boxes = [[(box[0]-(box[2]/2))*img_width,\n",
    "                       (box[1]-(box[3]/2))*img_height,\n",
    "                       (box[0]+(box[2]/2))*img_width,\n",
    "                       (box[1]+(box[3]/2))*img_height] for box in clean_boxes.tolist()]\n",
    "    #annotated_frame = annotate(image_source=image_source, boxes=clean_boxes, logits=accuracy, phrases=obj_name)\n",
    "    #sv.plot_image(annotated_frame, (16,16))\n",
    "    save_labels = True\n",
    "    if save_labels:\n",
    "        clean_boxes = clean_boxes.tolist()\n",
    "\n",
    "        for x in clean_boxes:\n",
    "            x.insert(0,0)\n",
    "\n",
    "        with open(f'{save_dir}/{os.path.splitext(os.path.basename(img_path))[0]}.txt', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            writer.writerows(clean_boxes)\n",
    "            #print(\"Labels saved in /DINO-labels\")\n",
    "\n",
    "    return absolute_boxes\n",
    "\n",
    "def load_dino_model(model_size):\n",
    "    #choose swinb or swint\n",
    "    if model_size == 'swint':\n",
    "        config_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinT_OGC.py\"\n",
    "        checkpoint_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\weights\\groundingdino_swint_ogc.pth\"\n",
    "    elif model_size == 'swinb':\n",
    "        checkpoint_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\weights\\groundingdino_swinb_cogcoor.pth\"\n",
    "        config_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinB_cfg.py\"\n",
    "\n",
    "    model = load_model(config_path, checkpoint_path)\n",
    "    return model\n",
    "def run_dino_from_model(model, img_path, prompt, box_threshold, text_threshold, maxarea=0.7, save_dir=\"DINO-labels\"):\n",
    "    image_source, image = load_image(img_path)\n",
    "    boxes, accuracy, obj_name = predict(model = model, image = image, caption = prompt, box_threshold = box_threshold, text_threshold = text_threshold)\n",
    "\n",
    "    #print(boxes, accuracy, obj_name)\n",
    "    #Convert boxes from YOLOv8 format to xyxy\n",
    "    img_height, img_width = cv2.imread(img_path).shape[:2]\n",
    "    clean_boxes = clean_labels(boxes, maxarea)\n",
    "    absolute_boxes = [[(box[0]-(box[2]/2))*img_width,\n",
    "                       (box[1]-(box[3]/2))*img_height,\n",
    "                       (box[0]+(box[2]/2))*img_width,\n",
    "                       (box[1]+(box[3]/2))*img_height] for box in clean_boxes.tolist()]\n",
    "    #annotated_frame = annotate(image_source=image_source, boxes=clean_boxes, logits=accuracy, phrases=obj_name)\n",
    "    #sv.plot_image(annotated_frame, (16,16))\n",
    "    save_labels = True\n",
    "    if save_labels:\n",
    "        clean_boxes = clean_boxes.tolist()\n",
    "\n",
    "        for x in clean_boxes:\n",
    "            x.insert(0,0)\n",
    "\n",
    "        with open(f'{save_dir}/{os.path.splitext(os.path.basename(img_path))[0]}.txt', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            writer.writerows(clean_boxes)\n",
    "            #print(\"Labels saved in /DINO-labels\")\n",
    "    return absolute_boxes\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(TP, FP, FN, TN):\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    mcc = ((TP * TN) - (FP * FN)) / np.sqrt((TP+FP) * (TP+FN) * (TN+FP) * (TN+FN)) if np.sqrt((TP+FP) * (TP+FN) * (TN+FP) * (TN+FN)) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if TN + FP > 0 else 0\n",
    "    return precision, recall, f1, mcc, specificity\n",
    "\n",
    "def pixel_accuracy(predicted, ground_truth):\n",
    "    correct = np.sum(predicted == ground_truth)\n",
    "    total = predicted.shape[0] * predicted.shape[1]\n",
    "    return correct / total\n",
    "\n",
    "def read_and_draw_boxes(file_path, image_dim=(1280, 720)):\n",
    "    boxes = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            class_id, x, y, width, height = map(float, line.strip().split())\n",
    "            x1 = (x-(width/2))*image_dim[0]\n",
    "            x2 = (x+(width/2))*image_dim[0]\n",
    "            y1 = (y-(height/2))*image_dim[1]\n",
    "            y2 = (y+(height/2))*image_dim[1]\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "    image = Image.new('L', image_dim, 0)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box, fill=255)\n",
    "        #draw.rectangle([1,1,20,20], fill=255)\n",
    "    image.save(\"test.jpg\")\n",
    "    return np.array(image, dtype=np.uint8)\n",
    "\n",
    "def calculate_pixel_metrics(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Calculate IoU based on pixel values from two masks.\n",
    "    \"\"\"\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection / union\n",
    "\n",
    "def process_files(predicted_mask_dir, ground_truth_mask_dir):\n",
    "    predicted_files = os.listdir(ground_truth_mask_dir)\n",
    "    metrics = {\n",
    "        'iou_scores': [],\n",
    "        'pixel_accuracies': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': [],\n",
    "        'f1_scores': [],\n",
    "        'mcc_scores': [],\n",
    "        'specificity_scores': []\n",
    "    }\n",
    "\n",
    "    for fname in predicted_files:\n",
    "        predicted_mask_path = os.path.join(predicted_mask_dir, fname)\n",
    "        ground_truth_mask_path = os.path.join(ground_truth_mask_dir, os.path.splitext(fname)[0] + '.txt')\n",
    "\n",
    "        if not os.path.exists(ground_truth_mask_path):\n",
    "            metrics['iou_scores'].append(0)\n",
    "            metrics['pixel_accuracies'].append(0)\n",
    "            metrics['precision_scores'].append(0)\n",
    "            metrics['recall_scores'].append(0)\n",
    "            metrics['f1_scores'].append(0)\n",
    "            metrics['mcc_scores'].append(0)\n",
    "            metrics['specificity_scores'].append(0)\n",
    "            continue\n",
    "\n",
    "        predicted_mask = read_and_draw_boxes(predicted_mask_path)\n",
    "        ground_truth_mask = read_and_draw_boxes(ground_truth_mask_path)\n",
    "\n",
    "        COMMON_HEIGHT, COMMON_WIDTH = 1280, 720  # or any other desired size\n",
    "\n",
    "        predicted_mask = cv2.resize(predicted_mask, (COMMON_WIDTH, COMMON_HEIGHT))\n",
    "\n",
    "        ground_truth_mask = cv2.resize(ground_truth_mask, (COMMON_WIDTH, COMMON_HEIGHT))\n",
    "\n",
    "        _, predicted_mask_bin = cv2.threshold(predicted_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        _, ground_truth_mask_bin = cv2.threshold(ground_truth_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        predicted_mask_bin = predicted_mask_bin / 255\n",
    "        ground_truth_mask_bin = ground_truth_mask_bin / 255\n",
    "        TP = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 1)))\n",
    "        TN = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 0)))\n",
    "        FP = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 0)))\n",
    "        FN = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 1)))\n",
    "\n",
    "\n",
    "        intersection = np.logical_and(predicted_mask_bin, ground_truth_mask_bin)\n",
    "        union = np.logical_or(predicted_mask_bin, ground_truth_mask_bin)\n",
    "        metrics['iou_scores'].append(np.sum(intersection) / np.sum(union))\n",
    "        metrics['pixel_accuracies'].append(pixel_accuracy(predicted_mask_bin, ground_truth_mask_bin))\n",
    "        precision, recall, f1, mcc, specificity = calculate_metrics(TP, FP, FN, TN)\n",
    "        metrics['precision_scores'].append(precision)\n",
    "        metrics['recall_scores'].append(recall)\n",
    "        metrics['f1_scores'].append(f1)\n",
    "        metrics['mcc_scores'].append(mcc)\n",
    "        metrics['specificity_scores'].append(specificity)\n",
    "\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def optimize_prompts(prompts_file, gt_path, img_dir, save_file):\n",
    "    inf_path = fr\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\DINO-labels\"\n",
    "\n",
    "    with  open(prompts_file, 'r') as file:\n",
    "        result_dict = {}\n",
    "        for x in file:\n",
    "            result_dict[x.strip()] = {}\n",
    "\n",
    "    #result_dict = dict.fromkeys(prompts,{})\n",
    "    for prompt in result_dict.keys():\n",
    "        print(f'Trying prompt: \"{prompt}\"')\n",
    "        for fname in os.listdir(img_dir):\n",
    "            box_threshold = 0.3\n",
    "            text_threshold = 0.1\n",
    "            model_size = 'swint'\n",
    "            run_dino(os.path.join(img_dir, fname), prompt, box_threshold, text_threshold, model_size)\n",
    "\n",
    "        metrics = process_files(inf_path, gt_path)\n",
    "\n",
    "        result_dict[prompt]['iou_scores'] = np.mean(metrics['iou_scores'])\n",
    "\n",
    "    results = sorted(list(result_dict.items()), key=lambda a:a[1]['iou_scores'], reverse=True)\n",
    "    print(results)\n",
    "\n",
    "    with open(save_file, 'w') as output:\n",
    "        for prompt_stats in results:\n",
    "            output.write(str(prompt_stats)+'\\n')\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def optimize_confidence(prompt, model_size, gt_path, img_dir):\n",
    "    inf_path = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\DINO-labels\"\n",
    "    best_iou = 0\n",
    "    #number of decimal points in confidence\n",
    "    final_precision = 2\n",
    "    ubound = 0.9\n",
    "    lbound = 0.0\n",
    "    for precision in [x+1 for x in range(final_precision)]:\n",
    "        for conf in [x/(10**precision) for x in range(int(lbound*(10**precision)),int(ubound*(10**precision)))]:\n",
    "            for fname in os.listdir(img_dir):\n",
    "                prompt = prompt\n",
    "                box_threshold = conf\n",
    "                text_threshold = 0.01\n",
    "                model_size = model_size\n",
    "                boxes = run_dino(os.path.join(img_dir, fname), prompt, box_threshold, text_threshold, model_size)\n",
    "            metrics = process_files(inf_path, gt_path)\n",
    "            iou = np.mean(metrics['iou_scores'])\n",
    "            if iou>best_iou:\n",
    "                best_iou = iou\n",
    "                best_conf = conf\n",
    "            print(f\"confidence: {conf}, IOU: {iou} (best: {best_iou})\")\n",
    "        print(f\"Best IOU at p{precision} is {best_iou} with confidence = {best_conf}\")\n",
    "        lbound = max(0,best_conf-(1/(10**precision)))\n",
    "        ubound = min(0.9,best_conf+(1/(10**precision)))\n",
    "    return best_iou, best_conf\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import time as t\n",
    "\n",
    "def multi_optmize(img_dir, gt_label_dir, model_size, prompts):\n",
    "    print(\"Be sure to change the category folders and model size in each function!\")\n",
    "    t.sleep(2)\n",
    "    start = t.time()\n",
    "    best_iou = 0\n",
    "    for prompt in prompts:\n",
    "        print(f\"Trying prompt: '{prompt}'\")\n",
    "        iou, conf = optimize_confidence(prompt, model_size, gt_label_dir, img_dir)\n",
    "        if iou>best_iou:\n",
    "            best_iou = iou\n",
    "            best_conf = conf\n",
    "            best_prompt = prompt\n",
    "        print(f\"So far: best prompt is '{best_prompt}', conf is {best_conf}, resulting in {best_iou} IOU)\")\n",
    "    print(f\"\\n\\n\\n\\n\\nFinal Result: best prompt is '{best_prompt}', conf is {best_conf}, resulting in {best_iou} IOU)\")\n",
    "    print(f\"final time: {t.time()-start}\")\n",
    "    return {\"prompt\":best_prompt, \"conf\": best_conf, \"iou\":best_iou }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Berries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Initial prompt picker\n",
    "prompts_file = r'C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\blueberry-prompts.txt'\n",
    "ground_truth_dir = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-berries\\train\\labels\"\n",
    "\n",
    "results = optimize_prompts(prompts_file,ground_truth_dir, r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-berries\\train\\images\", 'berry-results-dino.txt')\n",
    "\n",
    "top10 = [result[0] for result in results][0:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(top10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#swint\n",
    "multi_optmize(r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-berries\\train\\images\", r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-berries\\train\\labels\", 'swint', top10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#swinb\n",
    "multi_optmize(r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-berries\\train\\images\", r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-berries\\train\\labels\", 'swinb', top10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RedLeaf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"S3eK0rrtbGBZFhsHBOEK\")\n",
    "project = rf.workspace(\"autoannotate-study\").project(\"red-leaf-bounding-box\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov8\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Initial prompt picker\n",
    "prompts_file = r'C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\red_leaf_plant_prompts.txt'\n",
    "ground_truth_dir = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\red-leaf-bounding-box-1\\train\\labels\"\n",
    "\n",
    "results = optimize_prompts(prompts_file,ground_truth_dir, r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\red-leaf-bounding-box-1\\train\\images\", 'redleaf-results-dino.txt')\n",
    "\n",
    "top10 = [result[0] for result in results][0:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#swint\n",
    "multi_optmize(r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\red-leaf-bounding-box-1\\train\\images\", r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\red-leaf-bounding-box-1\\train\\labels\", 'swint', top10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#swinb\n",
    "multi_optmize(r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\red-leaf-bounding-box-1\\train\\images\", r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\red-leaf-bounding-box-1\\train\\labels\", 'swinb', top10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fescue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf = Roboflow(api_key=\"S3eK0rrtbGBZFhsHBOEK\")\n",
    "project = rf.workspace(\"autoannotate-study\").project(\"fescue-bounding-box\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov8\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Initial prompt picker\n",
    "prompts_file = r'C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\Fescue_prompts.txt'\n",
    "ground_truth_dir = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\fescue-bounding-box-1\\train\\labels\"\n",
    "\n",
    "results = optimize_prompts(prompts_file,ground_truth_dir, r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\fescue-bounding-box-1\\train\\images\", 'fescue-results-dino.txt')\n",
    "\n",
    "top10 = [result[0] for result in results][0:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "multi_optmize(\n",
    "    r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\fescue-bounding-box-1\\train\\images\",\n",
    "    r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\fescue-bounding-box-1\\train\\labels\",\n",
    "    'swint', top10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#swinb\n",
    "multi_optmize(\n",
    "    r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\fescue-bounding-box-1\\train\\images\",\n",
    "    r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\fescue-bounding-box-1\\train\\labels\",\n",
    "    'swinb', top10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Buds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"S3eK0rrtbGBZFhsHBOEK\")\n",
    "project = rf.workspace(\"autoannotate-study\").project(\"bounding-buds\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov8\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Initial prompt picker\n",
    "prompts_file = r'C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bud_prompts.txt'\n",
    "ground_truth_dir = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-buds-1\\train\\labels\"\n",
    "\n",
    "results = optimize_prompts(prompts_file,ground_truth_dir, r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-buds-1\\train\\images\", 'bud-results-dino.txt')\n",
    "\n",
    "top10 = [result[0] for result in results][0:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "multi_optmize(\n",
    "    r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-buds-1\\train\\images\",\n",
    "    r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-buds-1\\train\\labels\",\n",
    "    'swint', top10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#swinb\n",
    "multi_optmize(\n",
    "    r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-buds-1\\train\\images\",\n",
    "    r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\bounding-buds-1\\train\\labels\",\n",
    "    'swinb', top10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "detection_dir = r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\blueberry-images\\train\\images\"\n",
    "for fname in os.listdir(detection_dir):\n",
    "    prompt = 'smooth blueberry'\n",
    "    box_threshold = 0.360\n",
    "    text_threshold = 0.01\n",
    "    model_size = 'swint'\n",
    "    run_dino(os.path.join(detection_dir, fname), prompt, box_threshold, text_threshold, model_size)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = process_files(r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\GroundingDINO\\DINO-labels\", r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\blueberry-images\\train\\labels_yolo\")\n",
    "\n",
    "print(f\"Average IoU: {np.mean(metrics['iou_scores'])}\")\n",
    "print(f\"Average Precision: {np.mean(metrics['precision_scores'])}\")\n",
    "print(f\"Average Recall: {np.mean(metrics['recall_scores'])}\")\n",
    "print(f\"Average F1: {np.mean(metrics['f1_scores'])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Runs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "ground_truth_paths = [r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\berries-1\\train\\images\",\n",
    "         r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\red-leaf-1\\train\\images\",\n",
    "         r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\fescue-1\\train\\images\",\n",
    "         r\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\buds-1\\train\\images\"]\n",
    "dino_models = ['swint', 'swinb']\n",
    "prompts = [\n",
    "    ['smooth blueberry', 'blueberry'],\n",
    "    ['A red leaf plant growing among green plants', 'A cluster of red leaves surrounded by green foliage'],\n",
    "    ['grass spots', 'Fescue grass spots'],\n",
    "    ['buds emerging', 'developing bud']\n",
    "]\n",
    "confidences = [[0.36, 0.4, 0.06, 0.41], [0.36, 0.3, 0.04, 0.34]]\n",
    "SAM_model = ['sam_b.pt','mobile_sam.pt','sam_l.pt']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from ultralytics import SAM\n",
    "from pathlib import Path\n",
    "import time as t\n",
    "\n",
    "def save_masks(sam_results, output_dir):\n",
    "    segments = sam_results[0].masks.xyn\n",
    "    with open(f\"{Path(output_dir) / Path(sam_results[0].path).stem}.txt\", \"w\") as f:\n",
    "        for i in range(len(segments)):\n",
    "            s = segments[i]\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            segment = map(str, segments[i].reshape(-1).tolist())\n",
    "            f.write(f\"0 \" + \" \".join(segment) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "\n",
      "\n",
      "Time for berries,swint,dino_SAM_b: 5.138290643692017\n",
      "\n",
      "\n",
      "Time for berries,swint,dino_Mobile: 3.380899667739868\n",
      "\n",
      "\n",
      "Time for berries,swint,dino_SAM_l: 5.973753929138184\n",
      "\n",
      "\n",
      "Time for red leaf,swint,dino_SAM_b: 4.351370334625244\n",
      "\n",
      "\n",
      "Time for red leaf,swint,dino_Mobile: 3.410944938659668\n",
      "\n",
      "\n",
      "Time for red leaf,swint,dino_SAM_l: 6.352397680282593\n",
      "\n",
      "\n",
      "Time for fescue,swint,dino_SAM_b: 6.2403199672698975\n",
      "\n",
      "\n",
      "Time for fescue,swint,dino_Mobile: 5.288557767868042\n",
      "\n",
      "\n",
      "Time for fescue,swint,dino_SAM_l: 8.47096037864685\n",
      "\n",
      "\n",
      "Time for buds,swint,dino_SAM_b: 3.9416797161102295\n",
      "\n",
      "\n",
      "Time for buds,swint,dino_Mobile: 3.03104305267334\n",
      "\n",
      "\n",
      "Time for buds,swint,dino_SAM_l: 5.605805158615112\n",
      "final text_encoder_type: bert-base-uncased\n",
      "\n",
      "\n",
      "Time for berries,swinb,dino_SAM_b: 5.15105938911438\n",
      "\n",
      "\n",
      "Time for berries,swinb,dino_Mobile: 3.902315378189087\n",
      "\n",
      "\n",
      "Time for berries,swinb,dino_SAM_l: 6.486317873001099\n",
      "\n",
      "\n",
      "Time for red leaf,swinb,dino_SAM_b: 4.537626504898071\n",
      "\n",
      "\n",
      "Time for red leaf,swinb,dino_Mobile: 3.6452722549438477\n",
      "\n",
      "\n",
      "Time for red leaf,swinb,dino_SAM_l: 6.572909116744995\n",
      "\n",
      "\n",
      "Time for fescue,swinb,dino_SAM_b: 6.000791311264038\n",
      "\n",
      "\n",
      "Time for fescue,swinb,dino_Mobile: 5.141617059707642\n",
      "\n",
      "\n",
      "Time for fescue,swinb,dino_SAM_l: 8.097185373306274\n",
      "\n",
      "\n",
      "Time for buds,swinb,dino_SAM_b: 4.321226596832275\n",
      "\n",
      "\n",
      "Time for buds,swinb,dino_Mobile: 3.642432928085327\n",
      "\n",
      "\n",
      "Time for buds,swinb,dino_SAM_l: 6.209322929382324\n"
     ]
    }
   ],
   "source": [
    "dino_models = ['swint', 'swinb']\n",
    "#dino_models = ['swinb']\n",
    "sam_models = ['dino_SAM_b', 'dino_Mobile', 'dino_SAM_l']\n",
    "categories = ['berries','red leaf','fescue','buds']\n",
    "max_areas = [0.4, 0.4, 0.7, 0.25]\n",
    "folders = {'berries':'berries-1', 'red leaf':'red-leaf-1','fescue':'fescue-1','buds':'buds-1'}\n",
    "\n",
    "start = t.time()\n",
    "for y in range(len(dino_models)):\n",
    "    dino_model = load_dino_model(dino_models[y])\n",
    "    for x in range(len(categories)):\n",
    "        for z in range(len(sam_models)):\n",
    "            img_path = ground_truth_paths[x]\n",
    "            model = SAM(SAM_model[z])\n",
    "            prompt = prompts[x][1]\n",
    "            conf = confidences[1][x]\n",
    "            start = t.time()\n",
    "            for fname in os.listdir(img_path):\n",
    "                path = img_path+\"\\\\\"+fname\n",
    "                boxes = run_dino_from_model(dino_model, path, prompt, conf, 0.1, max_areas[x])\n",
    "                #print(len(boxes))\n",
    "                #print(f'conf{conf}, prompt{prompt}')\n",
    "                sam_results = model(os.path.join(img_path, fname), model= model, bboxes=boxes, verbose=False)\n",
    "                save_masks(sam_results, fr\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\Worldly_SAM\\trials\\{categories[x]}\\{dino_models[y]}\\{sam_models[z]}\")\n",
    "            print(f\"\\n\\nTime for {categories[x]},{dino_models[y]},{sam_models[z]}: {t.time()-start}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Segmentation Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_and_draw_masks(file_path, image_dim=(1280, 720)):\n",
    "    boxes = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        masks = []\n",
    "        for line in file:\n",
    "            raw_mask = [float(x) for x in list(line.strip().split())]\n",
    "            points = []\n",
    "            for point in range(int((len(raw_mask) - 1) / 2)):\n",
    "                p1 = int(raw_mask[(2 * point) + 1] * image_dim[0])\n",
    "                p2 = int(raw_mask[(2 * point) + 2] * image_dim[1])\n",
    "                points.append([p1, p2])\n",
    "            masks.append(points)\n",
    "    canvas = np.zeros((image_dim[1], image_dim[0]), dtype=np.uint8)\n",
    "    for mask in masks:\n",
    "        cv2.fillPoly(canvas, np.array([mask], dtype=np.int32), 255)\n",
    "\n",
    "    #plt.imshow(canvas, cmap='gray')\n",
    "    #plt.axis('off')\n",
    "    #plt.show()\n",
    "\n",
    "    #image = Image.new('L', image_dim, 0)\n",
    "    #draw = ImageDraw.Draw(image)\n",
    "    #for box in boxes:\n",
    "    #draw.rectangle(box, fill=255)\n",
    "    #draw.rectangle([1,1,20,20], fill=255)\n",
    "    #image.save(\"test.jpg\")\n",
    "    return np.array(canvas, dtype=np.uint8)\n",
    "\n",
    "def process_files_seg(predicted_mask_dir, ground_truth_mask_dir):\n",
    "    predicted_files = os.listdir(ground_truth_mask_dir)\n",
    "    metrics = {\n",
    "        'iou_scores': [],\n",
    "        'pixel_accuracies': [],\n",
    "        'precision_scores': [],\n",
    "        'recall_scores': [],\n",
    "        'f1_scores': [],\n",
    "        'mcc_scores': [],\n",
    "        'specificity_scores': []\n",
    "    }\n",
    "\n",
    "    for fname in predicted_files:\n",
    "        predicted_mask_path = os.path.join(predicted_mask_dir, fname)\n",
    "        ground_truth_mask_path = os.path.join(ground_truth_mask_dir, os.path.splitext(fname)[0] + '.txt')\n",
    "        if not os.path.exists(predicted_mask_path):\n",
    "            metrics['iou_scores'].append(0)\n",
    "            metrics['pixel_accuracies'].append(0)\n",
    "            metrics['precision_scores'].append(0)\n",
    "            metrics['recall_scores'].append(0)\n",
    "            metrics['f1_scores'].append(0)\n",
    "            metrics['mcc_scores'].append(0)\n",
    "            metrics['specificity_scores'].append(0)\n",
    "            continue\n",
    "\n",
    "        COMMON_HEIGHT, COMMON_WIDTH = 1280, 720  # or any other desired size\n",
    "\n",
    "        #print(\"predicted, ground-truth\")\n",
    "        predicted_mask = read_and_draw_masks(predicted_mask_path)\n",
    "\n",
    "        ground_truth_mask = read_and_draw_masks(ground_truth_mask_path)\n",
    "\n",
    "        predicted_mask = cv2.resize(predicted_mask, (COMMON_WIDTH, COMMON_HEIGHT))\n",
    "        ground_truth_mask = cv2.resize(ground_truth_mask, (COMMON_WIDTH, COMMON_HEIGHT))\n",
    "\n",
    "        _, predicted_mask_bin = cv2.threshold(predicted_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        _, ground_truth_mask_bin = cv2.threshold(ground_truth_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        predicted_mask_bin = predicted_mask_bin / 255\n",
    "        ground_truth_mask_bin = ground_truth_mask_bin / 255\n",
    "        TP = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 1)))\n",
    "        TN = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 0)))\n",
    "        FP = np.float64(np.sum(np.logical_and(predicted_mask_bin == 1, ground_truth_mask_bin == 0)))\n",
    "        FN = np.float64(np.sum(np.logical_and(predicted_mask_bin == 0, ground_truth_mask_bin == 1)))\n",
    "\n",
    "        intersection = np.logical_and(predicted_mask_bin, ground_truth_mask_bin)\n",
    "        union = np.logical_or(predicted_mask_bin, ground_truth_mask_bin)\n",
    "        metrics['iou_scores'].append(np.sum(intersection) / np.sum(union))\n",
    "        metrics['pixel_accuracies'].append(pixel_accuracy(predicted_mask_bin, ground_truth_mask_bin))\n",
    "        precision, recall, f1, mcc, specificity = calculate_metrics(TP, FP, FN, TN)\n",
    "        metrics['precision_scores'].append(precision)\n",
    "        metrics['recall_scores'].append(recall)\n",
    "        metrics['f1_scores'].append(f1)\n",
    "        metrics['mcc_scores'].append(mcc)\n",
    "        metrics['specificity_scores'].append(specificity)\n",
    "\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45984186433624635\n",
      "0.5108952116585704\n",
      "0.7074639623887788\n",
      "0.6249239381769502\n",
      "0.8326453508710557\n",
      "0.7878131021194605\n",
      "0.8764237931657928\n",
      "0.8505839710028191\n",
      "0.8403943069169851\n",
      "berries\\swint\\dino_SAM_b: Mean IOU = 0.721220611181851\n",
      "0.44528164858608327\n",
      "0.45849768107147504\n",
      "0.7852210693756553\n",
      "0.6126803619466862\n",
      "0.7978574755472753\n",
      "0.7525490792877796\n",
      "0.85728293813514\n",
      "0.8375980893892869\n",
      "0.8224400281583365\n",
      "berries\\swint\\dino_Mobile: Mean IOU = 0.7077120412775242\n",
      "0.46908518853982933\n",
      "0.4482562816448209\n",
      "0.8146875350192372\n",
      "0.6271991233944116\n",
      "0.7446329274301401\n",
      "0.7880975296759705\n",
      "0.8846364133406233\n",
      "0.850275671950379\n",
      "0.8577502899110939\n",
      "berries\\swint\\dino_SAM_l: Mean IOU = 0.7205134401007229\n",
      "0.465875997129954\n",
      "0.4828508771929825\n",
      "0.7016602124151682\n",
      "0.6261187214611872\n",
      "0.8847780126849895\n",
      "0.8664346895074947\n",
      "0.9432327166504382\n",
      "0.8505687693898656\n",
      "0.8410324581810125\n",
      "berries\\swinb\\dino_SAM_b: Mean IOU = 0.7402836060681213\n",
      "0.4033339439457776\n",
      "0.3746486890319826\n",
      "0.7154632495860629\n",
      "0.6121275163678639\n",
      "0.8840790238233586\n",
      "0.8553259141494436\n",
      "0.9232542503121698\n",
      "0.8381667235300807\n",
      "0.8225230102869519\n",
      "berries\\swinb\\dino_Mobile: Mean IOU = 0.7143247023370769\n",
      "0.455222869529874\n",
      "0.4150138528519284\n",
      "0.74215022744924\n",
      "0.6269802583475506\n",
      "0.8910246708036931\n",
      "0.8817760828112231\n",
      "0.9497359671425778\n",
      "0.8506657483930211\n",
      "0.8577181948582148\n",
      "berries\\swinb\\dino_SAM_l: Mean IOU = 0.7411430969097026\n",
      "0.5326436026567404\n",
      "0.677293488071036\n",
      "0.9690523729073875\n",
      "0.9291790154343559\n",
      "0.899630107317548\n",
      "0.966550601485802\n",
      "0.9664363640826144\n",
      "0.9718114375552717\n",
      "0.7643691079054885\n",
      "0.4810877986224038\n",
      "red leaf\\swint\\dino_SAM_b: Mean IOU = 0.8158053896038648\n",
      "0.807864311093241\n",
      "0.8366234809187829\n",
      "0.9706468424279583\n",
      "0.9728025075973319\n",
      "0.9072455427262809\n",
      "0.9669486254509267\n",
      "0.9687473346898493\n",
      "0.9778068843950448\n",
      "0.9541933999642136\n",
      "0.9522730491431809\n",
      "red leaf\\swint\\dino_Mobile: Mean IOU = 0.9315151978406808\n",
      "0.6702052432625909\n",
      "0.5858947041125204\n",
      "0.9763921358218679\n",
      "0.9804622211976255\n",
      "0.913187106803663\n",
      "0.9784451319198169\n",
      "0.9742741141732284\n",
      "0.9607993092389293\n",
      "0.9801065204208084\n",
      "0.9378897579537047\n",
      "red leaf\\swint\\dino_SAM_l: Mean IOU = 0.8957656244904756\n",
      "0.21476620630954668\n",
      "0.2964491248616156\n",
      "0.9690115966515629\n",
      "0.9322236120465698\n",
      "0.8994027417922612\n",
      "0.9562753444324107\n",
      "0.7580406836428114\n",
      "0.9192900283495624\n",
      "0.7660289042563038\n",
      "0.4798571145068466\n",
      "red leaf\\swinb\\dino_SAM_b: Mean IOU = 0.7191345356849491\n",
      "0.8062041579836897\n",
      "0.8182490205258359\n",
      "0.9705724576595908\n",
      "0.9729368143419594\n",
      "0.9070963565082905\n",
      "0.9651729510751013\n",
      "0.9664723564274501\n",
      "0.9778502747252747\n",
      "0.9540633784819831\n",
      "0.9521559438128468\n",
      "red leaf\\swinb\\dino_Mobile: Mean IOU = 0.9290773711542023\n",
      "0.6688119201476027\n",
      "0.6971912204070142\n",
      "0.9763716152770059\n",
      "0.9803898972438299\n",
      "0.9129917291132783\n",
      "0.9752741373015573\n",
      "0.901965383570776\n",
      "0.8829159710628826\n",
      "0.9801199657240789\n",
      "0.9378204111833413\n",
      "red leaf\\swinb\\dino_SAM_l: Mean IOU = 0.8913852251031367\n",
      "0.17745390705432246\n",
      "0.2335692976491351\n",
      "0.28281414288469553\n",
      "0.2783257467202033\n",
      "0.26740755620200535\n",
      "0.1833439687755798\n",
      "0.21139649180530137\n",
      "0.13424543921175167\n",
      "0.13157172291625127\n",
      "0.09096311231396904\n",
      "0.09019544706646708\n",
      "fescue\\swint\\dino_SAM_b: Mean IOU = 0.18920789387269837\n",
      "0.17595268403513423\n",
      "0.22348399331737934\n",
      "0.2300474979513862\n",
      "0.17158572783744913\n",
      "0.17965340761752338\n",
      "0.1650001847621242\n",
      "0.14163352334960885\n",
      "0.11056887861319513\n",
      "0.11577550674721925\n",
      "0.0861201864436313\n",
      "0.08605536177881609\n",
      "fescue\\swint\\dino_Mobile: Mean IOU = 0.1532615411321334\n",
      "0.17584942926148867\n",
      "0.28470894535032615\n",
      "0.23501405629143293\n",
      "0.28876254355857156\n",
      "0.2918604803350566\n",
      "0.1667405921541887\n",
      "0.17312162274120646\n",
      "0.12033796897179044\n",
      "0.11351704002113491\n",
      "0.0861388543648922\n",
      "0.08707159143931191\n",
      "fescue\\swint\\dino_SAM_l: Mean IOU = 0.18392028404449096\n",
      "0.22561480067978715\n",
      "0.09916114914151049\n",
      "0.128007809470808\n",
      "0.42197438001088483\n",
      "0.4081742204016913\n",
      "0.1563097395278748\n",
      "0.17006135651980395\n",
      "0.14333738383813766\n",
      "0.13587128736485957\n",
      "0.2335477318889641\n",
      "0.23933273676269826\n",
      "fescue\\swinb\\dino_SAM_b: Mean IOU = 0.21467205414609272\n",
      "0.21433528000256574\n",
      "0.14860677023107063\n",
      "0.06859364683442154\n",
      "0.3648445647315886\n",
      "0.38185194493312113\n",
      "0.13564408378576104\n",
      "0.1306544512293823\n",
      "0.11289971954214248\n",
      "0.12022328234889096\n",
      "0.1326668590371633\n",
      "0.1397237515354501\n",
      "fescue\\swinb\\dino_Mobile: Mean IOU = 0.17727675947377797\n",
      "0.2478950714402551\n",
      "0.15111956543580332\n",
      "0.07251949006635328\n",
      "0.38082029480433494\n",
      "0.4260461005279929\n",
      "0.13500797194722094\n",
      "0.1406770637950438\n",
      "0.12286764310326766\n",
      "0.11991584953574885\n",
      "0.16368600371754907\n",
      "0.20386565795034065\n",
      "fescue\\swinb\\dino_SAM_l: Mean IOU = 0.1967655193021737\n",
      "0.7903353578030103\n",
      "0.8650163650727963\n",
      "0.881268270744322\n",
      "0.42241261450120765\n",
      "0.8478619521112944\n",
      "0.7660119555935099\n",
      "0.0\n",
      "0.6461275803197268\n",
      "0.5051139608535382\n",
      "buds\\swint\\dino_SAM_b: Mean IOU = 0.6360164507777117\n",
      "0.7759577278731836\n",
      "0.8488910645027676\n",
      "0.8704271713922346\n",
      "0.41990569061026417\n",
      "0.8569651207688017\n",
      "0.7742984776857353\n",
      "0.0\n",
      "0.6364200724262804\n",
      "0.5013517729021525\n",
      "buds\\swint\\dino_Mobile: Mean IOU = 0.6315796775734911\n",
      "0.7893578260209504\n",
      "0.8674137735206149\n",
      "0.8775793501913409\n",
      "0.42354823959762233\n",
      "0.8549923609238789\n",
      "0.7662150605392267\n",
      "0.0\n",
      "0.6519864970137627\n",
      "0.5090502444606262\n",
      "buds\\swint\\dino_SAM_l: Mean IOU = 0.637793705807558\n",
      "0.8656323393352786\n",
      "0.8209872133723838\n",
      "0.8578093675961364\n",
      "0.7600362072867165\n",
      "0.8478755183021981\n",
      "0.8553833751343605\n",
      "0.40875780002902334\n",
      "0.6267316432130537\n",
      "0.683885764211216\n",
      "buds\\swinb\\dino_SAM_b: Mean IOU = 0.7474554698311519\n",
      "0.8480814408770556\n",
      "0.8055734190782422\n",
      "0.8482741042819691\n",
      "0.745257820634705\n",
      "0.8570509245355871\n",
      "0.8597747767182983\n",
      "0.4031786165793751\n",
      "0.6190606525071163\n",
      "0.6748921766664847\n",
      "buds\\swinb\\dino_Mobile: Mean IOU = 0.7401271035420927\n",
      "0.865089605734767\n",
      "0.8225661651165296\n",
      "0.8541704690636978\n",
      "0.7612510228202564\n",
      "0.8548445083587992\n",
      "0.8574059103565975\n",
      "0.41189315088434386\n",
      "0.6321491421029476\n",
      "0.6867311792684927\n",
      "buds\\swinb\\dino_SAM_l: Mean IOU = 0.7495667948562701\n"
     ]
    }
   ],
   "source": [
    "dino_models = ['swint', 'swinb']\n",
    "sam_models = ['dino_SAM_b', 'dino_Mobile', 'dino_SAM_l']\n",
    "categories = ['berries','red leaf','fescue','buds']\n",
    "folders = {'berries':'berries-1', 'red leaf':'red-leaf-1','fescue':'fescue-1','buds':'buds-1'}\n",
    "\n",
    "for category in categories:\n",
    "    for dino_model in dino_models:\n",
    "        for sam_model in sam_models:\n",
    "            metrics = process_files_seg(fr\"C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\Worldly_SAM\\trials\\{category}\\{dino_model}\\{sam_model}\", fr'C:\\Users\\Mechanized Systems\\DataspellProjects\\WSU_joint_data\\Auto Annotate\\{folders[category]}\\train\\labels')\n",
    "            for score in metrics['iou_scores']:\n",
    "                print(score)\n",
    "            print(rf\"{category}\\{dino_model}\\{sam_model}: Mean IOU = {np.mean(metrics['iou_scores'])}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
